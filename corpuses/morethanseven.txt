The following is the email I sent to lots of my colleagues at the
Government Digital Service last week.
So, after 3 rather exciting years I’ve decided to leave GDS.
That’s surprisingly difficult to write if I’m honest.
I was part of the team that built and shipped the beta of GOV.UK.
Since then I’ve worked across half of what has become GDS, equally
helping and frustrating (then hopefully helping) lots of you. I’ve
done a great deal and learnt even more, as well as collected arcane
knowledge about government infosec and the dark arts of procurement
along the way. I’ve done, and helped others do, work I wouldn’t even
have thought possible (or maybe likely is the right word?) when I
started.
So why leave? For all the other things I do I’m basically a tool
builder. So I’m going off to work for Puppet Labs to build
infrastructure tools that don’t suck. That sort of pretty specific
work is something that should be done outside government in my
opinion. I’m a pretty firm believer in “government should only do what
only government can do” (design principle number 2 for those that
haven’t memorised them yet). And if I’m honest, focusing on something
smaller than fixing a country’s civic infrastructure is appealing for
now.
I’ll let you in on a secret; I didn’t join what became GDS because of
the GOV.UK project. I joined to work with friends I’d not yet had the
chance to work with and to see the inside of a growing organisation
from the start. I remember Tom Loosemore promising me we’d be 200
people in 3 years! As far as anyone knows we’re 650+ people. That’s
about a person a day for 2 years. I’m absolutely not saying that came
without a cost, but for me being part of that that was part of the
point - so I can be a little accepting with hindsight.
For me, apart from all the short term things (side-note: this job now
has me thinking £10million is a small amount of money and 2 years is a
small amount of time) there is one big mission:
That means in 10 years time experienced tech people from across the
country, as well as people straight from university, choosing to work
for government. Not just for some abstract and personal reason (though
that’s fine too), but because it’s a a genuinely interesting place to
work. That one’s on us.
I’m a big fan of OWASP ZAP or
the Zed Attack Proxy. It’s suprisingly user friendly and nicely pulls of
it’s aim of being useful to developers as well as more hardcore penetration testers.
One of the features I’m particularly fond of is the aforementioned
proxy. Basically it can act as a transparent HTTP proxy, recording the
traffic, and then analyse that to conduct various active security tests;
looking for XSS issues or directory traversal vulnerabilities for
instance. The simplest way of seeding the ZAP with something to analyse is using
the simple inbuilt spider.
So far, so good. Unfortunately ZAP isn’t designed to be used from the
command line. It’s either a thick client, or it’s a proxy with a simple
API. Enter Zapr.
Zapr is a pretty simple wrapper around the ZAP API (using the
owasp_zap library under the
hood). All it does is:
This is fairly limited, in that a spider isn’t going to work
particularly well for a mor interactive application, but it’s a farily good
starting point. I may add different seed methods in the future (or would
happily accept pull requests). Usage wise it’s as simple as:
That will print you out something like the following, assuming it finds
an issue.
The above alert is taken from a simple example,
using the RailsGoat vulnerable web
application as a scape goat. You can see the resulting output from
Travis running the tests.
Zapr is a bit of a proof of concept so it’s not particularly robust or
well tested. Depending on usage and interest I may tidy it up and extend
it, or I may leave it as a useful experiment and try and finally get ZAP
support into Gauntlt, only time will tell.
While at Craft I decided to have a quick look at
Consul, a new service discovery framework with
a few intersting features. One of the main selling points is a DNS
interface with a nice API. The Introduction
shows how to use this via the dig command line tool, but how do you use
a custom internal DNS server without modifying all your applications?
One answer to this question is
Dnsmasq.
I’m not explaining Consul here, the above mentioned introduction does a
good job of stepping through the setup. The following assumes you have
installed and started consul.
I’m running these examples on an Ubuntu 14.04 machine, but dnsmasq
should be available and packaged for lots of different operating
systems.
Once installed we can create a very simple configuration.
All we’re doing here is specifying that DNS requests for consul services
are to be dealt with by the DNS server at 127.0.0.1 on port 8600. Unless
you’ve changed the consul defaults this should work.
Just in case you prefer Puppet their is already a handy
dnsmasq module. The resulting
puppet code then looks like this.
The examples from the main documentation specify a custom DNS server for
dig like so:
With Dnsmasq installed and configured as above you should just be able
to do the following:
And now any of your existing applications will be able to use your
consul instance for service discovery via DNS.
I’ve been a big fan of Vagrant since it’s
initial release and still find myself using it for various tasks.
Recently I’ve been using it to test collections of Puppet modules. For a
single host
vagrant-serverspec is
excellent. Simply install the plugin, add a provisioner and write your
serverspec tests. The serverspec provisioner
looks like the following:
But I also found myself wanting to test behaviour from the host
(serverspec tests are run on the guest), and also wanted to write tests
that checked the behaviour of a multi-box setup. I started by simply
writing some Cucumber tests which I ran locally,
but I decided I wanted this integrated with vagrant. Enter
vagrant-cucumber-host.
This implements a new vagrant provisioner which runs a set of cucumber
features locally.
Just drop your features in the features folder and run vagrant
provision. If you just want to run the cucumber features, without any
of the other provisioners running you can use:
Another advantage of writing this as a vagrant plugin is that it uses
the Ruby bundled with vagrant, meaning you just install the plugin
rather than faff about with a local Ruby install.
A couple of other vagrant plugins that I’ve used to make the testing
setup easier are vagrant-hostsupdater
and vagrant-hosts. Both
help with managing hosts files, which makes writing tests without
knowing the IP addresses easier.
At the excellent London Devops meetup last week I asked what was
apparently a controversial question:
This got a few people worked up and I promised a blog post.
Note that I wrote a post listing lots of open source monitoring tools
not that long ago. And I’ve been to both the
Monitorama events about open source
monitoring. And have a bunch of Puppet modules for open source monitoring tools. I’m
a fan of both open source and of open source monitoring. Please don’t
read this as an attack on either, and particularly on the work of
awesome people working on  great open source monitoring products.
So what would it cost to get up and running with a state of the art
software as a service monitoring system? In order to do this we need to
choose our software. For this post that means I’m going to pick products
I’ve used (sometimes only a bit) and like. This isn’t a comprehensive
study of all the alternatives I’m afraid - though feel free to write
your own alternative blog posts.
In total that all comes to $35,080 (£20,922) per month, or
$420,960 (£251,062) per year.
Now the first reaction of lots of people will be that’s a lot of money
and it is. But remember open source isn’t free either. We need to pay
for:
I think people with the ability to build software tend to forget they
are expensive, whether as a contractor or as a full time member of
staff. And people without management experience tend to forget costs
like insurance, rent, management overhead, recruitment, etc.
And probably more important than these for some people we need to consider:
The time needed to put together a good monitoring stack based on for
instance logstash, kibana, riemann, sensu, graphite and collectd isn’t
small. And don’t forget the number of other moving parts like redis,
rabbitmq and elasticsearch that need installing configuring and
maintaining. That probably means compromising in the short term or
shipping later. In a small team how core is building your monitoring
stack to what you do as a business?
For some people, using a software as a service product just isn’t going
to cut it. Here’s a list of reasons I can think of:
I think everything else is a cost/benefit issue or personal preference
(or bias). Happy to add more to that list, but I don’t think it’s a very
long list.
I’ve purposefully not talked about the quality of the tools here, just
the cost. I’ve also not mentioned that it’s likely not an all or nothing
decision, lots of people will mix SaaS products and open source tools.
Whether taking a SaaS approach will be quicker, cheaper or better will
depend on your specific business context. But try and make that about
the organisation and not about the technology.
If you’ve never used the current crop of SaaS monitoring
tools (and not just the one’s mentioned above) then I think you’re missing
out. Even if you stick with a mainly open source monitoring stack you
might look at your tools a bit differently after you’ve experimented
with some of the commercial competition.
A little while ago I published a template writing your own puppet modules. It’s
very opinionated but comes out of the box with lots of the tools you
eventually find and add to your tool box. I’m posting this as it came
up at the recent Configuration Management Camp
and after discussing it I realised I hadn’t actually wrote anything
about it anywhere.
Obviously you can choose not to use parts of this, or even delete
aspects, but I find that approach much quicker than starting from scratch
or copying files from previous modules and changing names.
Simple. The following will install the module skeleton to
~/.puppet/var/puppet-module/skeleton. This turns out to be picked up
by the Puppet module tool.
With that in place you can then just run the following to create a new
module, where puppet-ntp is the name of our new module.
We use puppet module like this rather than just copying the files
because otherwise you would have to rename everything from class names
to test assertions. The skeleton actually contains erb templates in
places, and running puppet module generate results in the module name
being available to those templates.
Assuming you have run the above commands you should have a folder called
puppet-ntp in your current directory. cd into that and then install
the dependencies:
Bundler is a dependency manager for Ruby. If you
don’t already have it installed you should be able to do so with the
following:
Now you have the dependencies why not run the full test suite? This
checks syntax, lints the Puppet code and runs the unit tests.
Unit tests give fast feedback and help make sure the code you write is
going to do what you intend, but they aren’t actually applying the
manifests to a real machine. For that you want an integration test.
You’ll need Vagrant installed for this next
step. Lets run those as well with:
This will take a while, especially the first time. This uses Beaker to
download a virtual machine from Puppetlabs (if you don’t already have
it) and then brings up a new machine, applies a simple manifest, runs
the acceptance tests and then destroys the machine.
The CONTRIBUTING.md file has more information for running the test
suite.
I’ve recently added a Guardfile to
help with testing. You can run this with:
Now in a separate tab or pane make a change to any of the manifests. The
tests should run automatically in the tab or pane where guard is
running.
Probably. Although I started the repo a few other people have
contributed code or made improvements already. Just sent a pull request
or open an issue.
One of my favourite topics for a while now has been infrastructure as
code. Part of that involves introducing well understood programming
techniques to infrastructure - from test driven design, to refactoring
and version control. One tool I’m fond of (even with it’s potential to
be misused) is code coverage. I’d been meaning
to go code spelunking to see if this could be done for testing Puppet
modules.
The functionality is now in master for rspec-puppet
and so anyone feeling brave can use it now, or if you must wait for the
2.0.0 release. The actual implementation is inspired by the same functionality in
ChefSpec
written by Seth Vargo. Lots of the how came
from here, and the usage is very similar.
First add (or hopefully change) your Gemfile line item for rspec-puppet
to the following:
Then all you need to do is include the following line anywhere in a
spec.rb file in your spec directory.
Here’s an example module,
including a file called
coverage_spec.rb.
When running the test suite with rake spec you now get coverage
details like so:
Here’s the output on Travis CI as
well for a recent build.
I’ve already found coverage useful when writing tests for a few of my
puppet modules. The information about the total number of resouces is
interesting (and potentially an indicator of complexity) but the list of
untouched resources is the main useful part. These represent both
information about what your module is doing, and potential things you
might want to test.
I’m hoping to find some more time to make this even better, providing
more information about untouched resources, adding some configuration
options and hopefully to integrate with the Coveralls API.
As of a few weeks ago Test Kitchen has a shell provisioner as well as the original Chef provisioners. This opens up all sorts of interesting testing potential.
If you’ve not already seen Test Kitchen, probably because you’re not using Chef, it’s a tool for integration testing infrastructure code. Configured by a simple YAML file it will setup a matrix of virtual machines, using Virtualbox, AWS, OpenStack and more, run some setup code (normally applying Chef recipes) and then run a test suite (with support for Bats, ShUnit2, Rspec and Serverspec). It’s all very pluggable. With the addition of the shell provisioner it’s useful to just about anyone. To try and prove that here’s a hello world style example.
First we need to install Test Kitchen. We’ll use vagrant and virtualbox for our example too so we need a few extra dependencies. I’m going to assume you have bundler installed, if not you may be able to do so with gem install bundler but as the number of ways of setting a ruby environment up is greater than the number of people on the planet I’ll have to defer to instructions elsewhere for getting that far.
First create a file called Gemfile with the following contents:
Then run:
This should install the above software. Note that the shell provisioner is not yet in an official release so where installing direct from GitHub for the moment.
Next we’ll tell Test Kitchen what we want to do. As much for demonstration purposes I’m going to grab one of the Puppetlabs boxes. This is just plain Vagrant so feel free to substitude the box and box_url for alternatives you already have installed locally. Otherwise the first run will take a little longer as it downloads a large file.
Pull all of the following in a file called `.kitchen.yml’.
The shell provisioner is going to look for a file called bootstrap.sh by default. You can overide this but we’ll leave it for the moment. Our bootstrap script is going to do something very simple, install the ntp package. But the important part is it could do anything; run Salt, run Ansible, run Puppet, execute any arbitrary code we choose. In this case our script is completely self contained but if it needed some additional files we could put them in a directory called data and they would be copied to the newly created virtual machine under /tmp/kitchen.
The last step is to write a test. I’m suddently finding lots of excuses to use Serverspec so we’ll use that, but if you prefer you can use pretty much anything. The following file should be saved as  test/integration/default/serverspec/ntp_spec.rb. Note the default in the path which matches our suite above in the .kitchen.yml file. Test Kitchen allows for multiple suites all with separate tests based on a strong set of file path conventions.
With all of that in place we’re ready to run our tests.
This should:
The real power comes from doing this iteratively as you work on code, probably code more complex than a simple one-line bash script. You can also test across multiple virtual machines at a time, for instance different operating systems or different machine roles. The kitchen command line tool provides lots of help too, with the ability to login to machines, verify that specific combinations of platform and suite are working and print lots of diagnotic information to aid development.
Hopefully this will make it into a release soon, and we’ll see more involved examples using higher level tools and more documentation. But even now I’d be looking at Test Kitchen for any infrastructure testing you might be doing.
Packer provides a great way of describing the steps for creating a virtual machine image. But it doesn’t have a built-in way of verifying those images.
Serverspec provides a nice framework for writing tests against infrastructure, asserting the operation of services or the installation of packages.
I’m interested at the moment in building continous delivery pipelines for infrastructure components and have a simple working example of testing Packer with Serverspec on
Github. The example uses the AWS builder and the Puppet provisioner but the approach should work with other combinations.
This doesn’t represent a complete infrastructure pipeline, but it does demonstrate an approach to automating one particular component - building base images.
In our example I’m using the Puppetlabs NTP module to install and configure NTP. Once the Puppet provisioner has run, but before we build the AMI (or other virtal machine image) we run a test suite. For our example the tests are pretty simple:
If the tests fail, Packer will stop and the AMI won’t be built. The combination of storing the code (Packer template) alongside a test suite (Serverspec) and building a new AMI whenever you change the code, makes this setup perfect for continuous integration.
As an example of a continuous integration setup the repository contains a wercker.yml configuration file for the excellent Wercker service. Wercker makes setting up multi-step built pipelines easy and nicely configurable via a simple text file in your repository.
The Wercker build for this project is public. Currently the build involves downloading Packer, running packer validate to check the template and eventually running packer build to boot an instance and run our serverspec tests.
Originally written as part of Sysadvent 2013.
Writing automated tests for your code is one of those things that,
once you have gotten into it, you never want to see code without tests
ever again. Why write pages and pages of documentation about how
something should work when you can write tests to show exactly how something does work? Looking at the number and quality of testing tools and frameworks (like cucumber,
rspec, Test Kitchen,
Server Spec,
Beaker,
Casper and
Jasmine to name a few) that have
popped up in the last year or so I’m obviously not the only person who
has a thing for testing utilities.
One of the other things I am interested in is web application
security, so this post is all about using the tools and techniques
from unit testing to avoid common web application security issues. I’m
using Ruby in the examples but you could quickly convert these to other languages if you desire.
Lets start out with something simple. Accidentally exposing
applications on TCP ports can lead to data loss or introduce a vector
for attack. Maybe your main website is super secure, but you left the
port for your database open to the internet. It’s the server configuration equivalent of forgetting to lock the back door.
Nmap is a tool lots of people will be familiar with for spanning for
open ports. As well as a command line interface Nmap also has good
library support in lots of languages so lets try and write a simple
tests suite around it.
With the above code in place we can then write tests like:
We can run these manually, but also potentially as part of a
continuous integration build or constantly as part of a monitoring
suite.
We had to do quite a bit of work wrapping Nmap before we could write
the tests above. Wouldn’t it be nice if someone had already wrapped
lots of useful security minded tools for us? Gauntlt is pretty much just that, it’s a security testing framework based on cucumber which currently supports curl, nmap, sslyze, sqlmap, garmr and a bunch more tools in master. Lets do something more advanced than our port scanning test above by testing a URL for a SQL injection vulnerability.
The Gauntlt team publish lots of examples like this one alongside the source code, so getting started is easy. Gauntlt is very powerful, but as you’ll see from the example above you need to know quite a bit about the underlying tools it is using. In the case above you need to know the various arguments to sqlmap and also how to interpret the output.
Prodder is a tool I put together
to automate a few specific types of security testing. In many ways
it’s very similar to Gauntlt; it uses the cucumber testing framework
and uses some of the same tools (like nmap and sslyze) under the hood.
However rather than a general purpose security framework like Gauntlt,
Prodder is higher level and very opinionated. Here’s an example:
Alternative) Name Matches/
Renegotiations: Rejected”
This is a little higher level than the Gauntlt example — it’s not
exposing the workings of sslyze that is doing the actual testing. All
you need is an understanding of SSL certifcates. Even if you’re not an
expert on SSL you can accept the aforementioned opinions of Prodder
about what good looks like. Prodder currently contains steps and
exampes for port scanning, SSL certificates and security minded HTTP
headers. If you already have a cucumber based test suite (including
one based on Gauntlt) you can reuse the step definitions in that too.
I’m hoping to build upon Prodder, adding more types of tests and
getting agreement on the included opinions from the wider systems
administration community. By having a default set of shared assertions
about the expected security of out system we can more easily move onto
new projects, safe in the knowledge that a test will fail if someone
messes up our once secure configuration.
As well as trying out some of the above tools and techniques for
yourself I’d recommend encouraging more security conversations in your
development and operations teams. Here’s a few  places to start with:
Originally published on Medium.
We have a bunch of internal mailing lists at work, and on one of them someone asked:
I ended up writing a bit of a long reply which a few people found useful, so I thought I’d repost it here for posterity. I’m sure this will date but I think it’s a reasonable snapshot of the state of open source monitoring tools at the end of 2013.
Simply put, think about four elements and you won’t be far off on the
technical front. Miss one and you’re probably in trouble.
For logs, some combination of syslog at one end and elasticsearch and
Kibana at the other are probably the state of the open source art at
the moment. The shipping around is more interesting; Logstash is improving constantly, Heka is an similar alternative from Mozilla, and Fluentd looks nice too.
For pure metrics it’s all about Graphite, which is both awesome and
perilous. Not much else really competes in the open source world at
present. Maybe OpenTSB (is you’re into a Hadoop stack.)
For collecting metrics on boxes I’d probably look at collectd or diamond both of which have pros and cons but work well. Statsd is also useful here for different types of metric collection and aggregation. Ganglia is interesting too, it combines some aspects of the metrics collection tools with an integrated storage and visualisation tool similar to Graphite.
Monitoring checks is a bit more painful. I’ve been experimenting with Sensu in hope of not installing Nagios. Nagios works but it’s just a bit ungainly. But you do need somewhere to write checks against metrics or other aspects of your system and to issue alerts.
At this point everyone loves dashboards, and Dashing is particularly lovely. Graphiti and Tasseo for Graphite are useful too.
For bonus points things like Flapjack and Reimann provide some interesting extra capabilities around alert control or real time monitoring respectively.
And for that elusive top of the class grade take a look at Kale, which provides anomaly detection on top of Graphite and Elasticsearch .
You might be thinking that’s a lot of moving parts and you’d be right. If you’re a small project running all of that is too much overhead, turning to something like Zabbix might be more sensible.
Depending on money/sensitivity/control issues lots of nice and not so
nice commercial products exist. Circonus, Splunk, New Relic, Boundary and Librato Metrics are all lovely in different ways and provide part of the puzzle.
And that’s just the boring matter of tools. Now you get into alert design and other gnarly people stuff.
If you got this far you should watch all the Monitorama videos too.
Originally published on Medium.
I’m a big fan of the Platform as a Service (PaaS) model of operating web
application infrastructure. But I’m a much bigger user and exponent of
Infrastructure as a Service (IaaS) products within my current role
working for the UK Government. This post describes why that is, and
hopefully helps anyone else inside other large enterprise organisations
reason about the advantages and disadvantages, and helps PaaS vendors
and developers understand what I personally thing is a barrier to
adoption in that type of organisation.
A quick word of caution, I don’t know every product inside out. It’s
very possible a PaaS product exists that deals with the problems I will
describe. If you know of such a product do let me know.
PaaS products make for the very best demos. Have a working application?
Deployment is probably as simple as:
Your app has started to run slowly because visitors are flooding in?
Just scale out with something like:
The amount of complexity being hidden is astounding and the ability to
move incredibly quickly is obvious for anyone with experience of doing
this in a more traditional organisation.
Even small systems are often being built out of many small services
these days. Many large organisations have been up to this for a while
under the banner of Service Orientated Architecture. I’m a big fan of
this approach, in my view it moves operational and organisational
complexity back into the development team where its impact can often be
minimised by automation. But that’s a topic for another post.
In a PaaS world having many services is fine. We just have more
applications running on the Platform which can be independently scaled
out to meet our needs. But services need to communicate with each other
somehow, and this is where our problems start. We’ll keep things simple
here by assuming communication is over HTTPS (which should be pretty
typical) but I don’t think other protocols make the problem I have go
away. The same problem applies if you’re using a SaaS database for
example.
Over what network does my HTTPS internal service call travel? The
internet? The internal PaaS vendor’s network? If the latter, is my
traffic travelling over the same network as other clients on the
platform? Maybe I’m running my own PaaS in-house. But do I trust
everyone else in my very large organisation and want my traffic on the
same network as other things I don’t even know about? Even if it’s just
me do I want internal service traffic mixing with requests coming from
the internet? And are all my services created equally with regards what
they can and cannot access?
Throw in questions like: is the PaaS supplier running on infrastructure
provided by a public IaaS suppliers who you don’t have a relationship
with and you start to question the suitability of the current public
PaaS products for building secure service based systems.
You might be thinking, pah, what’s the worst that can happen? If you
work for a small company or a shiny startup that might be completely
valid. If on the other hand you’re working in a regulated environment
(say PCI) or dealing with large volumes of highly sensitive information
you’re very likely to have to build systems that provide layers of
trust, and to be doing inspection, filtering and integrity checking as
requests flow between those layers.
Imagine that I have a service dealing with some sensitive data. If I
control the infrastructure (virtualised or not, IaaS provided or not)
I’ll make sure that service endpoint isn’t available to anything that
doesn’t need access to it via my network configuration. If I’m being
more thorough I’ll filter traffic through some sort of proxy that does
checking of the content; It should be JSON (or XML), it should meet this
schema, It shouldn’t exceed this rate, it shouldn’t exceed this payload
size or response size, etc. That is before anything even reaches the
services application. And that’s on top of SSL and maybe client
certificates.
If I don’t control the infrastructure, for example when running on a
PaaS, I lose some of the ability to have the network protect me. I can
probably get some of this back by running my own PaaS on my own
infrastructure, but without awareness and a nice interface to that
functionality at the PaaS layer I’m going to lose lots of the benefits
of running the PaaS in the first place. It’s nice that I can scale my
application out, but if new instances can’t connect to the required
backend services without some additional network configuration that’s
invisible to the PaaS what use is that?
The question becomes; how to implement security layers within existing
PaaS products (without changing them). And my answer is “I don’t know”.
Yet.
SSL doesn’t help as much as you’d like to think here because if I’m an
attacker what I’m probably going to attack is your buggy code rather
than the transport mechanism. SSL doesn’t protect you from SQL injection
or unpatched software or zero-day exploits. If the only thing that my
backend service will talk to is my frontend application, an attacker has
to compromise two things rather than just ignore the frontend and go
after the data. Throw in a filter as described above and it’s really
three things that need to be overcome.
I think part of the solution lies in exposing some of the underlying
infrastructure via the PaaS interface. IaaS is often characterised as
compute, storage and network. In my experience everyone forgets the
network part. In a PaaS world I don’t want to be exposed to storage
details (I just want it to appear infinite and pay for what I use) or
virtual machines (I just care about computing power, say RAM, not the
number of machines I’m running on) but I think I do, sometimes, want to
be exposed to the (virtual) network configuration.
Hopefully someone working on OpenShift or CloudFoundry or Azure or
Heroku or DotCloud or insert PaaS here is already working on this. If
not maybe this post will prompt someone to do so.
I’ve become increasingly interested in web application security issues over the last year or so. Working in Government will do that to you. And I’ve come to the conclusion that a) there are lots of good open source security tools, b) many of them are terribly packaged and c) most developers don’t use any of them.
I’ve been having related conversations at recent events I’ve made it along to, including Devopsdays London which featured some good open spaces discussions on the subject. Security is one of those areas that, for many organisations, is basically outsourced to third party penetration testing firms or consultants. Specialists definitely have a role to play, but with a move towards increasingly rapid releases I think in-house security testing and monitoring is going to get more and more important.
I’ve started to build a collection of tools on GitHub, along with a vagrant setup to test them out. Full instructions are available on that repository but the short version is you can run one command and have one virtual machine filled with security testing tools and, if useful, another machine running a vulnerable web application with which to test. The current list of tools runs to:
But I’ll add more tools as I discover them or as people file issues or pull requests.
When I started investigating tools for security and penetration testing most roads led to Backtrack. This is a complete Linux distribution packed with a huge number of security tools, including many if not all of the above. Why then did I write puppet code rather than create a Vagrant box from Backtrack? Firstly, Backtrack is probably great if you’re a professional penetration tester, but the barrier to entry to installing a new distibution for most developers is too high in my view. And with a view to using some of these tools as part of monitoring systems I don’t always want a separate virtual machine. I want to be able to install the tools wherever I want. A good configuration management tool gives you that portability, and Vagrant gives you all the benefits of a local virtual machine.
As mentioned I’d like to expand how some of these tools are used to include automated monitoring of applications, maybe look at ways of extracting data for metrics or possibily writing a Sensu plugin or two. The first step to that is probably breaking down the monolithic puppet manifest into separate modules for each tool. Along the way I can add support for more operating systems as required. I’ve already done that for the wackopicko module which is up on the Forge.
I’m also soliciting any and all feedback, especially from developers who don’t do any security related testing but feel like they should.
I’ve not been writing many blog posts lately, but I have been doing quite a bit of writing elsewhere. One of the things I’ve had a hand in at work is the new Government Service Design Manual. This is the work of many people I work with as well as further afield. It’s intended to be a good starting place to find information about building high quality digital services.
The manual is in beta and we’re looking for as much feedback as possible on the whole thing. It’s already proving useful and a good way of framing the scope of discussions, but it has lots of room for improvement.
If you’re reading this post I’m going to wager you’re interest lies in or around devops flavoured content. The following are guides I’ve written in this area that I’d love any and all feedback on.
If you’re interested in the background to this endeavour then a couple of blog posts from some of my colleagues might be of interest too. First Richard Pope talks about how the manual came about and here’s a post from Andrew Greenway about this beta testing of the service standard.
The source for all this is on GitHub so if you prefer you can just sent a pull request. Or I’m happy to get emails or comments on this post. In particular if people have good references or next steps for these guides then let me know as several of them in particular are lacking in that area.
I had fun speaking at QCon in London earlier this month with a talk on the Cloud track entitled the Perils of Portability.
This had some Governmenty stuff in but was mainly part rant, part hope for the future of cloud infrastructure. I had some great conversations with people afterwards who felt some of the similar pain which was nice to know. I also somehow managed to get 120 slides into a 40 minute presentation which I think is a personal records.
The videos will be available at some point in the not too distant future too.
With only a week or so to go before the end of February, it’s looking like March might be a little busy.
If you’re going to be at any of these events (QCon and Devopsdays still have tickets available I think) then let me know.
About a month ago I had the good fortune of speaking at the London Web Performance meetup. This was one of the first talks I’ve done about our work at The Government Digital Service since the luanch of GOV.UK back in October. The topic was all about moving quickly in a large organisation (The UK Civil Service is about 450,000 people so I think it counts) and featured just a hand full of technical and organisational tricks we used.
I had great fun back in November at the QCon conference in San Francisco. As well as currating one of the tracks and catching up with people in the area I managed to give the following talk.
In hindsight it might have been a bit odd to try and cover both Rails and Django examples in the one presentation but it was quite good fun putting together code examples using both of them at the same time. As well as a large set of tips, tricks and tools I settled on a few things that I think any web (or other) framework should support out of the box.
I’m a big fan of system packages for lots of reasons and have often ended up rolling my own debian package repository at work, or working with others that have done so. Recently I finally got round to setting up a personal package repo, at packages.garethrushgrove.com. More interesting than the repo is probably the tool chain I used, oh and the rather nice bootstrap based styling.

The source code for everything is on GitHub although not much documentation exists yet. In the middle are a few shell scripts that generate the repo. Around them is a Vagrant box (which makes it easier to build packages for different achitectures or distros) and some Rake commands
The recipes commands allow for building new packages based on scripts. A few examples are included which use fpm, but you could use anything. The repo:build command triggers the debian repository to be rebuilt.
The vagrant configuration shares various folders between and guest and host which also opens up a few useful features. One is I can just drop any old debian package into the debs folder and run the repo:build command and it will be in my repository. The other useful capability is that the resulting repo is shared back to the host, which means I can then check it into Git and in my case push it up to Heroku.
I’ve been spending a bit of time recently pushing a few Puppet modules to the Forge. This is Puppetlabs attempt to make a central repository of reusable puppet modules. I started doing it as a bit of an experiment, to find out what I liked and what worked and I decided to writeup a few opinions.
So far I’ve shipped the following modules:
Quite a few of these started as forks of other modules but have evolved quite a bit towards being more reusable.
I’ve also started sending pull requests for modules that basically do what I want but don’t always play well with others.
It turns out the experience is mainly a pleasurable one, partly down to the much improved tooling around Puppet. Specifically I’m making extensive use of:
Lots of those tools make testing Puppet modules both easier and useful. Here’s an example of one of the above modules being tested. Note that it’s run across Ruby 1.8.7, 1.9.2 and 1.9.3 and Puppet versions 2.7.17, 2.7.18 and 3.0.1 for a total of 9 builds. Handily the Redis module mentioned also had a test suite. The pull request includes changes to that, and Travis automatically tested the pull request for the modules author.
Using modules from the Forge really forces you to think about reusability. The pull request mentioned above for the Redis module for instance replaced an explicit mention of the build-essential package with the “puppetlabs/gcc”: class from the Forge. This makes the module less self contained, but without that change the module is incompatible with any other module that also uses that common package. I also went back and replaced explicit references to wget and build-essential in my Riemann module.
As a rule of thumb. For a specific module only include resources that are unique to the software the module manages. Anything else should be in another module with a dependency in the Modulefile.
This can feel a little much when you’re replacing a simple Package resource with a whole new module but it has two advantages I care about. As well as the ability to use the module with other third party modules more easily it also makes it more likely that the module will work cross platform.
I’d like to see a few things improved when it comes to the Forge.
Last week we shipped GOV.UK. Over the last year we’ve built a team to build a website. Now we’re busy building a culture too. I’ve got so much that needs writing up about everything we’ve been up to. Hopefully I’ll make a start in the next week or so.
I’m all of a sudden adding lots more code to GitHub. Here’s the latest project, grok patterns for logstash. At the moment this repo only contains one new pattern but I’m hoping to add more, and maybe even for others to add more too.
First, a bit of background. Logstash is the excellent, open source, log agregation and processing framework. It takes inputs from various configurable places, processes them with filters and then outputs the results. So maybe you’ll take inputs from various application log files and output then into an elastic search index for easy searching, or output the same inputs to graphite and statsd to get graphs of rates. One of the host powerful filters in logstash is the grok filter. It takes a grok pattern and parses out information contained in the text into fields that can be more easily used by outputs. This post serves hopefully as both an explanation of why and an example of how you might do that.
Rails logs are horrible, that is until you install the excellent lograge output formatter. That gives you lines like:
This contains loads of useful information that’s easily parsable by a developer. We have the HTTP status code, the rails controller and information about response time too. A grok filter lets us teach logstash about that information too. The working grok filter for filtering this line looks like this:
That was worked out pretty much with a bit of trial and error and use of the logstash java binary, using stdin and stdout inputs and outputs. It works but getting their wasn’t that much funand proving it works outside a running logstash setup was tricky. Enter rspec and the grok implementation in pure Ruby. The project above contains an Rspec matcher for use when testing grok filters for logstash. I’ll probably extract that into a gem at some point but you’ll get the idea. Now we can write tests like these:
The tests themselves are just basic Rspec with most of the work done in the custom matcher. This not only means I can be a bit more confident that my grok pattern works, it also provides a much nicer framework for writing more patterns for other log formats. Parsing rules like this are one area where test driven development is a huge boon in my experience. And with tests comes continuous integration, in this case via Travis.
I’ll hopefully find myself writing more patterns and tests for them, and if anyone wants to send pull requests and to start collecting working grok patterns together so much the better.
Thanks to an errant tweet I started playing with Riemann again. It ticks lots of boxes for me, from the clojure to configuration as code and the overloadable dashboard application. What started as using Puppet and Vagrant to investigate Riemann turned into a full blown tool and module writing exercise, resulting in two related projects on GitHub.
I like this combination, a separate Puppet module along with a vagrant powered test bed. I’ve written a reasonable rspec based test suite to check the module but it’s always easier to be able to run vagrant provision as well to check everything is working. This also turned out to be the perfect opportunity to use Librarian-Puppet to manage the dependencies and eventually to ship the module to the Puppet Forge.
A few weeks ago now Vagrantbox.es (a website I maintain for third party hosted Vagrant base boxes) dissapeared from the internet for a few days. This was completely my fault, the (lovely) hosting people ep.io had unfortunately closed down the service they had in beta and I’d been so busy that I hadn’t had chance to move it elsewhere.
The original version of the site (I had the code and good backups of the data) was a pretty simple Django application, but I’d used it to experiment (read over-engineer) with various bits of tech including Varnish, Solr, some ORM caching and lots more. This had been great, but it made it less portable. I had everything described in Puppet, but with virtually no spare time I decided to go a different route.
I threw a flat version of the site up on GitHub, served it using Nginx on Heroku and added a quick Fork me on GitHub badge to the top. Suggest a box moved from being a web form to a pull request. It’s fair to say I did this pretty quickly and made a good few typos on the way. But within a couple of weeks I’ve had 8 pull requests either fixing my bugs, removing dead boxes and adding new ones.
What I’m going to take from this is, if you’re building a community project that’s aimed at developers, then throw the content on GitHub. In my case I have the entire site on there too but I think that’s secondary. Pull requests are much better than any content management system or workflow you’re likely to build, and even more importantly the time to implement something drops hugely.
With all the spare time I don’t have I’ll be thinking about a content management model using GitHub for content, pull requests for workflow and post commit hooks for loading that content into a site or service somewhere.
I have a few static sites on Heroku but in one case in particular I already had quite an involved nginx configuration – mainly 410s for some previous content and a series of redirects from older versions of the site. The common way of having static sites on Heroku appears to be to use a simple Rack middleware, but that would have meant reimplementing lots of boring redirect logic.
Heroku buildpacks are great. The newer cedar stack is no longer tied to a particular language or framework, instead all of the discovery and knowledge about particular software is put into a buildpack. As well as the Heroku provided list it’s possible to write you’re own. Or in this case use one someone has created earlier.
I’ve just moved Vagrantbox.es over to Heroku due to the closure of a previous service. In doing that, instead of the simple database backed app, I’ve simply thrown all the content onto GitHub. This means anyone can fork the content and send pull requests. Hopefully this should mean I pay a bit more attention to suggestions and new boxes.
The repository is a nice simple example of using the mentioned Heroku Nginx buildpack too. You just run the following command to create a new Heroku application.
And then in typical Heroku fashion use a git remote to deploy changes and updates. The repository is split into a www folder with the site content and a conf folder with the nginx configuration. The only clever parts involve the use of an ERB template for the nginx configuration file so we can pickup the correct port. We also use 1 worker process and don’t automatically daemonize the process – Heroku deals with this itself.
Several things seemed to come together at once to make me want to hack on this particular project. In no particular order:
The Thoughtworks Technology Radar said the following:
I’ve been getting more interested in JRuby anyway, partly because we’re finding ourselves using both Ruby and Scala at work, and maintaining a single target platform makes sense to me. Throw in the potential for interop between those languages and it’s certainly worth investigating.
Play 2.0 shipped and currently only provides the ability to create a self contained executable with bundled web server. Creating WAR files for more traditional application servers is on the roadmap but interestingly wasn’t deemed essential for the big 2.0 release. I had a nice chat with Martyn Inglis at work about some of the nice side effects of this setup.
And throw in every time I have to configure straight Ruby applications for production environments I get cross. I know where all the bits and pieces are buried and can do it well, but with so many moving parts it’s absolutely no fun whatsoever.
Warbler, the JRuby tool for creating WAR files from Ruby source, has just added the ability to embed Jetty to the master branch.
I decided to take all of this for a quick spin, and the resulting code is up on GitHub.
This is the simplest Rack application possible, it just prints Hello Jetty. And the README covers how to install and run it so I won’t duplcate that information here.
But I will print some nearly meaningless and unscientific benchmarks because, hey, who doesn’t like those?
Running the same test on the same machine but using Ruby 1.9.2-p290 and Thin gives.
2736 requests per second on JRuby/Jetty vs 1600 on Ruby/Thin. As noted this isn’t meaningfully useful, in that it’s a hello world example and I’ve not tried to pick the fastest stacks on either side. I’m more bothered about it not being slower, because the main reason to pursue this approach is simplicity. Having a single self contained artefact that contains all it’s dependencies including a production web server is very appealing.
I’m hoping to give this a go with some less trivial applications, and probably more importantly look to compare a production stack based around these self-contained executables vs the dependency chain that is modern Ruby application stacks.
Thanks to Nick Sieger for both writing Warbler and for helping with a few questions on the JRuby mailing list and on Twitter. Thanks also to James Abley for a few pointers on Java system properties.
I’ve been pretty busy with all things GOV.UK recently but I’ve managed to get a few bits of unrelated code up and a few talks in. I’m still pretty busy so here’s a list of some of them rather than a proper blog post.
After someone bugged me on Twitter I realised the small bit of code we’ve been using for our Nagios dashboard wasn’t out in the wild. So introducing Nash, a very simple high level check dashboard which screenscrapes nagiosand runs happily on Heroku.
Although I’ve not been writing too much on here I’ve been keeping Devops Weekly going each week for over a year now. I’ve just crossed 3000 subscribers which is pretty neat for a pet project.
This is a bit of a cheat blog post really. I’ve been crazy busy all month with little time for anything except work (specifically shipping the first release of www.gov.uk). I have had a little time to blog over on the Cabinet Office blog though, about work we’ve done with dashboards.
http://digital.cabinetoffice.gov.uk/2012/02/08/radiating-information/
If you’re ever looking for good little hack projects dashboards are perfect, and often hugely useful once up and running. Convincing people of this before you have a few in the office might be hard – so just build something simple in a lunch break and find a screen to put it on. We’ve had great feedback from ours, both from people wandering through the office and from our colleagues who have a better idea of what’s going on.
In what turned out to be a productive holiday hacking with languages I’d not used before, I got round to writing some coffeescript on node.js. This was more to do with scratching a personal itch that pure experimentation. I had a play with Janky (Github’s Jenkins/Hubot mashup) but found it a little opinionated on the Jenkins side, but the campfire integration is excellent. Looking at the Jenkins commands in hubot-scripts though I found those even more opinionated.
The magic of open source though is you can just fix things, then ask nice people if they like what you’ve done. I set about writing a few more general commands and lo, the’ve been quickly merged upstream.
These add:

This was made much easier by first looking at the previous Jenkins commands, and then looking at other scripts in the hubot-scripts repository. The best way of learning a new language/framework is still on the shoulders of others.
I’ve got a few other good ideas for Jenkins related commands. I want to add a filter command to the jobs list, both by name and by current state. For longer running jobs I also want to report whether a build is currently running. And then maybe get information about a specific job, like the last few runs or similar. Any other requests or ideas most welcome.
For running ad-hoc commands across a small number of servers you really can’t beat Fabric. It requires nothing other than ssh installed on the servers, is generally just a one-line install and requires next to no syntaxtic fluff between you and the commands you want running. It’s much more of a swiss army knife to Capistranos bread knife.
I’ve found myself doing more and more EC2 work of late and have finally gotten around to making my life easier when using Fabric with Amazon instances. The result of a bit of hacking is Cloth (also available on PyPi). It contains some utility functions and a few handy tasks for loading host details from the EC2 API and using them in your Fabric tasks. No more static lists of host names that constantly need updating in your fabfile.
Specifically, with a fabfile that looks like:
You can run:
And get something like:
And then you could run:
And you’d happily get the load averages and uptime for all your EC2 instances.
A few more tricks are documented in the GitHub README, including filtering the list by a regex and some convention based mapping to Fabric roles. I’ll hopefully add a few more features as I need them and generally tidy up a few things but I’m pretty happy with it so far.
I nearly always try and grab some time over Christmas to try something new and this year I’d been planning on spending some time with Clojure. I have several friends who are big fans, but dipping in and out of a book hadn’t really worked. What I needed was an itch to scratch.
I stuck with a domain I’m pretty familiar with for this first project, namely building a little web application. It renders a web page, makes HTTP requests, parses JSON into native data structures and does a bit of data juggling. Nothing fancy or overly ambitious, I was mainly interested in picking up the syntax, understanding common libraries and getting something built. Here’s what I’ve got:

Jenkins has various API endpoints, but most dashboards I’ve seen concentrate on showing you the current status of all the builds. This is hugely useful when it comes to the simple case of continuous integration, but I’ve also been using Jenkins for other automation tasks, and making extensive use of parameterized builds. What the dashboard above concentrates on is showing recent builds for a specific job, along with the parameters used to run them.
Overall it was a fun project. Clojure made much more sense to me building this application than it had from simple examples. The Noir web framework is excellent and proved easy enough to jump into and simple enough that I could read the source code if I was interested in how something worked. The Leiningen build tool made getting started, downloading and managing dependencies and running tests and the application itself easy.
What I didn’t find particularly appealing was the lack of a strong standard library coupled with the difficulty of tracking down suitable libraries. JSON parsing, dealing with HTTP requests and date handing are very common activities in web programming and all needed me to jump around looking at the best way of dealing with the common case. I settled on clj-http, chesire and using Java interop for date formatting. clj-http suffered from having lots of forks on GitHub to navigate through. I started with clojure-json before discovering it had been deprecated. And neither clj-time or date-clj appeared to support unix timestamps as far as I could tell from the source. Throw in some uncertainty over the status of clojure-contrib that isn’t well documented on the official site and it needs some effort to get started.
The working code for this is already up on GitHub and I’d be interested in any Clojure experts showing me the error of my ways.
I’m not sure how novel this approach is but a few folks at work hadn’t seen it before so I thought it worth jotting down.
If you have even a small but dynamic set of servers then a problem arises with how those nodes are defined in puppet. A node remember is defined in puppet like so:
The problem is twofold. If you have a growing infrastructure, that list of nodes is going to get quickly out of hand. The other problem is around provisioning new hosts, the obvious approach to which is something like:
1. Summon new EC2 instance
2. Change the node definition to include the new hostname
3. Install puppet on instance and so the ssl certificate signing dance
4. Run puppet
Step 2 stands out. The others are easily automated, but do you want to automate a change to your puppet manifests and a redeploy to the puppetmaster for a new instance? Probably not.
Puppet has the concept of an external node classifier which can be used to solve this problem, but another simpler approach is to use an environment variable on the new machine.
Lets say we define our nodes something like this instead:

If a machine runs and sets the $machine_role variable to frontend it includes the web_server class, if that variable equals ‘data’ it’s going to include the db_server class instead. Much cleaner and more maintainable in my view. Now to set that variable.
Facter is the tool used by Puppet to get system information like the operating system or processor count. You can use these facter provided variables anywhere in your manifests. And one way of adding a new fact is via an environment variable on the client. Any environment variable prefixed with FACTER_ will be available in Puppet manifests. So in this case we can:
So our steps from above become something like:
1. Summon new machine
2. echo “export FACTER_machine_role=backend” >> /etc/environment
3. Install puppet on instance and so the ssl certificate signing dance
4. Run puppet
Much easier to automate. And if you’re looking at a box and want to know what it’s role is you can check the relevant environment variable.
I’m a huge Jenkins fan now, but that wasn’t always the case. I started (and still have a soft spot for) Cruise Control, mainly building .NET and PHP applications. I then jumped to much simpler projects like Integrity mainly for Python and Ruby projects. I reasoned I didn’t need the complexity of Cruise or Hudson, I just wanted to be able to run my tests on a remote machine and have something go green or red. I then worked out that wasn’t quite the case, and ended up committing webhook like functionality to Integrity so I could chain builds together. And then I eventually tried Jenkins and found it’s power mixed with flexibility won me over. That’s really all just context, but hopefully explains a little about why I like a few Jenkins features in particular, one of which is Parameterized builds.
The Jenkins wiki describes this by saying:
But then goes onto a usecase that probably won’t mean much to dynamic language folk. This is one failing of much of the documentation around Jenkins, it often feels geared towards developers of certain languages when in reality the tool is useful everywhere. The important take away here is that builds can take arguments, which can have default values. Here’s an example:
Imagine we have a build which runs a set of simple tests against a live system . And further imagine that said system is composed of a number of different web services. Oh, and we’re running a few different parrallel versions of the entire system for testing and staging purposes. We could have one Jenkins job per application/environment combination. Or we could have one parameterized build.
Lets first specify our parameters from the Configure build screen of our Job.

Here we’re specifying a TARGET_APPLICATION and TARGET_PLATFORM parameter. These are going to turn into environment variables we can use in our build steps. We can specify default values for these if we like too. I’m just using strings here, but I could also use a select box or file dialog, or other options provided by various plugins.
Now when we hit the build button, instead of the build just starting, we’re propted for these values.

So with our new build if we want it to run against the staging environment and just for the foobar application we enter those values and hit build. That on it’s own can be used to drastically cut down on the number of individual builds you have to manage in Jenkins. And we’re not just restricted to text inputs, we can use boolean values or even prompt for file uploads at build time. But throw in a few plugins and things get even more interesting.
Jenkins has an overwhelming number of plugin available. If you haven’t spent the good half hour it takes to go down the list I’d highly recommend it. One of Jenkins best features is the ability to trigger a build after the successful run of another job. It allows you to chain things like test runs to integration deployments to smoke tests to production deploys. Basically your modern continuous deployment/delivery pipeline. The Build Pipeline plugin is excellent for visuallising this and introducing human gates if needed. Another useful plugin in this context is the Parameterized Trigger plugin. A limitation of Jenkins is that downstream builds can’t pass parameters, but this plugin works around that. Instead of ticking the Build other projects option you go for the Trigger parameterized build on other projects box. This allows you to select the project and to specify parameters to pass. This could be hard coded values, paramaters already passed into the pipeline, or things from other plugins like the git sha1 hash or subversion version number.

Combine all this together and it’s simple to have a per project continuous integration build running a test suite, kicking off a standard set of downsteam jobs for deploying to a test environment (by passing th relevant parameters), running some basic smoke tests and allowing someone to click a button to deploy to production. Or going the whole continuous deployment, I trust my test suite route, and deploying automatically. All within Jenkins. Getting this working requires a bit of planning. You want all of your projects to be deployed the same way but you probably want this to be the case anyway.
Providing flexible push button builds/deploys and reducing the number of nearly identical jobs in Jenkins are just two advantages to using parameterized builds. Most of the tricks come from thinking about Jenkins as much more than a continuous integration tool and more of an automation framework – I know at least one large organisation who have pretty much replaced cron for many tasks with Jenkins for instance. Running tests automatically, and in a central environment as close to production as possible, is important. But it’s just a sanity check if you’re doing everything right already. Centralising activity on a build pipeline requires you to be doing all that anyway, but in my opinion gives way more useful and rapid feedback about the state of the code your team is writing.
I don’t appear to have been in a writing mood recently but I’ve been getting back into hacking on a couple of pet projects. The first fruits of this coding (mainly backwards and forwards on the train) I’ve just made available to anyone interested.
Web Facter is a gem which takes the output from Facter and exposes this as JSON over HTTP. In theory you could run this on a configurable port on each of your machines and have a URL you can hit to get information on uptime, networking setup, hostnames or anything else exposed by Facter. It comes with a simple built-in web server and optional http basic authentication if you’re not going to do this via a proxy. The JSON display should be both human and machine readable, and I have a few ideas for projects which needed this information.
The other project is very similar, and even has a similar name, Web Puppet. You can run this on your puppet master and it exposes the node information (currently including the facts and tags) again as JSON over HTTP. I’m still working on this to make it a little more usable. At the moment it just shows you all nodes and all information, if you’re working with a larger cluster this isn’t really sensible. Recent versions of Puppet do have an HTTP based API but it requires some hoops to be jumped through and I’m not quite sure from the docs it lets me do what I want (I have a specific usecase, of which more soon all being well).
Both projects have had me reading the source code of Puppet and Facter, which for the most part has been enjoyable and informative. Puppet in particular has some great comments lying around :) Both of the above projects are available as gems for anyone else to play around with and build on, but my main aim is a little more high level. All being well I’ll have a couple of projects built atop these APIs shortly.
We’re pretty fond of Mongodb at work and I’ve been getting an opportunity to kick some of the more interesting tyres recently. I thought I’d document something I found myself doing here, half hoping it might be useful for anyone else with a similar problem and also to see if anyone else has a much neater approach. The examples are obviously pretty trivial, but hopefully you get the idea.
So, we’re making using of the rather nice Mongoid Ruby library for defining our models as Ruby classes. Here’s a couple of very simple classes. Anyone familiar with DataMapper or Django’s ORM should be right at home here.
So we now have a good few publications and longer publications in our system. And folks have been creating sections with wild amandon. What I’d like to do now is do some reporting, specifically I want to know the numbers of Publications by type and publication status. And lets allow a breakdown by section while we’re at it.
One approach to this is using Mongo’s built in map-reduce capability. Mongoid exposes this pretty cleanly in my view, by allowing you to write the required javascript functions (a mapper and a reducer) inline in the Ruby code. This might feel evil, but seems the best of the available options. I can see for much larger functions that splitting this out into separate javascript files for ease of testing might be nice, but were you can just test the input/output of the whole job this works for me.
In our case that will return something like the following, or rather more specifically it will return a Mongo::Cursor that allows you to get at the following data.
I’ve been pretty impressed with both Mongo and Mongoid here. I like the feel of mapreduce jobs for this sort of reporting task. In particular it’s suprising how writing two languages mixed together like this doesn’t really affect the readability of the code in my view. Given that with a relational database you’d probably be writing SQL anyway maybe that’s not that suprising – the syntactic differences between Javascript and Ruby are much smaller than pretty much anything and SQL. Lots of folks have written about the increase of polyglot programming, but I wonder if we’ll see an increase in the embedding of one language in another?
I’ve been playing with Rundeck recently. For those that haven’t seen it yet it’s an application for running commands across a cluster of machines and recording the results. It has both a command line client and a very rich web interface which boths allows you to trigger commands and shows the results.
I’ve played with a few different jobs so far, including triggering Puppet runs across machines triggered by a Jenkins plugin. I’ve also been looking at running all my monitoring tasks at the click of a button (or again as part of a smoke test triggered by Jenkins) and I thought that might make a nice simple example.
My checks are written as Nagios plugins, and run periodically by Nagios. I also trigger them manually, using Dean’s NRPE runner script.

The above shows a successful run across a few machines I use for experimenting with tools. Hopefully you can see the summary of the run on each of the four machines, each ran five NRPE checks and all passed. On failure we’d see the results as well as different symbols and colous. We can easily save the output to a file if we need to, rerun or duplicate the job (maybe to have it run against a different group of machines) or we can export the job definition file to load into another instance.
The same job can also be run on the command line (which makes use the of Rundeck API)
This example shows running a specific pre-defined job, but it’s also equally possible to fire of adhoc commands to some or all of the machines rundeck knows about.
One thing in particular that I prefer about this approach to say using Capistrano or Fabric for remote execution tasks is that you have a centralised authentication and logging capability. It would be easy enough to encapsulate the jobs into cap or fabric tasks (and manage that in source control) which means you’re not stuck if Rundeck isn’t available.
This blog post is mainly an excuse to use the pun in the title. It’s also an opportunity to tell folks that don’t already know I’ll be starting a new job on Monday working for the UK Government. I’m going to be work for the Government Digital Service, a new department tasked with a pretty wide range of sorting the Government out online.
The opportunity is huge. And when it came around I couldn’t turn it down. I’m going to be working with a bunch of people I’ve known and respected for a while, as well as other equally smart people. That means I’m going to be back in London again as well.
Hopefully I’ll be able to talk lots about what we’re up to. The groundwork for that has already been laid by the alpha.gov team who have been blogging furiously about topics of interest.
I stepped in at the last minute to do a talk at the last London Ruby User Group. From the feedback afterwards folks seemed to enyoy it and I certainly had fun. Thanks to everyone who came along.
     
As well as the slides the nice Skills Matter folks have already uploaded the videos from the night.
I’ve spend a bit of time this weekend cleaning, tidying and upgrading software on my mac. While doing that I got round to compiling my own Vim. I’d been meaning to do this for a while, I prefer using Vim in a terminal to using MacVim, and I like having access to things like Command-T which requires Ruby support which the inbuild version lacks.
Vim isn’t in Homebrew, because Homebrew’s policy is to not provide duplicates of already installed software. Enter Homebrew Alt which provides formulas for anything not allowed by the homebrew policy. As luck would have it a Vim Formula already exists. And installing from it couldn’t be easier.
As it turns out this failed the first time I ran it because I had an rvm installed Ruby on my path. I reset this to the system version and everything compiled fine.
Note also that it’s really quite simple to use a different revision or different flags when compiling. Just download that file, modify it, serve it locally (say with a python one line web server) and point brew install at it. Next step, running off head for all the latest and greatest Vim features.
The idea of a build pipeline for web application deployment appears to have picked up lots of interest from the excellent Continuous Delivery book. Inspired by that, some nice folks have build an excellent plugin for Jenkins unsurprisingly called the Build Pipeline Plugin. Here’s a quick example of how I’m using it for one of my projects*.

The pipeline is really just a visualisation of up and downstream builds in Jenkins given a starting point, plus the ability to setup manual steps rather than just the default build after ones. That means the steps are completely up to you and your project. In this case I’m using:
I’m triggering builds on each commit too via a webhook. But I can also kick off a build by clicking the button the pipeline view if I need to.

Note that I’m only allowing the last build to be deployed given only that one can be checked on staging. Again this is configuration specific to my usage, the plugins lets you operate a number of different ways. There are a number of tweaks I want to make to this, mainly around experimenting with parameterized builds to pass useful information downstream and even allow parrallel execution. For the moment I have the Block build when upstream project is building flag checked on the deploy.
I did a quick lightning talk at the Refresh Cambridge meetup last night, a very quick introduction to Varnish. Given 10 minutes all I really wanted to do was get people to go away and take a look at it. Lots of folks in the room hadn’t come across it before so I think the talk was hopefully well pitched.

Several people asked about slighly dynamic pages and I only got chance to mention support for ESI (Edge Side Includes) at the end. Conversation afterwards turned to various parts of the modern web stack and I had a pretty good time being opinionated. Hopefully more of the same next month.
As Django has matured it’s being used for bigger and bigger projects. At the same time it’s also being used by more and more people building relatively simple applications quickly. Everyone’s application is different, but I’d wager the vast majority of these have a range of common performance problems. Performance is often something only larger teams get to spend time really getting to grips with. This is sometimes because smaller projects can’t afford the time, or more often probably that things are thought to be fast enough anyway.
One advantage of using a framework is the sharing of common solutions to common problems that happens as a community forms. In what is hopefully going to be a bit of a series I’m going to cover some simple things everyone can do to improve application performance. The patterns are generally applicable, but I’m going to focus on Django examples.
I’m going to be pretty opinionated about the stack I’m using when necessary. I’m not looking to compare different web servers or databases or python versions. And I’d rather give concrete examples than generalise. If you’re using a different stack that’s fine, somethings will just work and others will need you to know how to configure the software you’ve already chosen. I’m also going to focus on a very small and simple to understand application. Most of these techniques scale up just fine, but I feel people don’t often use them on smaller projects because they thing you can only use them on larger ones. Or that you won’t see much impact on a smaller project. Both of these don’t ring true in my opinion and I’ll hopefully show why.
In this first part of the series lets take a quick detour to frame everything else. Lets talk about ways we can measure performance so we can see if the changes we’re making have the desired impact. If you’re not measuring performance already then start here.
We’ll start out looking at a few tools which are useful when taking a view by view approach to analysing performance. These generally ignore the impact of load on the system but because of this are generally easier to understand and read.
Most Django developers will hopefully already be using the excellent Debug Toolbar. It has a number of features relevant to our current quest but the most interesting is the query count. Less queries is nearly always better. That’s a whopping generalisation, but looking for unnecessary queries or duplicated queries or particularly slow running queries is a great way of making your application faster. The ORM makes it pretty easy to end up with a querysplosion if you’re not paying attention.
It’s very simple to install:
The query section shows you the number of queries, the individual queries themselves and the time taken. It’s designed to be run in debug mode, so the actual query times will likely be lower in production, but the query that’s taking ages in development will probably still be slow when you go live.

YSlow is a browser extension for Firefox and Chrome that gives information and recommendations about a number of mainly HTTP, CSS or javascript issues individual pages might have. It will give you a score as well as suggestions for improvement:

Also useful is the break down of the number of HTTP requests, and the affect of a primed cache on page loading.

Sometimes you want to know the very low level calls that go into making a page render. For this you’ll want to look at profiling tools. The Django wiki has a useful page on profiling which is good if dispiriting reading.
Django Snippets has several profiling middleware, one of which is packaged up in the excellent Django Snippetscream. We can install that like so:
Just include the middleware in your debug environment:
You can then append ?prof to any of your URLs and instead of seeing the view you’ll see something like the following:

Look at where your code spends it’s time and whether you have repeated calls to the same methods and functions. Sometimes getting down to this level of detail is the easiest way of finding the bottleneck in your application.
Here’s the first time I’m being opinionated about the stack, by choosing Nginx as my frontend server. I’ll talk a little about why this is a good idea later, but for the moment lets concentrate on why this is useful for measuring performance.
Log files are wonderful things, and Nginx has quite a powerful syntax for adding extra information to log files. Note the last line in the definition below.
We are adding the entire request time, the time taken by the upstream server (in my case gunicorn) to respond and also the gzip ratio. This is really handy if you’re optimising an application already in production. By collecting this data here it’s easy to then analyse the logs and pull out things like slow urls or assets not being gzipped effectively.
Very similar to the above nginx logging, but implemented as a django 1.3 application (so it can be used in development as well) is one of my projects, django-timelog. As well as logging the time taken for each request, django-timelog provides a management command to analyse the resulting log file. It produces output which can show in aggregate the average response time of either views or individual URLs.
It’s packaged so installation should be straightforward.
Again this can be used in a production environment, or it can be used locally while developing. You can also use load testing tools as described in a moment to generate traffic which is then logged.
I’m mainly looking for a tool here which can easily generate HTTP traffic in volume, sending a decent number of concurrent requests against your application and returning some useful results. I mainly turn to ab (Apache bench) because it’s available everywhere and it’s very simple to use.
For example lets hit a site with 100 requests, with requests being sent in batches of 5.
This will print something like the following. For our purposes we’re mainly interested in the requests per second value and the mean request time.
Load testing is a pretty large topic. For instance even with the above simple example how do we know if 100 requests is enough? (it’s not.) Or whether a concurrency of 5 is useful? Often what you’re interested in is where your application starts to saturate or where it starts to error. But even without getting bogged down in the details a simple test like this can show changes have had a positive or negative effect. I’ll show examples of this as we investigate optimisation techniques.
If you’re working on a larger project hopefully you’ll have the time to investigate other approaches too. I’m quite a fan of using production logs to replay requests for instance, and of using Funkload for running scenarios under load. I’ll hopefully write more about those later. I’ve heard good things about Tsung as well, HTTPerf is excellent and JMeter has many fans. I’m using ab for examples because it’s point and shoot and you probably already have it installed without knowing.
Hopefully that’s a useful list of tools to get a baseline of where you’re at with performance. The rest of the articles in this series will show approaches to improve performance, and come back to one or more of these tools to confirm we’re heading in the right direction.
So I’m a huge Ganglia fan. It’s my go-to tool for standard low level metrics and for more ad-hoc higher level stuff as well. So I’m very happy to see the newly released version of the Ganglia Web interface.
Here’s the old look and feel:

And here’s the new version:

The first thing that stands out to me is the wide range of new options. We have a navigation bar with items like search, views, agregate graphs, events and mobile. And we have dynamic date ranges, and filtering and sorting options for metrics.
I’ve concentrated here on the new visual features, but the new UI also contains CSV and JSON output for all metrics as well as a much easier JSON based approach to defining custom metrics. The amount of stuff in this release is huge. Massive thanks to the folks behind all the new features, in particular Vladimir Vuksan.
Ganglia is such a staple that I’m used to just using whatever is in the latest distro provided packages. When it comes to the web UI however I’m going to make an exception from now on. Not only can you run the new UI in parallel with the old, deployment is simply copying a directory into place (It’s easy to forget just how simple PHP deployment is sometimes). If you’re already using Ganglia spend a few minutes installing the new UI. If you’ve rejected Ganglia previously because it didn’t have one feature or another then now is the time to look again.
I’ve been doing some basic performance profiling work with Ruby on Rails recently and one tool I found very useful was the request log analyzer. It’s a relatively simple command line application that you can point at the Rails application log files and it outputs lots of information in agregate. So information about request duration averages or about SQL queries run. When working on a recent Django project I wanted a tool to do the same thing and ended up writing timelog.
I did a bit of research to see if I could find something similar. Here are a few other interesting tools that didn’t quite do what I wanted:
Timelog is very similar to the middleware in Zamboni, the only real difference being I’m using the new logging support in Django 1.3. More interesting is the bundled management command which will output something like:
At the moment I’ve not done any real benchmarking or optimisation of the script, but it will chew through a 300,000 line (20MB) log file in under 20s on my aging macbook.
The code for Timelog is on github at github.com/garethr/django-timelog and I’ve uploaded to PyPi as well at pypi.com/django-timelog. You can install it with the usual tools like pip:
Once installed you need to do a little configuration to get things working. First add the middleware to your MIDDLEWARE_CLASSES in your settings file.
Next add timelog to your INSTALLED_APPS list. This is purely for the management command discovery.
Then configure the logger you want to use. This really depends on what you want to do, the django 1.3 logging setup is pretty powerful. Here’s how I’ve got logging setup as an example:
In order for the analyser script to work correctly you’ll need to use the plain formatter and to register a handler for the timelog.middleware logger.
With that all configured try hitting your application a few times. You should see a log file created at the location specified in TIMELOG_LOG. Fill that up with a few lines and then run the analyze_timelog management command:
This should output something similar to the above table but with your timings and views. The management command currently allows you to point the analyzer at a different file say from a backup, or a file you’ve pulled down from production but want to run the command locally. I’ll likely add some filters as time permits for things like not showing all views or showing only views between a given date range.
The above table showing the view function is good for big picture trends, but if you want to dig down into a particular path then you can pass an argument to not reverse the path:
This should give you something more like:
I’ve been using the Vagrantbox.es site as a bit of a playground recently and I’ve been meaning to blog about some of the overengineering I’ve been doing. Here’s a smaller starter.
Getting the headers returned by your web server correct is both easy to do and easy to forget about. Unless you go actively looking for headers with curl or similar you’ll probably miss them, and even then will you spot an incorrect header by eye? RedBot is an excellent online tool that not only shows you the headers but makes recommendations about what might be missing, invalid or things you haven’t considered.
For instance the RedBot results for vagrantbox.es look like this:

Or at least they do now after a few tweaks to my nginx configuration. In particular I’ve added
I’d assumed that enabling gzip compression with gzip_vary would have set this automatically and never checked. The way RedBot provides a checklist of recommendations is very handy.
As well as checking the page itself you can also check all the assets associated with a page by adding a query string argument. For instance here is the assets view for Vagrantbox.es. RedBot also provides a JSON encoded version of the result which might be useful in a CI system. If you’d rather run your own instance of the software you can, the code can be found on github at mnot.github.com/redbot/. It currently doesn’t work with HTTPS resources but that’s about the only thing I’d like to see added.
For those that haven’t yet had a look Cloudfoundry from VMware is two things, one of which is nice, one of which is very cool indeed:
I’m pretty interested in the latter. Its API could in theory become a defacto standard for application and service buildouts, in the same way as we’re seeing the EC2 API expand outside AWS for managing infrastructure (and arguably how we’re using Chef and Puppet for managing the things installed on that infrastructure). The really interesting bit is the fact it’s all open source. Not only does that mean you can run it on your own infrastructure (including as I’m doing on a virtual machine on my laptop), but it’s also designed so new services (like Redis, MySQL, RabbitMQ), new runtimes (like Ruby 1.8, RUby 1.9, Java) and new frameworks (like Rails, Sinatra, Spring) can be added easily.
I’m running vcap on a vagrant managed VirtualBox instance, but you could install it anywhere you like. I used these chef recipes to get everything installed. I ran into an issue with the mysql service not starting correctly that I fixed and I needed to manually install chef into the rvm gemset part way through the install, but the recipes pretty much just worked.
Looking for an excuse to have a route through the vcap code I decided to see if I could add rudimentary support for Python applications. After a day of noodling around I’ve forked the code and sent a few pull requests back with it basically working. This required changes to the vmc client, to the vcap service and like all good open source contributions to the test suite.
Thanks hugely to existing pull requests (mainly the one’s for adding PHP support) it’s not taken long at all. The Sinatra and Rails support which shipped with the first release from VMWare supports using Bundler to define gem dependencies that can be pulled in at deploy time. It shouldn’t be too much effort I’m hoping to do the same for using pip and virtualenv for each deployed python application. I think I can also see how to support python3 and how to add django as a supported framework.
I had huge fun, but you might not at this early stage in the project. I’m relatively happy with reading and writing Ruby, futzing with rvm, debugging installation woes and hunting down service configuration problems. The best tool for working out what was happening was generally tailing the logs in /tmp/vcap-run/ and finding the code that wrote a given message. If you just want something to work I’d wait a little while, if you like the sound of the above it’s a pretty nice codebase to play around in. I’d love to eventually see some official contributor documentation and some hints and tips on things like running the tests. At the moment flicking through reported issues and pull requests on GitHub is the best place to start.
After Patrick released Sahara, a nifty extension for the Vagrant command line tool, I’ve been meaning to put together a similar extension for interacting with the growing list of base boxes on vagrantbox.es.
After a bit of hacking this morning I’ve just pushed out an initial release of the vagrantboxes gem and you can find the source code and some documentation on GitHub.
The extensions adds a vagrantboxes namespace to the vagrant command line tool which provides a few useful commands. Specifically you can list all the available boxes, do text searches, show the full details of a box and most handily of all add a box that takes your fancy to your local base box store, all without having to worry about the URLs of the boxes if you don’t want to.
Here’s an example of a simple search:
And another quick example, this time of show printing the full description. In the future I might look to store more structured metadata and make this more useful.
This proved a good excuse to delve into the Vagrant source code which is pretty readable for the most part once you understand the libraries it’s build upon. It’s simple enough to extend for adding commands like this too, which bodes well for other more useful additions in the future.
If anyone has any problems with this extensions do let me know. Error handling currently consists of returning blank output rather than sensible error codes or messages, and as I’ve yet to add a small test suite so their might (will) be a few bugs lying around. I’ll try and make it better behaved in the next week or two but reasoned it’s useful straight away.
A recent question on Twitter prompted me to write a quick blog post about managing cron jobs. As more and more people want to automate provisioning and deployment of web applications some, maybe previously manually managed, items come into the fold.
Cron jobs are interesting because you may prefer to see them as part of the infrastructure (like apache or mysql) or as part of your application code. I think both are valid, sometimes at the same time. For instance you might have a cron job which deals with scheduled database backups. All that requires is the database and the script to be present. At other times your cron jobs might require your entire application stack. For instance a rake task which uses a Rails application model, or a django management command.
Both Chef and Puppet provide a cron resource type. This is particularly handy for things like database backup scripts or ganglia gmetric scripts. You probably want these scripts and cron jobs to be installed on machines that have the related software installed, and you’re probably already describing this in your Chef recipes or Puppet manifests. If you’re not already using one of these tools using them to manage just your cron jobs is a nice way of starting out.
Using the Puppet Cron Type looks like this:
And the equivalent Chef Reasource looks like:
The important part is that by describing your cron jobs in code you can then version control them easily, and both Chef and Puppet have mechanisms to push these jobs out to be installed by the relevant hosts. With cron jobs you might not want all your machines to be running the same jobs for instance.
An alternative, or complimentary, approach to versioning and deploying cron jobs would be to tie it in with your application code. This makes sense when those jobs are tightly coupled to your application, for instance rails specific rake tasks or django management commands. Whenever is a tool I’ve been using recently that makes this pretty simple. You describe your cron jobs in a file called schedule.rb like so:
And then running the provided whenever command line application will generate a working crontab. Whenever ships with capistrano integration and some useful shortcuts for running rake tasks or accessing Ruby code, but it’s simple to write your own command shortcuts without having to write ruby code too.
I have seen some tools which replace cron completely, but I’ve never liked that idea much. Cron works pretty well, and is clever enough to deal with things like daylight saving time and leap years inteligently if needed. I know other folks who are centralising all regular jobs into something like Jenkins. I can see advantages to that, although I’ve yet to find a really nice way of automating this outside the gui interface or manually modifying configuration files.
I’ve written before about why I like System Packages, but even I’ll admit that the barriers to creating them mean I don’t use them for everything. FPM however is making it much easier, to the point where I’m starting to create a few packages I can reuse on projects. I thought a write up of how I’m doing that for Cucumber-Nagios might be useful.
For those that haven’t seen it yet, FPM (or Effing Package Management) is a tool that helps you build packages, like RPMs and DEBs, quickly. It can take in gems, python packages, node.js npm files or just plain directories and make files and from that create you one or more system packages. Lets have a look at a full example with a Ruby gem.I really like using cucumber-nagios, whatever platform or language I happen to be using at the time. I have a number of Django projects for instance with cucumber-nagios checks, so being able to not worry about most of the Ruby install is useful.
In order to use FPM you’ll need to install it. It’s available as a Ruby gem so lets start there. I’m not going to delve into setting up a Ruby Gems environment as it’s annoying and covered for most platforms elsehere on the internet.
First off lets install the cucumber-nagios gem, along with all it’s dependencies, into a local folder on my build machine. This might be a virtual machine or a separate machine in your cluster. It should be running the same OS as the intended production machines however. The following examples are from Ubuntu, but it’s much the same for RPM based distros.
Cucumber-nagios has a large number of dependencies, so we’re going to need to create packages for all of those too. FPM will cleverly deal with maintaining the specified dependencies thought. We’ll use find to do this quickly, and then instruct FPM to convert from a gem to a deb. You could obviously do this line by line if you prefer.
That should leave us with lots of new .deb files that we can have a closer look at:
If you already have a private apt repository set up then just upload these packages and away you go. I’d like to use apt for installing them so I can leave it to manage all the dependencies easily. If not then I’ll show you briefly how to create a local file system repo, it’s not much more work to create a shared repo available over HTTP.
First create a directory to store our packages and move our newly created .deb files into it. You’ll need to be able to execute some of these commands as root but given the topic I’m assuming you’ll be able to deal with that.
For the next part you’ll need to install the dpkg development tools, and then create a file that can be read by apt when it’s looking for packages it can install.
Now add your new package repo to your apt sources and update your package cache.
At this point everything should be up and running. Let’s do a search in our repo:
And finally lets install cucumber-nagios from our shiny new package.
If everything has worked out you should be able to run the cucumber-nagios-gen command to create a new project. Note that the path may be different, and in the case of debian based distros the gem bin path is not on the Path.
Since I launched it back in November my Devops Weekly email has been pretty well received I think. Folks on twitter seem to like it, as do a few people I’ve met at recent events who have said nice things. One thing a few people have been after though is an online archive, either because email just doesn’t work for them or more often because they missed out on the earlier issues.
With a little time this weekend and the help of Jekyll I’ve just added the seventeen iss§ues to date to the new Devops Weekly archive. I’d have liked to do something a bit more useful, maybe introduce per link tags or a nifty search feature, but I’m pretty busy at present and reason this should serve the main purpose of getting these links online.
As recent blog posts on here make clear, I’m a big fan of Vagrant. And when Michael asked if I’d fancy talking to some of his colleagues at The Guardian about how I use it I really couldn’t say no.
I gave a short talk, running through the following slides, and running a few demos showing creating, destroying and provisioning new machines.
     
More interesting I thought were the questions and conversations that followed. We talked a little about how Vagrant might fit into a continuous integration setup. Another aspect some of the systems folk took to was how flexible the network configuration was and whether they might be able to use this to more effectively test firewall configurations well before the final push into a production environment. It’s not something I’ve been doing but it sounds feasable and useful in some organisations. If anyone is doing interesting things with Vagrants network config I’d be interested to know.
I had the pleasure of speaking at Cambridge Geek Night on Monday again, the topic of conversation being using Ganglia to collect more than just base systems metrics.
     
The audience of web developers, the odd sysadmin and business folk seemed to enjoy it and we had lots of time for questions at the end. The main point I tried to get across was that Ganglia makes a great platform for ad-hoc metrics gathering due to the ability to just throw values at it and get time series graphs without any extra configuration.
The slides include a few bits of code I’ve been using that I’ll throw onto GitHub as a proper project when I get the time. These are very simple Python servers, one which allows for sending metrics information over HTTP, the other using TCP instead. Both really handy for getting more people to add hooks into applications.
I was reading Devops is a poorly executed scam and just couldn’t resist a reply. Not because of the entertaining title, but because I both agree and disagree quite strongly with parts of the post. Read it first if you haven’t already. And yes I know I’m feeding the internet.
I’m going to pick parts of the post out and then comment. Hopefully I’m not quoting these in any way out of context.
People are pretty aware of this fact I think, but watch what happens when people post on the mailing lists or turn up at community events with a purely marketing hat on. They just get no traction and even damage their product brand amongst the early adopters. The fact the term is starting to get used in job adverts and marketing materials isn’t really being driven by the people talking about what devops might or might not be. I think the main reason for this is that most of the people I talk to in person or online are actually pretty happy with their jobs and generally work inside companies rather than as independent consultants. They have often reached an age where they want to improve within a given field but would like a wider network than their current colleagues to discuss things with.
I don’t think you do. The comparisons with Agile are interesting from a community point of view but Scrum is a methodology. To me at least devops isn’t, it’s just a banner or tag under which interesting conversations are happening. The argument that “You should be doing this anyway. Not earth shattering.” is a good thing. You’d be suprised by how many people don’t do all the things they should be doing, especially in small and young companies. And one of the reasons for that is no one bothered writing a list of these things down anywhere and then discussing them. I’m not saying this huge list exists or even whether it should, but the discussion is happening.
This is spot on. I think this maybe does get missed in talk that focuses more on tools but not in the wider discussion happening about business improvements. Devops quite litterally brings those two things together. You’ll always have individual goals but where you have separate operations and development teams they should have the same fundamental goals.
Yes, yes and yes again. I’m definitely from the developer side of the tracks and I’m constantly telling people this and it’s definitely something I don’t see enough people doing. What I’d love is for all the operations people to state this to their development team and most importantly to help them set that up. Just saying work like me or I won’t let you near the production machines is just being obstructive. Educating and helping with tooling helps build those bridges and trust. And with trust comes the access the developers want. And less stupid bugs and less deployment issues with differing package dependencies.
Still unsure which Devops people are saying it’s a bonefide methodology. I see the word used sometimes but generally in passing and not I don’t think meant how you mean here. And I don’t think I’ve heard people speak about it in person. “Scrum methodology” returns more than 113,000 results in Google. “Devops methodology” returns about 150, some of which 404 and half of which are agregators pointing at the other half.
Some company or other is definitely going to be scammed into paying over the odds for a consultant because they used the word Devops in the sales pitch. That will have next to nothing to do with what I’d see as the Devops movement and everything to do with human nature (and sales people).
One of the reasons behind getting around to building Vagrantbox.es recently was I was giving a talk to a group of startups on The Difference Engine programme and I wanted to have an example project to demonstrate various things. I wanted to demonstrate everything from sensible version control habbits, configuration management, basic orcestration and most importantly a solid deployment process. I’ve decided to write up what I’m doing for deployment because I think it’s pretty nice, and for all the talk about Continuous Deployment I haven’t seen many examples of code and configuration to make it happen.
Most of what I’ll cover is pretty easy to map to whatever technologies your using. For this project I’d gone for Git, Django, Gunicorn, Nginx, Fabric, Mysql and Jenkins and I’m deploying to Ubuntu running on Brightbox Cloud. Apart from the Jenkins instance in the middle you could follow the instructions and swap things out easily.
First up lets install Jenkins. I setup a separate cloud instance just to run the Continuous Integration server. I find this approach easier to manage but you could always run this locally if you prefer. The Jenkins folk provide very up to date packages for Debian so I chose to use those.
Jenkins provides a huge number of optional plugins which enable various additional features. Plugins are installed via the web interface at /pluginManager. I’ve installed:
Only the Git plugin is really required for what I’m doing with deployment. Cobertura and Violations are code quality metrics tools that I use to record output from pylint and code coverage for my test suite.
My finished project was already on GitHub in a private repository. I’m using a requirements.txt file to record python dependencies so I can use pip to install them automatically and I’m using Virtualenv to sandbox this installation. I’m also using South to manage my database schema changes. I won’t go into that here as it’s pretty Python specific, Rails for instance has Active Record migrations, RVM and Bundler which do pretty much the same job. PHP has PEAR and some of the frameworks offer a migration tool.
I then created two projects in Jenkins:

This is the main build of my master branch in Git. As well as setting up the Git repo as shown below I’ve set a polling schedule to */5 * * * * (that’s every 5 minutes) and also set Trigger builds remotely so I can have a task in my fabfile which triggers a build immediately.

I then have two build steps, both of which execute shell commands. The first installs any new requirements via pip:
The second runs my test suite and generates the XML output required to show the test results in Jenkins:
I’m using the rather handy Django Jenkins application for this.
So far so good. This gives us a project that, when we push some changes to GitHub, will pull those changes down to the CI server and run our test suite, giving us feedback as to whether the tests pass or fail.
Now for the trick, in Post-build Actions tick Build other projects and specify the name of another project that we’ll setup next. Mine is called Vagrantboxes-deploy.

This project is triggered only when the previous project runs successfully. And all it’s going to do is run the deployment script on the project we just built. The setup for this project is very simply, it has one build step which just executes the following:
The specifics of the Fabric script here aren’t that important but I’m doing something not too disimilar to what I described here.
The reason I’ve setup a separate project for these is so I can, if I choose, trigger a deployment separately to the full build, and also so I can very easily disable deployments even if the main build is still running.
With this setup whenever I push code to master it triggers a build. If the test suite passes it runs the deployment script and pushes out the code to the live web servers. This suites me and this project but you might find it easier to start by pushing all successfull builds out to a staging environment. And maybe then moving on to having a new project which is only triggered manually for deploying to production.

This setup has other advantages too. The Jenkins dashboard becomes a handy tool for recording deployment events. You can easily setup emails or IM messages or Campfire posts to alert other team members whenever a deployment happens. And it really really makes sure your delployment scripts work without hand holding.
This is a simple project that I’m working on on my own, but in a team environment you’d likely have a more complex branching strategy and more Jenkins projects. You might also introduce some gateways for manual testing but the starting point is the same. Jenkins makes archiving successful build artifacts relatively easy as well, this setup has a few race condition possibilities that you can fix by building artifacts from successful builds. Jenkins also supports building from different branches and having different branches trigger different projects, all handy if you want to grow this kind of setup.
A brief conversation with Matt Keating on Twitter finally pushed me over the edge and I’ve built a site I’d been meaning to do for a while.
I’m a huge Vagrant fan, but one thing that often comes up is where to find base boxes. My newly launched site Vagranbox.es provides just that. At the moment that just means user submitted boxes being checked and then posted. I’ll likely add comments and ratings and the like if things become popular but that’s for later.

So, if you know of or host a useful box please let me know. I’ll try to keep up with any submissions.
I’ve just found my notes from James Turnbull’s talk at FOSDEM. I found the talk excellent, and I’m already part of the choir. But much of the audience I’d guess have only come across the devops term in passing, or worse had it pushed at them as part of marketing materials. Hopefully I captured the main points:
I had the pleasure of speaking at Fosdem last weekend to a packed Configuration amd systems management devroom.
My presentation covered some of the same ground as recent blog posts, namely why you should be using virtualisation and config management tools to manage your local development environment.

People even said nice things about it:
All in all another good event, I have notes about some of the other talks I went along to that I’ll try write up soon.
Michael Brunton-Spall wrote last week about some frustrations with packagings and deploying Python web applications. Although his experience was with Python, the problems he describes are the same for Ruby and PHP and a whole host of languages. The following example uses Python, but works equally as well for anything else.
Michael has three simple rules for his servers:
I won’t go into all the reasons for doing this (you can read the blog post linked to above) but these are pretty sensible security precautions.
My approach to this problem would be to use your friendly system packages and using a handy tool called Checkinstall to create a deb or rpm. I’m going to use as an example the Eventlet library. This is available in PyPi and one of it’s dependencies (Greenlets) provides a C extension. The same approach would work for an entire Python web application too. I’m as ever using the apt package management tool but this should work with yum as well.
The first step is to build the package on a build machine. This should be a machine or virtual machine running the same operating system as your production web servers. You might build these packages manually or as part of a continuous integration system. On this machine we’ll need the compilers and development tools:
We’ll also create a virtualenv into which we’ll be installing our packages:
Now, instead of just calling easy_install to install the package, we prefix it with checkinstall.
This will prompt for various meta data about the package you want to create, including the name and version of the package. If you’re using this method in the real world you’ll want to decide on a versioning and naming scheme for your packages to avoid clashes with system provided packages. You can also set many of these options from the command line rather than having to manually fill them in each time.
Once everything has been filled in successfully this should run through, installing eventlet and greenlets and eventually creating a deb or rpm package depending on what platform you’re running on. You should see something like:
Now lets grab that package and take it to one of our front end web servers via a controlled deployment process. That front end web server needs the virtualenv creating but nothing else. So:
(Now you might be thinking that installing the python-virtualenv package in this way breaks rule 1 above. And you’d be right in most cases, but I’m guessing Michael’s systems team have a local package repo for authorised packages, or alternatively you could download the package to the build machine and push it to the production environment.)
Now install the package we created earlier.
That should throw all the required files into the virtualenv environment we created. No compilers. No calls to internal or external systems. Just move some precompiled binaries and text files to predefined places on disk.
I used a PyPi package as an example. Checkinstall could have been pointed at a custom build file written especially for your own application, one that moves files and folders to where they are needed. Say something that looks like this:
The running checkinstall against that (or a more complex build file using capistrano or ant or fabric) you can create a package containing your application code and install it into the specified place.
First a bit of background. I’m a software developer (lately in Ruby and a tiny bit of Java, previously in Python, C# and PHP; yes I got around a bit), but have spent enough time looking after production hardware (mainly debian, solaris and recently a bit of RHEL) to have a feel for sysadmin work. I even have friends who are systems administrators. I mainly use a shiny apple laptop for my development work, but I actually execute all the code on Linux virtual machines. The aim of this post is to bridge a divide, not start a flame war about specific tools.
I’m writing this partly to address a tweet I made that in hindsight needed more than 140 characters. Actually a number of my recent tweets have been on the same theme so I should be more helpful. What I’m seeing recently is an increase in the ways I’m being asked to install software and for me at least that’s annoying.
I’m a polyglot programmer (so I shouldn’t care about #3) that uses git for everything (scratch #2) and who writes little bash scripts to make my life easier (exactly like #1). So I understand exactly how and why these solutions appear fine. And for certain circumstances they are, in particular for local development on a machine owned and maintained by one person. But on a production machine and even on my clean and tidy virtual machines none of these cut it for me in most cases.
Most developers I know have only a passing awareness of packaging so I’m going to have an aside to introduce some cool tricks. I think this is one place where sysadmins go wrong, they assume developers understand their job and that they know the various tools intimately.
I’m going to show examples using the debian tools so these apply to debian and ubuntu distros. RPM and the Yum tool have similar commands too, I just happen to know debs better.
This one is a bit obvious, it’s probably going to be available in anyones home grown package management system. But if you’re installing software via hand using git or a shell script then you can’t even ask the machine what is installed.
I love this one. Have you ever installed a package and wondered where the config files are? You can soft of guess based on your understanding of the OS file system layout but this command is handy.
Have a file on disk that you’re not sure where it came from? Ask the system package manager. The more everything is installed from packages the more useful this becomes.
At the heart of a good package system is the ability to map dependencies and to have unmet dependencies installed as needed. Having tools to query that tree is useful in various places.
Will give you output a little like the followning:
The apticron tool can alert you to packages that are now out of date. It’s easy to set it up to email you each day for each host and tell you about packages that need upgrading. Remember that the reason one of these might have an update could be a documented security bug and it becomes even more important to know about it quickly.
I’m really not an expert on using debs but even I find these tools useful, and you don’t get the same capabilities when you use anything else.
Still here? Good. I’m going to pick on a few pieces of software to give examples of what I mean. All of this software I actively use and think is brilliant earth shattering stuff, I’m not dissing the software so if any fanboys reading can kindly not attack me please, I’m one of you.
The nice folk building the RabbitMQ message queue provide downloads of the source code as well as various system packages. Knowing that some people will want to use the latest and greatest version of the application they also host the latest deboan packages in their own package repo with details on their site.
The Chef configuration management system also provides multiple methods to install their software. For people already using, happy and familiar with it they provide everything as a ruby gem. If you prefer system packages they have those too. They also provide their own deb repo for people to grab the latest software.
Before I found the Cloudera Hadoop packages I remember having great fun manually applying patches to get everything working. Cloudera do exactly the same as the above two developers, namely host their owns debs.
RVM is a fantastic way of managing multiple ruby versions and multiple isolated sets of gems. But it’s also probably the first place I saw the install from remote shell script approach.
I like to do the same things on my development machine as I do in production, and the main problem I have with RVM is that it’s so useful I want it everywhere. I’d prefer if the system wide install had some sort of option to install the rubies from packages rather than compile everything on the machine (meaning you need a full set of compile tools installed everywhere), or that we can automate the creation of the packages using rvm.
You’ll probably find packages for the Solr search server in recent distros. It’s hugely popular predominantly because it’s a fantasic piece of software. But everytime I have a look at the system packages I can’t quite get them to work, or they are out of date. I now know my way around Solr setup relatively well and just end up creating my own packages and I’ve spoken to other folk who have done the same. The Solr documentation recommends downloading a zip file to get started and I can’t see any mention of the packages. My guess is the packages aren’t maintained as part of the core development which is a quick way to get them out of sync with current progress.
System packages aren’t blameless, I think the culture often seen in debian of splitting the developer from the package maintainer is part of the problem. This manifests in various ways, all negative:
First up lets talk more about the distribution and installation of software. And lets do that in the spirit of making things better for everyone involved. The ongoing spat between Ruby and Debian people is just counterproductive. This would be a good article if it didn’t lead with:
We need better documentation aimed at developers. I’m going to try and write some brief tutorials soon (otherwise I’d feel like this rant was just me complaining) but I’m not an expert. I’ll hapily help promote or collate good material as well. Maybe it already exists and I just can’t find it?
I’m a git user and a big GitHub fan, but one of the features of Launchpad I really like is the Personal Package Archive. This lets you upload source code and have it automatically built into a package. This is specific to Ubuntu but that’s understandable given Launchpad is also operated by Canonical. What I’d like is the same feature in GitHub but that allowed building debs and RPMs for different architectures. Alternatively a webhook based third party that could do the same would be awesome (anyone fancy building one? I might pitch in). The only real advantage of it being GitHub would be it would make packages immediately cool, which hopefully you all now realise that they are.
I’ve written about Vagrant previously and the more I use it the more it impresses me and the more it changes how I work. For those that haven’t yet used vagrant the brief summary is, it’s a way of managing, creating and destroying headless virtualbox virtual machines. So when I’m sat at my computer and I want a new 32 bit virtual machine based on Maverick I just type.
It has some other magic tricks as well, like automatically setting up NFS shares between the host and guest and allowing you to specify ports to forward in the configuration file. You access the machine via ssh, either using the handy vagrant ssh command or by using vagrant ssh-config to dump the relevant configuration to place in ~/.ssh/config.
I’ve been using virtualisation for a few years, initially purely for testing and experimentation, and then eventually for all my development. I’d have a few VMware images, I’d use snapshots and occasionally rollback, but I very rarely created new virtual machines. It was quite a manual process. With vagrant that’s changing. Everytime I start investigating a new tool or new technology or work on a pet project I create a new virtual machine. That way I know exactly what I’m dealing with, and with vagrant the cost of doing that is the 30s waiting for the new machine to boot.
Or rather it would be if I didn’t then have to install and configure the same few things on every machine. Pretty much whatever I might be doing I found myself installing the same things, namely zsh, vim, git and utils like ack, wget, curl and lynx. This is exactly what the provisioning support in vagrant is for, so I set out to use chef to do this for me.
I decided to use a remote tar file for the recipes. I’m not really bothered about managing a chef server just for my personal virtual machines, but I did want to have a canonical source of the cookbooks that wasn’t local to just one of my machines. Plus this means anyone else who shares my opinions about what you want on a new virtual machine can use them too.
My Vagrantfile now looks like this:
You can see the cookbook on GitHub at github.com/garethr/chef-repo. By default it uses the official oh-my-zsh repo and the vim configuration from jtimberman. My own versions are very minor personal preference modifications of those. The Vagrantfile example above shows how you can override the defaults and use your own configs instead if you choose.
One question I was asked about this approach was why I didn’t just create a basebox with all these things installed by default, this would reduce the time taken on first boot as software wouldn’t have to be installed each time. However it would also mean maintaining the basebox’s myself, and as I use different Linux distributions or versions this would be a headache. While doing this and working with vagrant I’ve been thinking about the ecosystem around the tool and I’m planning on writing my thoughts on that subject over the next week or so.
I’m a huge Solr fan. Once you understand what it does (it’s a search engine, which means more than you think) and how it works you spot lots of thorny problems that map to it’s features really well. In my experience it’s also very fast and very stable once installed and setup. Oh, and the community support is great as well.
When I talk to some folks about Solr all they can think about is full text search. The main reason for this I think is a number of poor libraries. I’ve come across lots of Python or Ruby libraries that simply say you don’t have to know anything about Solr, just install this code and you get full text search! This works in the same way as using the default Mysql or Apache configs works, nowhere near as well as if you get your hands dirty even a little. Some of the ruby gems even ship the Solr jar file in the gem. Now you don’t even need to know Solr exists. You take a generic configuration and run it using a rake task behind which is some unknown Java application server. Good luck debugging that when it goes wrong, that’s one hell of a leeky abstraction.
In better news I’ve now found two excellent Solr libraries, one’s that start with the assumption that you know what you’re doing or happy to learn about the tools you’re using. All you really want from a library is a good API that maps to how you write in that language.
The delsolr API is beautiful. It seemlessly merges the worlds of Ruby and Solr in a way that’s easy to write and easy to guess. It’s also clever, the design accepts that new features might be added to Solr before the library is updated or that the library might not support every usecase or option. In these cases you can still pass information through to Solr directly.
Solr’s interface is based around URLs, so any library is really just giving you an interface to creating those URLs.Writing the following in Ruby:
Results in the following URL:
If you already know Solr and how to construct URLs for searches by hand you’ll immediately get the Ruby code. You can probably even guess how to pass other params like sort or order.
Another nice touch is that you can use either hashes or Lucene search syntax for each attribute. So:
Is the same as:
Sunburnt is a python Solr interface from the nice folks at Timetric. I’ve not had chance to use this library in anger as it was released after I’d dont quite a bit of python-solr work in an old job but I’d definately use it now. The API looks like:
It’s based around chaining so again you can probably guess how to make further queries from even this simple example.
Both Sunburnt and Delsolr also support adding documents to the index.
Once you understand facets and the usefulness of filter queries you see lots of places where Solr is useful apart from text search. Lots of ecommerce operations use facetted search interfaces, I’m sure everyone has spent time clicking through nested heirachies and watching the numbers (showing the number of products) next to the links decrease? You can built these interfaces using SQL but it’s incredibly expensive and gets out of hand quickly. Caching only helps a bit due to the number of permutations in all but the smallest stores or simplest products. It’s a similar problem with tagging, it’s pretty easy to kill your database
But it’s not just things that have the word search in that you can map Solr to. Two good examples are Timetric (from whom the Sunburnt library comes from) and the Guardian Content API. Both of these present lots of read data straight from Solr with great success and less database killing performance issues. Solr can really be seen as a simple place to denormalise your data, one advantage being that it keeps your database schema clean.
Solr could do with better documentation for beginners. The wiki is an excellent reference once you know how to write schema and configuration files but I think the getting started section sacrifices introducing configuration in favour of getting people searching quicker. The example schema and solrconfig files that ship with Solr are also amazingly useful references (officially the best commented XML I’ve ever seen) but also intimidating to beginners. The Drupal community appear to be writing some good docs that fill this gap though, here’s a few links that I’d recommend:
With the success of Heroku, both in terms of the recent sale and the fact it’s awesome, it was always just a matter of time before other languages and frameworks got into the platform as a service game. Here’s all the one’s I know about so far, many of them in or entering beta testing at the moment. Any others I’m missing?
Update Thanks for all the comments on here and on Hacker News, I’ve updated this list with all the suggestions.
As mentioned loudly and repeatedly on here and on Twitter I love vagrant. While writing a chef cookbook to bootstrap my virtual machines I started thinking about how things around vagrant could help it be more useful. These might be things I’m going to do, or ideally get involved with others to do. If anyone has any other ideas, or suggestions please leave comments, I definately think this is the time for discussion.
I don’t really want to have to maintain baseboxes but I want access to lots of them. I’m sure some people will want a Ruby on Rails in a box but all I really care about is having access to recent 32 and 63 bit vanilla linux distributions. I want a good source for trusted baseboxes. At the moment the approach is to look on the wiki, then look on the mailing list and then search the web, then create your own (even using VeeWee it’s still a little fiddly). I’ve managed to find good lucid, maverick and debian boxes, but have had problems with centos and a few others. Part of this is the rate of change recenty of both vagrant and now VirtualBox (both good things), part of it is the lack of reviews and shared experiences around baseboxes.
What I’d love to see is a single place where anyone can post a link to a basebox and vagrant users can come along and assign metadata about whether it worked and on what hardware, vagrant version, virtual box version, etc. It could even act as a tracker, counting downloads of boxes to gauge popularity.
As mentioned previously I have a chef cookbook I use to bootstrap all my new virtual machines. My process is therefore: vagrant init, make some manual changes to the Vagrantfile (or copy it from elsewhere), vagrant up. I’m lazy and want a nicer way to reuse Vagrantfiles or to script their creation.
I started out thinking that the ability to point the init command at a template and to provide context on the command line might be a good idea. Now I’m wondering whether we just need a command line application which allows for writing or modifying the Vagrantfile? Something like:
I dissed the idea of a Ruby on Rails in a box basebox above but I still want to be able to let people more easily share custom configuration for specialist applications. But what I’d prefer would be people sharing packaged cookbooks, a bit like I’ve done for my default virtual machine setup. Again the beauty of this is it’s pretty much just sharing a URL to a tar.gz file. This makes more sense to me at least than random people connecting to my chef server (I shouldn’t know about their machines) and lowers the barrier to entry for those not interested in hosting their own chef server or using the opscode platform for local virtual machines.
I’m also not talking here about just sharing individual cookbooks like cookbooks.opscode.com, but rather a packaged collection of individual recipes designed for a specific purpose. A fully working solr instance, a django application server using apache/mod_wsgi, etc.
Many of the points about baseboxes above would work here too I think. Having a good community resource which points to lots of cookbook tar files. Allowing people to feed back about what works for them. I’ve mainly taked about Chef here as that’s what vagrant initially shipped with, with the puppet provisioner now ready to go with would stand for puppet manifests too.
I wrote a quick article last week for the excellent sysadvent advent calendar, Smoke Testing Deployments with Cucumber talks a bit more about using a few of my favourite tools to check whether a deployment just broke anything important.
I magically turned into a Java developer last week for a bit when I had to do some integration with a SOAP based API that really really wanted me to write Java on client as well. That led me down the route of having a good look at Jruby (which I’ve used before, mainly for testing using celerity) and in particular how easy it was to use native Java classes in Jruby (very, very easy as it turns out).
All that meant I’ll probably end up writing a nice Jruby application in the not too distant future, and not knowing too much about running such a thing in a production environment I thought I’d take a quick look. I went with Glassfish as the application server for no other reason that it took my fancy. I’d definately be interested in hearing about any positive or negative experiences people may have with it or other similar servers. My quick look turned into running a tiny Sinatra application.
First install the required gems for our little experiment. You’ll obviously need jruby which is sort of the point, I’d recommend using RVM for that.

Now create a sinatra app. OK, it could be any Ruby rack based application but I like Sinatra. First we need a rackup file.

Now for our application itself.

Warble is the gem we’re going to use to create a WAR file, which is basically an all in one bundle of our application and it’s dependencies which we can deploy to a java application server.

Now we’re ready to generate our WAR file.

This should create a file called sample.war or similar. Then just deploy that to your application server and away you go. I got this working very easily with Glassfish which seemed to be the recommended tool for such things. Installing Glassfish was time consuming but well documented here. Uploading to Glassfish was done via the web interface for the moment. I just selected a Ruby project from the deployment drop down and uploaded the war file.
I’ve really been enjoying Ruby Weekly recently, it’s an email newsletter by Peter Cooper which brings the latest Ruby related news and articles to your inbox.
I have to admit to being sceptical at first about the format, I think I unsubscribed from most email newsletters many years ago, moving my reading to RSS and then Twitter. But I’ve actually found a regularly appearing email a great way to catch up with the goings on. A think the reasons for that change of heart are:
I think Ruby Weekly works because it’s collated by someone with taste. Peter spends a bit of time putting together a small number of high quality links so I don’t have to. With Devops Weekly I’m hoping to do the same for a different niche.

So head over to devopsweekly.com to sign up. I’m not 100% sure yet when the first issue will go out but expect it in the next few weeks, and after that I’ll try and get one issue out each week. Any ideas are more than welcome, as are any news or articles that you think should go in.
Before starting with FreeAgent I decided I should spend a bit more time with Ruby and set about building something I’d been thinking about for a while. I’ve just launched the first one of my related pet projects so thought I better link to it from here.

Devops Books is exactly what it sounds like; a list of books that people interested in the whole devops concept should read. It’s not a complete list just yet and I’ll try and keep it up to date as new interesting books get released. Any suggestions to add to the list most welcome.
It gave me a proper excuse to use Heroku which has been very pleasant. Under the hood it’s a very simple sinatra application using the Mustache template language. The majority of the code is a build script that pulls down information from the Amazon API and mixes it with my own content. It then creates a JSON document that is used as the basis for generating the pages. I love static generators and hand rolling your own for a particular domain is often worthwhile. It means I have the json already lying around if I want to make a JSONP style badge for instance.
I plan on using the code as a bit of learning tool. Try out some different testing approaches, maybe add in GEOIP detection, make some of the commands I’m running into Rake tasks, that sort of thing. Given how easy it will be to throw up little sites like this using the generator I have a few other similar things in mind too.
My main development machine for a while has been an apple laptop. From looking around at conferences, offices and usergroups I know I’m not alone. But I don’t really run code on my mac very often, certainly not for work. I might edit the code on my mac but I execute it running in a virtualised Linux environment matching (as close as possible) the production environment it’s going to end up in. This blog post is an attempt to explain why this is a good idea to the majority of people who develop on a mac (or a windows machine) and deploy to something else. This isn’t language specific either. You might be working on small PHP web sites or huge Python applications, you’ll still one day run into the same issues.
Bugs happen, but catching them early, way before they even hit your shared source code repository makes them much less of an issue. Catching bugs only after a live release, when they affect customers and cost someone money is bad. And if your release is a long period of time after the work was done then fixing them is harder to boot. These are just some of the reasons we’re all fond of unit testing and continuous integration.
But if you’re running those tests against code executing on different hardware, on a different operating system, with different low level libraries or a different web server version or a different database server then you are not going to catch all the problems. If you take this to an extreme then you can only get rid of all of these problems by giving each developer a full production stack of their very own. This is obviously impossibly expensive for anything past the most trivial setup. But eliminating even some of these issues makes it more likely you’ll catch bugs early and less likely you’ll have a bug on your hands that you can’t recreate locally. So we’ll aim for production like rather than a 100% copy.
Here’s a real example, a case insensitive file system. Grab a terminal prompt on your mac and type the following in an empty directory. Then do the same on a typical linux machine. All we’re doing is using touch to create a file.
On you’re mac you’ll probably see:
On your linux box you’re more likely to get:
What? The mac treated Test and test as meaning the same thing. It wont let you have a file called test and one called Test in the same place. It’s case insensitive. But the linux machine didn’t have this problem. Now imagine we’re not dealing with empty files called test but either files you’re running code is creating at run time (a file cache maybe or a user uploaded file) or even more interestingly your source code files. Lets say you have git on a linux box in the corner of the office and someone checks in two files from a linux machine called Pages_controller.rb and pages_controller.rb. What happens when you get these to you’re mac? I haven’t actually tried this but it’s not going to end well. And imagine debugging this sort of issue? If you think this is all hypothetical I know about this little quirk exactly because I saw someone trying to fix a bug related to it.
What if the bug was because you had one version of lib_xml on your local development machine and a different one on production. Up to that point you might not even know what libxml was or how it got on your shiny apple laptop.
How many people can genuinely say they have never had a bug they could recreate on live and not on their development machine? Same code, different behaviour. Load and data often play a part in bugs like this as well but these can be isolated and tests created in at least some cases. Being pragmatic what we’re aiming for isn’t to eliminate all differences, it’s to get rid of those that are easy to eliminate.
Virtualisation tools used to be cumbersome and expensive and generally not aimed at consumers. I’ve used both VMWare Fusion and VirtualBox on my mac and even compared to a few years ago these tools are increasingly easy to use. And VirtualBox is free and open source too. On top of that we now have tools like vagrant which I’ll give a quick example of here.
Vagrant for those that haven’t come across it yet describes itself thus:
What it really is is a tool for quickly and painlessly building virtual machines based on sensible default configurations, and then providing programatic hooks for more advanced configuration. For instance you’ll have a configuration file to describe which ports you want forwarded and you can use Chef to install packages when the VM first boots. Once you have it installed it’s as easy as this to use
The first line downloads a 32bit Ubuntu disk image but you’ll only need to do that once. You’ll find lots of images for different distros too. The next two lines create and then boot a new headless virtual machine. That’s it.
Will let you jump straight into a ssh session with the new machine, for an idea of what else it can do here’s the help output:
I’ll leave it their as this post is more of a rant than a tutorial, but I might write more about using vagrant later. But in the meantime read the web site for a pretty simple walkthrough. And don’t be put off by the fact it’s written in Ruby or that the example shows a Rails app, this is a great tool whatever language you’re going to be using on the virtual machine.
I see too few developers doing this for it to just be about a lack of awareness. Lots of developers not doing this might be running local virtual machines for cross browser testing for instance. Here’s a few complaints I’ve heard and what I think the answer is.
If something is slow and you don’t have as much RAM as you can get into your machine then do that before complaining about anything. Running a few extra operating systems inside your main operating system is obviously going to be intensive so don’t scrimp on your tools. Also the defaults when setting up new virtual machines in VirtualBox or VMWare Fusion at least are quite minimal. Try increasing the ammount of RAM you let them use or give them access to more processes. I can genuinely say I’ve had a problem with this once and the real solution was changing the code, not throwing away all the advantages of virtualisation. If you’re doing some crazy real time video processing thing then your mileage will vary, but then you probably want a faster machine anyway.
This argument just bugs me, but I do know part of that is me. I need to know how all the bits work and fit together and I accept not everyone does, or indeed needs to understand everything. But someone on your team probably wants to know this stuff and importantly be able to tell others how they should do things. It’s pretty common for developers to setup a development environment for a pure frontend developer or a designer so they can make changes and commit CSS or new templates. This is no different. Most designers don’t need to know about the software environment in detail, it’s easier for them to defer to a domain expert. If a developer just wants to write code then they too should defer to someone who does know about the lower level when it comes to their development environment too.
This argument has some merit. We’re all busy and downing tools to setup something for you and your team is time consuming. And I think until pretty recently the time taken and the knowledge needed was a genuine barrier. Personally I’ve tended to have few problems, but then I’m familiar enough with Linux administration to avoid some common pitfalls. But problems with setting up port forwarding or shared folders can be pretty irritating when you want to work on a pressing bug or shiny new feature. But with tools like vagrant providing a simple interface to do this I think this is hopefully a thing of the past.
I agree up to a point here. Discussions of standardising individual developer tools turn into holy flame wars over whether everyone should use some IDE, Vim or Emacs (answer: Vim). This is pointless. File managers, utilities, test editors, terminal styles, host operating system. All of these and more should be up to the individual developers. But in the same way you generally don’t allow individual developers to use a new language no one knows without at least some discussion, why would this be different for the web server or operating system on which you’ll be running that code in production. Most of the time it’s not that developers make a consious decision to use a different version either. It’s more likely that they will take the path of least resistance and follow a tutorial or just use a package manager. It’s more likely if you ask the question “what specific version of Apache are you using on your development machine” they won’t know the answer.
I’ve not even gone into some of the other advantages of virtualisation here. Being able to snapshot your environment at any point and roll back an entire virtual machine like you do your code is hugely handy. As is the ability to create virtual machines that you can share with other members of your team. No more do new employees have to spend the first week installing dependencies and just getting code running.
I’m certainly not the only person doing this and it’s not a new idea. But it’s never been easier or cheaper. And with an increasing move towards virtualisation or cloud computing production environments it’s even easier to share good practices with your friendly systems administrators.
I’ve renabled comments on this blog after something of a break and I’d love to hear what people think, positive and negative.
I’ve been playing with Chef recently, in particular the solo variant. The new job at FreeAgent meant setting up new development virtual machines and rather than just jot down instructions I decided to script everything. I’d been wanting an excuse to take a look at Chef for a while and it’s certainly suited to this sort of job.
Unfortunately the getting started documentation isn’t yet great. I’m pretty sure this will improve over time, I had exactly the same problem with the Puppet docs a year ago. The main problem I had was I wanted to know how to use it, not just how to download someone elses cookbook. What I wanted was the following, the absolute simplest thing that will work. A Hello World example for Chef if you will. I’ll say now that I’m not an expert, their may be ways of doing this that are even simpler, but this works for me. And before someone mentions knife or rake tasks a generator isn’t simpler. It might be better when you know what’s going on but until you do it’s a big ass abstraction that will just get in the way.
All my sample cookbook is going to do is install a single package, curl. I’m going to assume you have chef installed for this already. The documentation did an OK job of that, although I’m relativly familiar with installing gems. I did find that the default system packages on Ubuntu at least were way out of date. Either get the packages direct from opscode or use the gem.
First create a directory and file structure that looks like this:
When you run the chef-solo command you need to tell it a few bits of information. The minimum appears to be just telling it where to find the cookbook we’re going to create. I think you can call this file anything you like but in the tree above it’s called solo.rb.
Next up is the details of the given node. This in our very simple case is just a list of the recipes we want to run when we execure chef-solo. Put the following content in the node.json file in the config directory as indicated above:
Last up we want to create a cookbook. Now you can go and download example cookbooks from all over the place. This is great for learning new tricks and commands but for me at least to begin with most of them were more complicated than I needed for my simple usecase. Lots of options. Lots of knowing the package names on different distros. I’m just calling this cookbook example. That means the folder in the cookbooks folder is called example and the run list above references example. Feel free to change this to whatever you like, or create new cookbooks with different names. Inside that folder we create a recipes folder and inside that we create a default.rb file with the following content.
And that’s it. A bigish directory structure, three files, each with about one line of content. Simple.
Now to run all that just issue the following command:
This should output various messages to the console about what chef is doing and, when it’s finished, you should find curl has been installed. Try and add another line to the recipe for another package (or even a gem) and rerun the chef-solo command. Now go read the docs for all the other cool things you can do.
So, I’ve just accepted a new job at FreeAgent, the rather snappy online small business accounting startup. I’ll be starting in about a month, when I get back from devopsdays in Hamburg.
I’m joining a pretty well knit group of designers and developers working on a pretty well loved piece of software. I’ve known Roan (one of the directors) for years, ever since the first Highland Fling conference I think. And the rest of the team seem a good mix of technical smarts and professionalism. Hopefully I can live up to their already high standards.
I’m looking forward to jumping into a decent sized Ruby project and getting properly reacquainted with Rails as well. I’ve spent most of the previous few years in the Python world so that should be a nice challenge. I’m constantly impressed with the number of high quality tools written in Ruby andRails 3 looks a genuinely nice piece of software. Hopefully with a bit more idiomatic ruby under my belt, more time on my hands and some new real world problems to solve I’ll increase my open source contributions and get back to writing again as well. I’ll likely be spending most of my time on those good developer staples new feature development, supporting existing code and scaling systems. I’ll also be involved in some operations stuff if I get chance, introducing software I’m a huge fan of and automating everything I can get my hands on.
The rest of the team are based up in Edinburgh but I’ll be working from home most of the time. I’m quite looking forward to working from Cambridge after having lived here for a few years. I’ll also get to see various friends in Edinburgh that I don’t get chance to meet up with very often and still have time to make it to London for the odd event or get together. I’ll likely be looking into coworking spaces or desk shares in and around Cambridge so if anyone has any recommendations.
Given this weeks pubstandards is apparently all about weddings, birthdays and new jobs (and I have all 3 within a month) it would be rude not to go along for at least one drink. If you’re around London I might see you there.
After something of a break I found a bit of time for some hacking at the weekend and decided to scratch a personal itch. I like writing shell scripts for everything from checking on things to deploying code. For anything that is more than just executing one command, or anything with detailed output or that takes a while I like to be able to see what’s going on. I also like to (OK, only sometimes) let other people run those commands and not all those people want to check something out and run a console. The web maks a pretty good interface for this sort of situation.
Enter Bolt. It’s my first proper go at using websockets for the communication between client and server, and my first stab at EventMachine. The code at the moment needs some improvement (tests, configurability, deployment options) but it works as a proof of concept.
It’s pretty simple, designed for running a single command at the push of a button and showing the results scroll past as they happen.

Hopefully it’s useful to a few more people, or at least it will be when I clean it up a bit. For more information head over to the GitHub page.
I’m getting married rather soon so time has been somewhat short (in a good way) for just hacking on stuff, but I’ve finally found a little bit of time to play with something I’ve been mulling over for a while. Namely a continuous deployment workflow using the integrity continous integration server.
I’m hoping to have an incredibly simple but fully operation example available at some point – mainly to act as a good discussion point. For now here’s my current pre-receive hook.
My friend Jamie Rumbelow has started a new project and decided to use Python. He asked a great question over on Stack Overflow which basically came down to what should I use for my first proper Python web application project. After a quick prompting on twitter I decided to have a go. I’ve cross posted my anwser below more because it took as long as a typical blog post to write.
OK, so I’m a little biased here as I currently make extensive use of Django and organise the Django User Group in London so bear that in mind when reading the following.
Start with Django because it’s a great gateway drug. Lots of documentation and literature, a very active community of people to talk to and lots of example code around the web.
That’s a completely non-technical reason. Pylons is probably purer in terms of Python philosophy (being much more a collection of discrete bits and pieces) but lots of the technical stuff is personal preference, at least until you get into Python more. Compare the very active Django tag on Stack Overflow with that of pylons or turbogears though and I’d argue getting started is simply easier with Django irrespective of anything to do with code.
Personally I default to Django, but find that an increasing amount of time I actually opt for writing using simpler micro frameworks (think Sinatra rather than Rails). Lots of things to choose from (good list here). I tend to use MNML (because I wrote parts of it and it’s tiny) but others are actively developed. I tend to do this for small, stupid web services which are then strung together with a Django project in the middle serving people.
Worth noting here is appengine. You have to work within it’s limitations and it’s not designed for everything but it’s a great way to just play with Python and get something up and working quickly. It makes a great testbed for learning and experimentation.
On the MongoDB front you’ll likely want to look at the basic python mongo library first to see if it has everything you need. If you really do want something a little more ORM like then mongoengine might be what you’re looking for. A bunch of folks are also working on making Django specifically integrate more seamlessly with nosql backends. Some of that is for future Django releases, but django-norel has code now.
For relational data SQLAlchemy is good if you want something standalone. Django’s ORM is also excellent if you’re using Django.
The most official Oauth library is python-oauth2, which handily has a Django example as part of it’s docs.
Piston is a Django app which provides lots of tools for building APIs. It has the advantage of being pretty active and well maintained and in production all over the place. Other projects exist too, including Dagny which is an early attempt to create something akin to RESTful resources in Rails.
In reality any Python framework (or even just raw WSGI code) should be reasonably good for this sort of task.
Python has unittest as part of it’s standard library, and unittest2 is in python 2.7 (but backported to previous versions too). Some people also like Nose, which is an alternative test runner with some additional features. Twill is also nice, it’s a “a simple scripting language for Web browsing”, so handy for some functional testing. Freshen is a port of cucumber to Python. I haven’t yet gotten round to using this in anger, but a quick look now suggests it’s much better than when I last looked.
I actually also use Ruby for high level testing of Python apps and apis because I love the combination of celerity and cucumber. But I’m weird and get funny looks from other Python people for this.
For a message queue, whatever language I’m using, I now always use RabbitMQ. I’ve had some success with stompserver in the past but Rabbit is awesome. Don’t worry that it’s not itself written in Python, neither is PostgresSQL, Nginx or MongoDB – all for good reason. What you care about are the libraries available. What you’re looking for here is py-amqplib which is a low level library for talking amqp (the protocol for talking to rabbit as well as other message queues). I’ve also used Carrot, which is easier to get started with and provides a nicer API. Think bunny in Ruby if you’re familiar with that.
Whatever bits and pieces you decide to use from the Python ecosystem I’d recommend getting to who pip and virtualenv – note that fabric is also cool, but not essential and these docs are out of date on that tool). Think about using Ruby without gem, bundler or rvm and you’ll be in the right direction.

The videos from the DIBI conference are now up on Vimeo. Lots of good stuff and more to come. The one disadvantage of a a two track conference is you miss half the talks so when I get a chance I’ll be catching up with those talks I didn’t get chance to see.
Logging useful information from running systems for monitoring purposes is pretty important if you want to see how your software is behaving in the real world. It’s one thing to test something locally, another to test something under load on a testing environment and quite something else to watch production code while running.
The numbers can be useful for checking newly released code isn’t having a detrimental effect on performance, observing what changes in load are doing to systems over time and planning for future capacity growth.
Creating log files, agregating files from multiple machines and then analysing the results is one approach. Another is using something like Ganglia. Ganglia is great for trending data over time, and ties in nicely to Nagios for reporting. Installing the monitoring daemon on machines and generally getting the default checks (memory, disk, network, etc.) up and running is nice and easy. From there using the gmetric command line to create custom metrics (say checking some mysql statistics) is again straight forward.
So far, so good. The only issue I’ve run into was creating custom metrics on the fly from a machine outside the network. For bonus points these metrics were nothing to do with the machine on which they were collected, but to do with the system overall. More specifically the metrics were web site performance data gathered via some cucumber and celerity scripts.
For this I knocked up a tiny web service wrapper around the gmetric command line. It’s very feature light at the moment (I only needed it to collect time based stats at regular intervals) but it could be made more featureful and expose the rest of the gmetric API if needs be. It does it using a very simple URL scheme:
So for example I can create metrics on the fly simply using an HTTP client or a web browser.
The code is up on GitHub and is completely self contained. I’ve been running it mainly using spawning but any small WSGI server could surfice. I looked very briefly at the API for Ganglia but found the gmetric approach to be much simpler.
And if you’re a Ganglia expert and know a much better way of doing this then let me know. Ganglia is awesome, and collecting metrics is both useful and fun (for me at least) but it’s not always obvious how to get into creating simple custom metrics which tell you something about your own appliction code.
I’ve been hacking on appengine again and have thrown up a simple twitter aggregator for devops. It’s again based on TwitterEngine with an increasing number of additions and changes.
As well as just the tweets I want to build a few other small features. The first of which is link extraction, so at the moment you’ll see recent links a the top of the page. I’ll hopefully make that a little more useful, with better browing and converting short urls into the real ones. I also have vague plans for providing exports, listing people talking about devops and some useful graphs to track general activity around devops.
I’ve been playing with Integrity again as a simple continuous integration server and have installed it on a few debian and ubuntu machines in the last few weeks. The current site has good installation instructions for the Ruby side of things but leaves it as an excercise for the installer to make sure all the system level dependencies are installed.
So probably as much for me in the future, here is what I had to install to get the installation instructions to work for me.
I also needed to install the following package on Ubuntu:
If you want to use a database other than the default SQLite then you won’t need those packages and I’ll assume you know what you’re doing.
I’m just getting back from Newcastle after getting to present at the first Design It Build It conference. It was great to be back up in Newcastle and to see lots of familiar faces. As with most conferences it was also good to meet new people (especially those for whom it was their first conference) and to listen to people talking about interesting stuff. Personal highlights for me were David Singleton from Last.fm and Michael Brunton Spall from The Guardian going through really interesting case studies from their respective organisations. It’s the sort of gritty content it’s often hard to come by. Speaking did mean I missed out on most of the design track unfrotunately, but videos should be available soon and by all accounts it sounds like the larger designer crowd went home happy.

I think my presentation at DIBI went OK. I’d got a little bit carried away with cramming content in, which meant it felt rushed at times and I still went over by 5 minutes into my Q&A time. But hey, a few people said nice things afterwards.
I wanted to tell the world (of web developers) about as many different tools as I could. I think most people who read this blog have probably come across most of this software, heck you might be commiting code to it or already using it in production. But lots of people haven’t ever heard of Memcached, never mind Cassandra or RabbitMQ. And more inportant than the specific software is the differnt types of tools available. Small web servers, message queues, HTTP caches, etc. Conferences are a good place to find and educate people. Hopefully I managed to do just that.
One thing I hope I got across, I’m not for a moment saying you shouldn’t be using tools you know and love. Nor am I saying you should jump in and start using lots of crazy software. But keeping an eye on new developments can serve you well when it comes to deciding whether the best approach is really to build something from scratch. I’ve spoken to several people over the last few weeks who where starting to write simple queing systems using cron and mysql, or using a hand rolled file system based caching setup. And in both cases I think they would be better served by existing tools.
My slides had a lot of links in them and I mentioned during my talk I’m put a list of them somewhere:
I’m at barcamp cambridge this weekend and decided to do a short talk on devops. It’s still a term that not too many people have come across and something that lots of people building websites should think about.

I put the slides together this morning so much of it will be familiar to people who have been reading the same blog posts as me over the last year or so. For anyone else either just finding out about the whole world of operations or old hand sysadmins finding likeminded people hopefully the slides will be useful.
So, DIBI is just over a week away and lots of people seem to be getting excited. Personally I’m really looking forward to it. I get a nice trip back up to my old home of Newcastle and get to talk geek to an audience of likeminded (and not so likeminded) web professionals. I’m also going to catch up with quite a few people I haven’t seen in quite a while.
Anyway, Twitter is at it’s best at conferences like this. It’s perfect for the where to go now situation. But I also like going back in time after the fact and looking at what people said, especially to get feedback about my talk. With that in mind I’ve build a little aggregator for all things #dibi. So far that appears to be a mixture of excitement and travel chaos.
I specifically wanted something that ran on the server, which discounted an awful lot of pure javascript versions. I think Twitter clients generally do a good job of the right now anyway. I also like to collect data for mining later, assuming I have the time. After a number of discussions on Twitter, then just some wandering around GitHub I found TwitterEngine which is fantastic. I’ve hacked it around quite a bit into something more general purpose. I’ve added some caching, some other App Engine performance tweaks and some monitoring. I’ll commit those to GitHub when I get the chance. I’d forgotten quite how much fun App Engine development was.
I still want to add a few more features before the conference. The styles work quite well on my Android but less well (text too small) on my iPod Touch. So I’ll fix that first. Any other requests?
I’ve been playing with Hive recently and liking what I’ve found. In theory at least it provides a very nice, simple way of getting into analysing large data sets. To make it even easier to show other people what you’re up to Hive has a nascent web interface with a little documentation on the wiki

On the one hand it’s rather simple at this point, but that should be easily enought to prettify given a bit of time. The bigger problem was getting it working in the first place. What follows worked for me using the latest cloudera packages on debian testing. I’m assuming you already have Hive and Hadoop installed, the basic packages worked fine for me here.
Next up you’ll need the JDK (not just the JRE) as their is some compilation that will go on the first time you run the web interface.
Next up I had to modify the installed /etc/hive/conf/hive-site.xml file as follows:
I changed this:
To this. Note the hivevar path doesn’t exist so I’m not sure if this was a typo in the source.
I also change the following section regarding the metastore name:
To this, with a fixed name. When using the above confirguration the file was actually called ${user.name} rather than my username being subsituted in. Elsewhere this seems to work fine.
I’m not convinced the above two changes are needed but have left them here just in case. The main tricky part is making sure a load of environment variables are correctly set. The following worked for me:
All being well that should allow you to run the hive command with the web interface like so:
That should bring up a webserver on port 9999 where you should see something similar to the screenshot above.
Quite a while ago I released some handy scripts for building up Django project layouts. Part of the reason behind this was to kick off discussions about ideal pproject layouts and maybe even get a few user submitted layouts into the project. Paster makes this soft of thing really easy to do and I was interested in what people might come up with.
Well, the team over at The Chicargo Tribune have done just that, creating a branch of the original project and adding a very rich example project to it
I’ve finally gotten around to merging that back into my branch and releasing a new version to PyPi. It’s got a good number of PostGreSQL commands that might be of use, as well as several Amazon S3 and EC2 examples. Even if none of these complete templates fit what you’re looking for exactly their are lots of smaller fabric recipes worth taking a look at.
You can read all about what is in the project over on their developer blog, which if filled with other interesting web and operations goodness.
So, huge thanks to Chris, Brian, Ryan and Joe for some great work.
I’m a huge fan of virtualenv for Python. It’s a simple tool that lets you have an isolated python environment into which you can install libraries via setup tools. It makes experimenting with different versions of code easier and avoids lots of problems with hard to find bugs caused by unknown third party conflicts.
Sandbox aims to do exactly the same for Ruby. It isolates your gem installation from the system libraries and providesa script to activate the named environment. The GitHub repo has been around since 2008, yet only has 37 followers. I’m not sure whether that is because another more popular and feature rich solution exists or because Ruby people haven’t yet found this technique useful. For me finding tools that work the same way as those I already like in other languages makes me happy.
My latest on a train project is Dumper, a static generator for web services. I’m a huge fan of Nanoc and tools like Jekyl for building websites. But I spend at least as much time building small webservicds. I wanted something super simple that would let me expose data I had access to as a read only web service.
At the moment that means using a mysql database, specifying a SQL query and running a python script. Hey presto you have lots and lots of XML and JSON files representing your data. Dumper provides hooks for you to customise the ourput or even overide the database layer. It should be possible if you were so inclined to replace the mysql backend with another database, or other type of data store. Hopefully some of these might end up in my branch at a later date.
At a basic level all you need is a config file which looks a little like:
And then run a command line application against that file:
The application supports a number of flags for specifying where you want the files to be generated, what your config file is called and to clean up any generated files if you want to try again. The output will let you know which files have been updated, which deleted and which added too. If you’d rather have a single file but with all your records in then that’s easy too – just add something to the config file.
It’s somewhat early days for Dumper, and I’ve not seen anything similar so their are definately some rough edges that could do with some work. All of that will really come down to how much use it gets. I’d appreciate any feedback from anyone with a similar itch to scratch too.
I’m a big fan of Piston, the django app for creating RESTful web services. As part of a project at work I ended up looking through the source code, mainly at some of the neat tricks of serialisation of objects. While poking around I came across something in my mind that wanted fixing. This being open source rather than just file a bug report I setup a bitbucket account and got hacking.
Can you spot the problem? Note the use of the callback passed in the query string arguments and then used without any checking in the output.
What we really want to do is something like this:
Which is exactly what has just gone into the code for Piston. This article contains lots of background information about why JSONP callbacks can be a security hole, and helpfully provides a nice Python module to help with the sanitisation. Nice to be on the authors list for something I’m using actively.
Thanks to Jesper Noehr for Piston, for some pointers on bitbucket and for quickly taking the patch. If you’re accepting a callback on your site or application, especially if it’s a public service, you really want to do something like this or you just might have an exploitable security hole.
It’s taken longer than I would have liked but I’ve finally gotten around to relaunching this site on nanoc.
After looking through lots of code from the nanoc showcase I had a pretty good feel for how I wanted things to work and I then used the excellent nanoc3_blog template to get started. I’ve hacked around quite a bit with the code to get things how I wanted them. Using Less to make the CSS more manageable, Coderay for lovely syntax highlighting and making everything default to textile rather than markdown. I’ve also written import scripts for my old blog (in Python) and another one so I can use tumblr is I want to create items on here (in Ruby).
Nanoc really is a joy to work with and I’m hoping that alone will get me back into writing more freqently than I have done for a while. The fact I can just write in Vim or WriteRoom or whatever editor I have to hand feels nice. And using Git, Rake and Rsync complete my little toolset. Everything is still served via Nginx.
I’ve thrown the all the code, including all the content, up on GitHub for anyone interested. Back to writing.
I just noticed Lindsay had committed the amqp steps for cucumber-nagios and remembered I hadn’t mentioned on here some other work I’ve been doing on the same project. We use MySQL quite a bit at work and I’ve been wanting to extent our monitoring for a while. So I set about thinking how that would work with cucumber-nagios. What I’ve come up with looks something like this:
The numbers, username details and host details are all variables. So you can write senarios for your specific deployments. The tests over time are based on a very short lived sampling mechanic which I’ve yet to test in anger. I’m not sure just yet is this approach will lead to too many false positives but we’ll have to see.
This mysql gmetric script gave me lots of the ideas for invidual steps. I’ll be writing more about some work I’ve been doing with cucumber-nagios and ganglia soon as well.
For the moment if anyone want’s to try these steps out you can either check out my cucumber-nagios fork or just grab the steps from the mysql_steps.rb file. Any feedback much appreciated.
Every now and again I feel the need for a change and spent a little time tonight looking at different blogging software. I’m currently running a custom django app I wrote a good while ago, more as an excuse to play with Django than anything. Previously I’ve used Wordpress, Textpattern and even Radiant. But I’m coming to the conclusion that what I want doesn’t exist. In theory that means an opportunity for someone to enter the market, in reality I think it might just be me. So, what do I want?
I have a sneaking suspicion what I might be thinking about is a private tumblr blog used as a datasource for Nanoc3. But that relies on me having the time to build that as I can’t find anyone who might have written such a think. Maybe one day.
The next Django User Group London is in two weeks time. You can register over on Eventwax. So far we have Brad talking with more speakers to be announced shortly. Hope to see a few people there.
I was just thinking about the Design it Build it conference later in the year (full disclosure: I’m speaking). Specifically the people speaking on the developer track. Between myself, Michael Brunton-Spall from The Guardian, David Singleton from Last.fm and Emma Persky from Gumtree four of the six speakers work on in-house teams. Not early stage start-ups, not large software/advertising companies, not as freelancers but in a reasonable sized company on a development team.
My original background was working in agencies, and then a stint working for myself and I’m constantly interested by the different facets of the web software industry. I think conferences or magazines aimed at your average interested web developer or designer play an interesting role in what people perceive as normal. If all you see are people who work as a freelancer you start to think that must be way cooler than whatever it is you’re doing at the time. I remember attending the first @media event and being surprised at the small number of people from larger agencies. Everyone was from smaller boutique places, or Yahoo! or a freelancer. Now lots of people I see at events are involved in startups.
Interestingly as well none of it is about a particular language or framework. Thinking about it as I type I think between the four of us we spend are day jobs mainly using different languages (java, python, php, perl). But I bet we all work in environments where we use other languages at least occasionally, or at the least the people around us do. Mixed environments are commonplace in companies that have been around a good while and run on software. They are far less common elsewhere with startups using whatever is cool (lets build a mobile search engine in Haskel anyone?) and small agencies often using whatever they built their first client website with (probably PHP).
What I’m really interested in though is the type of topics that are going to be talked about. Last.fm vs the Xbox, Scaling the Guardian, my rambling thoughts on a modern toolbox for developers beyond your average LAMP or .NET stack. This is the sort of think I’m interested in. It’s the sort of problems I like having. It’s also, I think, the sort of stuff that doesn’t get a showing a many mainstream conferences. I’m hoping it’s all going to be fairly practical too – things that whatever role people have they can take away and apply.
I’ve been doing more operations related work of late and am starting to use Cucumber-nagios for various monitoring tasks. Nagios might not be the most attractive of web interfaces but it’s so simple to get clients up and running and extend to do what you need. Cucumber however has a lovely, text based, user interface. And although I’m mainly working with Python at the moment cucumber-nagios (written in Ruby) really is the easiest way I’ve found of writing simple functional tests.
Cucumber-nagios is the creation of Lindsay Holmwood and after several brief conversations over Twitter I set about adding a feature I wanted for my own monitoring setup. Namely support for keeping an eye on RabbitMQ.
At the moment the code is in a fork on GitHub but I’m hoping that once any rough edges have been ironed out and a few people have kicked the tyres then it will make it’s way into trunk. If you want to use this with an existing project straight away you can always drop the contents of amqp_steps.rb into your feature steps file after installing the amqp gem.
I’ve included a little documentation in the fork as well with a quick example:
My main usecase was to keep an eye on a known queue size and number of consumers. I’m sure I’m missing some features at the moment so any feedback much appreciated.
I found myself using a couple of powerful but underused command line applications this week and felt like sharing.
My problem involved a large text file with over three million lines and a script that processed the file line by line, in this case running a SQL query against a remote database.
My script didn’t try and process everything in one go, rather taking off large chunks and processing them in turn, then stopping and printing out the number of lines processed. This was mainly so I could keep an eye on it and make sure it wasn’t having a detrimental affect on other systems. But once I’d run the script once (and processed the first quarter of a million records or so) I wanted to run it again, except without the first batch of lines. For this I used sed. The following command creates a new file with the contents of the original file, minus the first 254263 lines.
I could then run my script with the input from new.txt and not have to reprocess the deleted lines. My next problem came when the network connection between the box running the script and the database dropped out. The script printed out the contents of the last line successfully processed, so what I wanted was a new file with the all contents of the old file past the last line. The following awk command does just that, assuming the last line processed was f251f9ee0b39beb5b5c4675ed4802113.
Now I could have made the script that did the work more complicated and ensure it dealt with these cases. But it would have involved much more code and the original scripts where only a handful of throw away code. For one off jobs like this a quick dive into the command line seemed more prudent.
I’ll be heading back up to Newcastle in April to give a talk at what’s shaping up to be a good looking conference to kick off the year with. DIBI is trying to please everyone, with both front and backend focused streams.
I’m not a big fan of making a point of dividing frontend and backend work. You nearly always end up with javascript dominated horribleness (because we only had a front end person available) or a so called content management system that means all sites have to look the same except for the colour palette. So I’m hoping lots of cross over stuff happens and interesting conversations abound.
Oh, and if you’re wondering what I’ll be speaking about it’s probably going to be something about all the cool tools you could and should be using when building or looking after web applications. I’ll probably be doing my best to convince people to look outside the comfort of the LAMP or C#/MSSQL stacks and realise the future for lots of web developers might just be more devops.
I’ve just found Dreque from Samuel Stauffer on GitHub. It’s yet another take on the whole messaging things which is definitely seeing a lot of activity at the back end of this year. It’s using Redis on the backend and looks really rather nice:
Submitting jobs:
Worker:
As mentioned at the last event I’ve taken over organising the Django User Group London event from Rob. Tickets are now available for the next event which is going to be on the 3rd of December at The Guardian offices in Kings Cross.
You can sign up on eventwax
I’ve been trying to learn Erlang for a while. What I actually mean is it’s been on my list of things to learn for months, along with all sorts of other incredibly interesting bits and pieces. I spend a little bit of time at home but the majority of my learning time is now spent commuting to London and back most days. Sometimes I’m even going all the way to Swindon which gives me even longer to not learn Erlang.
The main problem with learning something new on the train is space. Reading a book (or my new Kindle) or just using my laptop is fine. Trying to do both at once is nearly impossible (I’ve tried). So I’ve decided to give another approach a try, namely screencasts.
I’ve only done the first Erlang in Practice episode so far but I was hugely impressed with the content and general presentation. $5 as well doesn’t seem bad at all I don’t think. The episode was half an hour long, but took me a little longer, probably closer to 45 minutes, as I was playing along at home and typing the code examples as I went. I also got sidetracked with messing with my vim configuration at the same time but hey. This makes them perfect for my hour long commute. The full series is 8 episodes long and with luck I’ll be able to work through them this week.
So, good job Kevin Smith and Pragmatic for a nice, accessible start to Erlang. All I need to do now is find something interesting to hack on in Erlang.
I’ve been lurking on the django-developers mailing list for the last couple of weeks and that provided an excuse to play with the new Twitter Lists feature. So here’s a list of djangocommitters on twitter. If I missed someone do let me know. Their is a chance you won’t be able to see this if you’re not on the beta yet I think, sorry!
The Hadoop wiki has a great introduction to installing this piece of software, which I wanted to do to have a play with Dumbo. The Dumbo docs also have a good getting started section which includes a few patches than need to be applied.
Unfortunately it’s not quite that simple, at least on Ubuntu Jaunty. Hadoop now uses Java6, but if you just follow the instructions on the wikis you’ll hit a problem when you run ‘’ant package’‘, namely that a third party application (Apache Forrest) requires Java 1.5. Once you fix that, the build script will complain again that you need to install Forrest. Here’s what I did to get everything working:
With all that out of the way you should be able to run the simple examples found on the rather excellent dumbotics blog. If you’re using the Cloudera distribution, or when the Hadoop 0.21 gets a release, these problems will disappear but in the meantime hopefully this saves someone else a bit of head scratching.
I’m keep meaning to get around to writing about why I think the future of web developers is operations but in lieu of a proper post here’s a list of things I’ve been spending my work life getting to know this month:
I do wonder if it’s just me that’s drawn to knowing how everything in the full web stack works. But personally I can’t just write code if I don’t understand how to deploy it or what it’s running on. Front end types know this all too well. Being a master of CSS, HTML and Javascript simply isn’t enough. You need to understand the browser to get anything done. I’m not sure it’s the same for all backend inclined folk; how many PHP programmers really understand Apache and a few other useful bits of web tech?
Thanks to Brad I’ve just released a new version of Django Test Extensions (also on GitHub with support for running tests without the overhead of setting up and tearing down the database. Django still has a few places were it assumes you’ll have a database somewhere in your project – and the default test runner is one of them.
On the first day at Barcamp Brighton this year I did a brief talk about getting started with automating deployment. I kept it nice and simple and didn’t focus on any specific technology or tool – just the general principles and pitfalls of doing anything manually. You can see the “slides on Slideshare”:

As part of the presentation I even did a live demo and promised I’d upload the code I used. The following is an incredibly simple fabric file that has most the basic set of tasks. Fabric is a python tool similar to capistrano in Ruby. I don’t really care whether you’re using make, ant, rake, capistrano or just plain shell scripts. Getting from not automating things to automating deployments is the important part – and it’s easier that you think.
The other part of the code example was a very basic apache virtualhost so just in case anyone needed that as well here it is:
DJUGL is back, the monthly Django meetup in London. I think the last few times have been as much about useful Python stuff as just using Django, and this time it’s officially a bit more broad ranging. If you’re in or around London on the 24th September then come along.
You can get more information on Twitter or by following Rob. But expect a few short talks, some interesting conversations and maybe some beer with other like minded developers.
I’m going to be talking about automating deployment of Python web applications. If you follow me on Twitter you’ll have heard me rambling a little about some of what I’ve been up to, and some of the posts here give an insight into what I’ve been working on. But the short version is that several friends mentioned how difficult it could be to get a working Django application from a local machine to a production web server. And I though I better get down in script form my experiences of Django, WSGI applications and web server setup to make things easier.
I think this situation is partially caused by the success of the Django development server, and partially by people coming from a PHP background. In my PHP days I think I always wanted to know how Apache did it’s think, so long ago jumped into anything and everything in httpd.conf from loading modules to virtual hosts. But not everyone does the same, and PHP does make simple deployments easy enough that you might get away without doing so. Rails went through the same problems and seems to be coming out the other side. I’m hoping that Django and Python is soon to be in the same position, where basic deployment is just a given.
Now I’m generally an Ubuntu guy, but I’ve just had the need to setup some boxes running Solaris for Django and a handful of WSGI applications. I know my way around Ubuntu pretty well. I know all the packages I need to install and in what order. Hell, I even have all that scripted so I can just run a command and it works by magic. I’ll script the following steps if I can do when I get round to it but here, in one list, are the installation instructions for Apache, mod_wsgi, Mysql, MySQLdb, setuptools and memcached that worked for me on the latest version of Open Solaris (2009.06 at the time of writing).
First up I needed to install Apache and start the service running.
You should be able to test that’s running by hitting localhost on a browser running on the same box. Now for MySQL.
This installs the mysql binary into /usr/mysql/5.0/bin/mysql on the system I’m working on. As I want to talk to the MySQL database server using Python I need to install MySQLdb.
This installs the library files into /usr/mysql/5.0/lib and Python doesn’t know were to find them. The above command links them into the more standard /usr/lib folder were Python will pick it up nicely.
I tend to use mod_wsgi for serving Python apps behind Apache,  however a mod_wsgi package isn’t part of the default package list. It is however available in the pending list so first you need to add that list of packages.
This installs the module but you then need to tell Apache to load it. Add the following line to /etc/apache2/2.2/conf.d/modules-32.load or /etc/apache2/2.2/conf.d/modules-64.load depending on your architecture.
To get Apache to load that module you need to restart it like so:
I use Pip for installing Python code, but tend to install setuptools to make installing Pip easier. I don’t know if an up to date Pip package exists.
This should leave you with easy_install on your path so installing Pip, then virtualenv should be a breeze.
As an added bonus I also installed memcached for some snappy caching.
This won’t start up by default and needs a little configuration. The first command will launch you into a prompt where you can type the rest of the commands.
Once you’d done that you should be able to start memcache on the standard port.
Et voila. The internet helped massively on my quest to track down this information. Not all of the following links turned out to work for me but all of them led me in the right direction. Thanks everyone.
I’m not a Solaris admin. I’m not really a sysadmin at all, I just end up pretending to be one of late. Any experienced Solaris people with experience of these tools reading this I’d be grateful for any hints and tips. Hopefully this saves a few people from the head scratching I’ve been doing for the last few days.
I’ve been playing with automating Django deployments again, this time using Fabric. I found a number of examples on the web but non of them quite fit the bill for me. I don’t like serving directly from a repository, I like to have either a package or tar I can use to say “that is what went to the server”. I also like having a quick rollback command as well as being able to deploy a particular version of the code when the need arises. I also wanted to go from a clean ubuntu install (plus SSH) to a running Django application in one command from the local development machine. The Apache side of things is nicely documented in this Gist which made a good starting point.
I’m still missing a few things in this setup mind and at the moment you still have to setup your local machine yourself. I’m probably going to create a paster template and another fabfile to do that I think. The instructions are a little rough as well at the moment and I’ve left the database out of it as everyone has there own preference.
This particular fabric file makes setting up and deploying a django application much easier, but it does make a few assumptions. Namely that you’re using Git, Apache and mod_wsgi and your using Debian or Ubuntu. Also you should have  Django installed on your local machine and SSH installed on both the local machine and any servers you want to deploy to.
note that I’ve used the name project_name throughout this example. Replace this with whatever your project is called.
First step is to create your project locally:
Now add a requirements file so pip knows to install Django. You’ll probably add other required modules in here later. Creat a file called requirements.txt and save it at the top level with the following contents:
Then save this fabfile.py file in the top level directory which should give you:
You’ll need a WSGI file called project_name.wsgi, where project_name  is the name you gave to your django project. It will probably look like the following, depending on your specific paths and the location of your settings module
Last but not least you’ll want a virtualhost file for apache which looks  something like the following. Save this as project_name in the inner directory. You’ll want to change /path/to/project_name/ to the location on the remote server you intent to deploy to.
Now create a file called .gitignore, containing the following. This prevents the compiled python code being included in the repository and the archive we use for deployment.
You should now be ready to initialise a git repository in the top level project_name directory.
All of that should leave you with
In reality you might prefer to keep your wsgi files and virtual host files elsewhere. The fabfile has a variable (config.virtualhost_path) for this case.  You’ll also want to set the hosts that you intend to deploy to (config.hosts) as well as the user (config.user).
The first task we’re interested in is called setup. It installs all the  required software on the remote machine, then deploys your code and restarts the webserver.
After you’ve made a few changes and commit them to the master Git branch you can run to deply the changes.
If something is wrong then you can rollback to the previous version.
Note that this only allows you to rollback to the release immediately before the latest one. If you want to pick a arbitrary release then you can use the following, where 20090727170527 is a timestamp for an existing release.
If you want to ensure your tests run before you make a deployment then you can do the following.
The actual fabfile looks like this. I’ve uploaded a Gist of it, along with the docs, so if you want to improve it please clone it.
With the release candidate for Django 1.1 out the door I decided to have a quick look at what’s new. This isn’t a complete list, rather the bits I found most interesting.
A nice set of decorators for dealing with ETags and Last-Modified headers. Again very simple to use and set up, and a simple way of squeezing a little more performance out of you application.
Anything that makes the admin a little more powerful and a little more flexible is a good idea in my book. Admin actions allow you to run code over multiple objects at once, simple select them with a checkbox then select an action to run. This is worth it for the delete action alone, but you can write your own actions simply enough as well (for instance for approving a batch of comments, or archiving a set or articles.)
Another time saving admin addition, this time for making some fields editable from the change list rather than the object view. For quick changes, especially to boolean fields, I think this again is a nice addition.
I particularly like this addition. One of the issues I had with Django was some of the built in assumptions, in particular that you’d be using a SQL database backend. Using unmanaged models looks like a great approach to using an alternative database like couchdb, tokyotyrant or mongodb or representing a webservice interface in your application.
I’m sure I’ll have missed a few other interesting changes or additions. Anyone else have a favourite?
Asteroid is a simple web interface for running scripts and recording the results. It’s like a much simpler and more general purpose version of something like Cruise Control. You can get the code on Github.

I built it to solve two main problems:
So it should be useful for running deployments, running test suites, running backups, etc. It currently doesn’t have scheduling or similar build in, but as everything is triggered by hitting a URL it would be simple enough to use cron for something like that. It should also be useful whatever language you write your scripts in; rake, ant, shell scripts, etc. At the end of the day it just executes a command at the console.
Asteroid uses the Django Python framework under the hood.
You’ll also need a database. The default in the shipped settings is to use sqlite but this should work with any database supported by Django.
You’ll also need a decent web browser. I’ve gone and used HTML5 as an experiment and with this being a developer tool I’m hoping to stick with it. It would be easy enough to convert the templates if this is a problem however.
The application has an optional message queue backend which can be enabled in the settings file. This is used to improve the responsiveness of the application as well as allow commands to be executed on a remote machine, rather than on the box Asteroid is running.
Other AMQP compliant message queues should work but it’s currently only tested with Rabbit.
If you are intending to do any development on Asteroid, or just want to look more closely at the code, I’d recommend installing
You should be able to just download asteroid and run it from wherever you put it, once you setup the database.
This should bring the local web server up on port 8000 so visit http://localhost:8000 and see.
If you’re using the message queue backend you’ll need to run the listener script in order to get your commands executed. At the moment that means modifying a constant in the listener script to point at a running message queue instance at asteroid/bin/asteroid_listen.py.
Once you’re up and running you should be able to add commands via the admin interface at http://localhost:8000/admin/. The username and password should be those you added when creating the database via the syncdb command above.
The development configs include a few additional applications (mentioned above) which I use for testing and debugging. You can run the test suite like so:
This is an early release that just about works for me. I can already see a number of areas I’d like to clean up a little or extend. For instance:
I’m pretty happy with how it’s shaping up so far. Under the hood it works by having the web app put a JSON document on the message queue. The JSON contains the command to be run and a callback URL. The script listening to the message queue picks up the message, runs the command, and posts a JSON document back to the webhook url. It keeps the web interface snappy, as well as meaning it can show which commands are currently in progress at any given time. It also has the side benefit of meaning you can execute commands on a remote machine, as the listener doesn’t care where it’s running.
As noted above I have a few ideas of where I want to take it, but I’m going to try using it for a bit and see how that goes. If anyone else finds it useful then do let me know.
I feel a little self adsorbed quoting myself (from a recent Refresh Cambridge discussion) but I did like the turn of phrase. What I was rambling on about was Cambridge County mapping data, after a question from a nice chap from the council about what “new, exciting map technology” we’d like to see. But it applies to any data that you’re trying to make public what-so-ever, be it government or otherwise.
What myself and a few other people were talking about, and one of the things that has been discussed as part of the Rewired State group, is that it’s all about the data, not necessarily about a nice web based API.
Now I’ve written and spoken about the need for well designed API’s being treated as part of the user interface. But remember interface design, and by association API design, isn’t easy. API design is often about building manageable flexibility. A public API is often about managing the flow of data you control out to third parties, as well as the information itself it might include limitations on usage, or request rate, or storage. A public API codifies how that information can be accessed. APIs also have to tread a fine line between making it easy for you to solve your problem, and making it easy for everyone else to solve their completely different problems. These compromises are design.
But not everything needs an API. Sometimes it’s just about the data, and the best way of getting at that data is as raw as possible. Government data is an easy sell here, as it is (or rather should be) our data. It’s also for the most part interesting to read rather than write (historical council tax data, or population data for instance). Raw data can generally be provided quicker than via an API. It doesn’t need fragile computer systems or extensive manual labour. It doesn’t need particularly clever computing resources. Just upload a spreadsheet or a CSV file to a sensible URL on a known, regular basis and away we go.
And giving data like this away to the development community is likely to have a few additional benefits if that data is useful (it probably is to someone). We’ll happily write software libraries, or create APIs over the top of it for you. We’ll also write all sorts of useful tools using the data in ways no one else thought of. So if you’re sat on a load of data that’s not core to your business, or is meant to be public anyway, then lets start talking publicly about how to just get this out on the web quickly and cheaply, rather than spending lots of your time and money on something fancy.
I wasn’t going to write anything about the whole XHTML2 thing. I noted its passing, got a nice message on Twitter and thought that would be it. But no. The web standards world exploded. I honestly didn’t see that coming.
Let’s get a few things straight:
Web Standards are interesting, in that they are standards for both implementors (browser makers) and for authors (us). I like coding standards in programming languages too, it’s one of the things I love about Python and PEP8. But with these standards it’s not about making your code work, it’s about shared conventions and readability. So common spacing, UPPERCASE for constants and Leading caps for class names for instance. It’s also about having a tool to check everyone is adhering to standards, like pep8.py or FXCOP for .NET. If everyone writes code in the same way it’s easier to read, write and to pick up someone else’ code. You can do that with HTML, but you have to do that with XHML.
Now the whole HTML 4.0 vs XHTML 1.0 thing has come up lots of times, on mailing lists, at conferences as well as down the pub. I know on occasion me, Drew, Rachel and Jeremy side against Simon and Nat on the issue. But what’s interesting is that I think we all agree on all the typographical conventions stuff. My former colleagues with a passion for front end standards and HTML 4 did the same thing. I even remember Simon looking for ways to validate against HTML 4 but also to check for all lower case elements, closed paragraphs and the like.
Which brings me to the reason why I use XHTML: The validator enforces my preferred coding standards for HTML – lowercase elements, quoted attributes and closed elements. That’s it. Not much really. I know it’s marketing XHTML rather than technical XHTML. I don’t care. Or rather I do care, I just make a conscious pragmatic decision based on a small personal advantage. I’m both pedantic and like having a tool chain which enforces that, XHTML suits my style.
The markup language debate is being talked about in terms of pragmatists vs purists. But ignoring the people who both really understood and really wanted XHTML2, it’s mainly the pragmatists arguing amongst themselves now. Some of them are big company people, others working for themselves. Some have standards or academic leanings, others are rooted in commercial web design. Some people probably work on huge long term projects, others relatively small sites and apps. And I think it’s these cultural differences that are the root of arguments now. So blog posts coming out saying the same thing but arguing with other people give a strange impression of disagreement. Throw in that the web lends itself to popular blogs gathering a crowd of like-minded people around them and hey presto we have people feeling unfairly put upon and getting agitated.
What a storm in a teacup. Who doesn’t genuinely think the best approach is to use whatever you’re using now for most projects, investigate HTML5 as time permits, and then expect to start using HTML5 in bits and pieces in the short to medium term, when being mainly dependent on your target audience?
In my opinion the only genuine problem that this saga has highlighted is the fear, uncertainty and doubt around all flavours of HTML amongst a large number of web professionals. People don’t get this stuff at all. With the added resources soon to be put into the HTML5 working group at the W3C this outreach and education side of the project has to have just as much love and attention as the spec itself.
One of the projects that came out of the Django Dash recently was PyPants which I’m finding very cool.

It’s basically a quality tracking service for Python modules. For instance my recent UrlTest module has a page on PyPants, scoring a good B grade after some cleanup work earlier today.
Under the hood I think it’s probably CheeseCake which is available as a command line application, maybe with a hint of PyLint and pep8.py thrown in. But the nice interface, as well as tracking of scores over time, really add something. GitHub has been credited by some as making sharing code more fun, I’m hoping projects like PyPants can do the same for quality in Python code.
Congrats to Eric Holscher, Travis Cline, and Nathan Borror on a fantastic addition to the Python community.
I’ve been meaning to add some of my code to the Python Package Index for a while and  have finally gotten around to it with Urltest, my simple DSL for testing WSGI apps.
You can now find it at pypi.python.org/pypi/urltest and install it using setuptools with:
At the moment I’ve not added any categorisation or detailed description to the setup.py file, I’ll be doing that soon. I wanted to get it working with the absolute minimum setup.py file, which turned out to look like:
Uploading it to PYPI itself was incredibly simple, partly as I was already using setup tools for local installation.
Let me know if anyone uses this and gets it working. I’ll be adding more details and maybe even some more features when I get the chance. Once I do that I’ll probably work on a few more packages as well.
Following on from yesterdays first useful ant task, here’s another commonly used task – restarting a remote service. I’ve used apache in this example, but it could be any service running on your remote machine and it doesn’t have to be the restart command.
In order to do this we’ll use the sshexec target which has a third party library dependency. This is the same third party library needed for the scp task in yesterdays post
You need first to download JSCH and then compile the source using ant. Just run ant dist in the downloaded folder and you should get a .jar file in the /dist/lib folder. Save this .jar file as as jsch.jar to a folder in your home directory ~/.ant/lib where ant can automatically load it. Alternatively you can run ant with the -lib command to load libraries from a different location.
With that out of the way lets have a look at the task.
And running it is as simple as:
One potential issue with tasks like this is storing the password in the build file in plain text. The target we’re using can also use key authentication is you’re happy using ssh keys. Alternatively you can set properties on the command line each time you run ant like so.
I occasionally get carried away with Apache Ant. For those that haven’t come across it, Ant is a build tool written in Java, using an XML syntax to describe a series of repeatable tasks. In your typical web standards savvy, dynamic language favouring, web developer types that description is probably all they (think they) need to know. It’s Java. It’s XML. It’s only really useful in the context of building software (dull).
But I think Ant is a particularly handy tool to have around for anyone working on even simple websites. A couple of strong use cases come to mind:
So with all that in mind I’m going to try and do a series of posts each covering a single task, aiming to cover things that your regular web developer will find useful. With that in mind if anyone has any requests or questions let me know either by email or in the comments.
Out first task lets us backup a file from our remote web server, in this case it’s the apache2.conf file used to setup apache. Obviously it could be any file you want to get hold of. The example below has a couple of properties for the username and hostname of the remote machine. Save the following snippet into a file called build.xml and place it anywhere you like on your machine.
Running the task, once you have ant installed (it comes already installed on OS X and is generally available in whatever linux package management system you prefer), is as simple as typing the following into a console.
This should download the apache2.conf file to you local machine, into the same directory as your build file.
The above task requires that you have scp installed on your machine, which is pretty likely if you’re using OS X or Linux. Ant comes with an inbuilt scp task, but it requires you to install a separate java library. If you’re happy doing that then you can write tasks like:
Ruby people really don’t like CSS do they? But Less is actually pretty cool. It’s basically an attempt to bootstrap features, specifically Variables, Mixins, Operations and Nested Rules, into CSS. The best part about this is it uses CSS syntax and a simple one step compiler. I’d be interested to know what the folks at the W3C think about this.
So for instance you can do:
and compile it down to:
Last night was the first Cambridge Geek Night and saw 35 people or so fill a room above a pub to listen to a few short talks and converse with fellow geeks. I had the pleasure of giving the first talk, a short introduction to using message queues for web developers.

I got lots of good questions from interested people and by the sounds of things it had the desired effect – for people unfamiliar with using a message queue to go out and have a play with some of the cool software available to solve your problems.
Overall the night was definitely a success. Suitably geeky conversations. A chance to meet new people as well as old friends. Good job Vero and David for organising the event and here’s to the next one.
Neil Crosby got me thinking yesterday about which language to learn/play with next by tweeting
Neil appears to have gone for Python, but more specifically I’m interested in how you decide what to learn next? And improving the likelihood of you seeing it through and being able to add it to you toolbox. Personally I’ve messed around with a wide range of languages but I would say I’m proficient in only a few of those.
So here goes with a list of questions to ask yourself.
I don’t think all of these apply to everyone or apply all the time, but it’s worth considering and rejecting them when they don’t.
Feel free to disagree in the comments or, even better, add extra ones. Or alternatively just cut to the chase and tell me what I should learn next.
I’ve updated my vanity domain at garethrushgrove.com with a bit of information in case anyone might be interested in my services.

The short version is I’m on the lookout for future projects, probably of a freelance or contract basis but if it’s particularly interesting then maybe a full time position. Basically I’m in quite a nice position and able to wander about a bit looking for something cool to do.
If you reading this site then you know what floats my boat. Python, testing, automation, system design, maybe get into Ruby or another language properly, etc. I’d particularly like to help people get started with testing, continuous integration or automated deployment and the like.
It’s the first Cambridge Geek Night next week, on Wednesday the 10th to be precise. You can find more information on Upcoming or on the Cambridge Geek Night blog. The event also has a twitter feed to keep up with goings on. If you’re in Cambridge you don’t want to miss the perfect combination of geeks and beer.
I found myself wanted something to make writing high level, functional tests for WSGI application easier and quicker. If I liked the term I’d call it a domain specific language for testing URLs. Basically I found myself writing a lot of tests like:
Testing more than a view URLs like this got boring quickly. What I wanted was a short hand syntax for defining this sort of simple test and then running them all individually. So was born Urltest. It uses the rather nifty Webtest module and hooks into unittest from the standard library. You’re test script then looks a little like:
Let me know if you use it as at the moment this is works for me ware, although it’s reasonably well tested and commented.
Although still a big fan of Django, but for some problems I’m finding more and more cases where I prefer less code and more freedom. My biggest issue for some types of problems being Django’s assumption that you’ll be using a relational database, or a database at all. Django wasn’t the reason I started using webapp for App Engine stuff, but in doing so I found that webapp often did all that I needed.
So when I small, non appengine project cropped up I started looking at the different options available and played with a few of them.
So, I set about forking MNML to create my own branch. I added extra comments as I was making my way through the code, wrote a few tests to checks thinks worked and allowed for pluggable routing mechanisms. MNML applications look a bit like the following:
If you want to use the token based routing you would substitute in something like the following:
The best bit is that it’s only about 350 lines of code, a great deal of which is accounted for by comments. It’s also really quite fast – especially using something like spawning to run the WSGI application. The other thing I like is the ease with which you can add WSGI middleware into the mix.
So, if you have a small scale problem where simple and fast beats everything else then have a look and let me know what you think. It will take less time to read the code and tests than it will be read the introductory chapter on whichever larger framework you choose to look at.
I did a talk at the recent barcamp North East on web development tools. Specifically I wanted to talk about the fact that an awful lot of people just use the basic stack of tools they are familiar with. So Microsoft people will just use C#, MSSQL and ISS and lots of people just use PHP, MySQL and Apache. I’m not saying their is anything wrong with those tools, but if they are all you have in your tool box you’re limited how well designed your software can be.

I’d knocked the presentation together in my hotel room pretty quickly before heading down to barcamp and the lack of an internet connection meant I didn’t have links and didn’t cover a few tools I should have. It did seem to have the desired result in any case as several people spoke to me afterwards about wanting to use one of the tools I mentioned for something specific.
We also had a good discussion afterwards and people mentioned a few other tools.
Now it’s possible to spend too much time playing with small tools that are likely to be peripheral to the bulk of your application. But the number of stories I’ve head of people writing their own messages queuing systems in PHP, or using PHPMyAdmin as an application admin interface or ignoring the fact that their fancy new application only supports a few people at once.
Andy Clarke, as only he can, has started something of a slagging match with his proposals for a single, central IE6 stylesheet. My first impression was that this is basically a much better version of the browser defaults.
Between backslapping and shouts of heresy there are a few good comments floating on the post so far (I’d expect more). But most of them seem to assume only two types of website exist:
Obviously that covers only a fraction of web pages, but it covers a much larger proportion of web sites. What do I mean by that?
Most people (ok so this might be my opinion) seem to work within small to medium sized agency style places. Smallish companies need to rely more on employees as part of their marketing effort, younger people tend to be more militant and people working for smaller organisations (like Andy) tend to benefit more from a little celebrity. But that ignores an army of people who work in house or in other types of company that just happen to build top notch web sites or applications. I’ve now worked in everything from small agencies, via medium sized agencies to freelance and inhouse. They are all different and all place different types of time pressure on the people involved.
Another argument to be had here is summed up by Sion (http://twitter.com/sionnnn/status/1872141409)
And this is pretty much how I used to work. But this means ignoring whole swathes of the unsupported parts of CSS2 and in particular CSS3. CSS3 doesn’t really allow you to make designs that you couldn’t make before (ok, that’s a little unfair, maybe) but it does allow you to do what you did before much more efficiently. Multi-column layout stuff, multiple background, rounded corners, opacity, RGBa. Smaller stylesheets are easier and quicker to write, test and maintain. And sometimes the time saved is worth more than the additional overhead of writing and maintaining CSS for IE6. Maybe not for a decent sized consumer project with a reasonable team and a few hundred thousand budget. Maybe not for a local council or government site. But for a surprisingly large range of other types of projects this might be a worthwhile approach.
Something else I think that comes through in many of the comments is that it’s often seen as the web designers job to fight for things. So we fight to ensure time is spend on making something accessible. We fight to make sure we use valid code wherever possible. We fight for sensible fallbacks when javascript isn’t available. But we also get accused of not being able to see both sides and at times being unrealistic. Worth considering with regards peoples initial reactions to this, mine included.
The only thing I would say to anyone looking to use this is don’t create your own version. Use the one from google code. As Andy suggests, do suggest improvements, ideally by making the changes yourself presumably along with a solid test case. The time saving benefits basically disappear if we all have to maintain our own versions, before that means we have to test our own versions.
I’m not saying ignore IE6, and neither is Andy (I don’t think). I’m saying pick your battles. I’ve build websites to support everything under the sun. Sometimes it’s been absolutely the right thing to do, sometimes in hindsight it’s probably been a little bit of wasted time. I’m sure you can think of similar times from the sites you’ve been involved in.
After what seems like longer than a year I’ve finally managed to make it back up to Newcastle. It’s the Thinking Digital conference again this year and so far it’s been a hoot. A mix of practical, inspirational and just odd speakers (and acts) suits me pretty well. Lots of twitter activity too.
The highlights for me so far I think has been Dan Lyons talking about the future of media and print businesses. Basically a somewhat rambling attempt to describe where newspapers and print publications find themselves in this day and age (in trouble) and where they might head for salvation (either dumbing down or becoming more exclusive/expensive). Throw in interesting stories and anecdotes, the odd joke and some personal thoughts on the future for journalists and everyone’s happy.
An honorable mention goes to Johnny Lee Chung as well for showing off drag and drop between a laptop and a table. Oh, and everything else has been pretty interesting as well and it’s not even the end of day one yet.
I’m around until next week too, which means I should be able to get along to barcamp – assuming my mind hasn’t melted with all the stuff to take in.
So, it was the Bamboo Juice conference last Friday at the rather impressive Eden project in Cornwall. Along with Jeremy, Dom, Paul and Relly I presented to the crowd of mainly local first time web conference goers.

It was a great event, and felt a lot like the first Highland Fling in that it was the first big event in an area that’s actually quite a distance from the bright lights of London (it took 8 hours to get back to Cambridge). The party afterwards, held in one of the biomes at the Eden project itself, was also pretty impressive.
I’ll hopefully right up some of my notes from the presentation soon, but for the moment I’ve uploaded the slides to Slideshare. The whole thing was video’d as well so look out for those in the future.
Another project I hacked together on the train running on App Engine I’m afraid. Anyone getting bored of my new project each week posts please stop reading now.

Issue or bug tracking is just one of those things we all deal with and probably have opinions on. Lots of open source software exists (BugZilla, Trac) to do the job and various companies have commercial products (Lighthouse, Sifter, Fixx). So why did I go and create another one?
Certainly not because I think I can do a better job for the majority. This is very much a personal pet project and I don’t very much fancy competing with teams of smart people building good products.
Like most software projects my issue tracker was designed to scratch a personal itch. I now have 28 repos over on GitHub (how did that happen?). Some of these are public projects I’d like other peoples input on, and for me that means having a public list of issues. But these aren’t active projects being worked on by a team of people – it’s mainly just me (with Brad and Simon occasionally correcting my spelling and grammar) hacking on the train to work and back. I don’t need collaboration features. I don’t want to be limited to a small number of projects. I’d rather not pay lots of money, or worry too much about hosting.
So that’s where GitBug comes in. Bug tracking for people with minimal needs.
The only real feature past a very minimal bug tracker is the ability to close bugs via a GitHub webhook. Oh and lots of JSON and RSS feeds for everything but that’s just the way we should build things now anyway.
Their are areas of the interface that could do with some streamlining and a couple of open bugs that don’t yet cause me enough pain to fix. I have however opened it up for anyone with a Google account to use and a few people have started adding their own projects and issues already. I’m hoping that for the most part by keeping it as simple as possible I can avoid having to do any work on it and work on bugs in other projects instead, of which I’m sure there are many.
There has been lots of talk recently about URL shortening. Services like TinyURL have been around for a good while, offering shortened versions of URLs like tinyurl.com/dd7w2m which are easier to put in a tweet or an email. The problem with this is that not only does the shorter version mask any information about the destination, but if TinyURL or one of the other shortening services goes away, or loses control of it’s domain name, a large number of links are going to stop working the way they should.
Kellan in particular has been proposing some simple steps that might get us out of this hole. You can read more about the ideas behind using Rev=Canonical and try out the future (maybe) of these services at revcanonical.appspot.com.
The nicest thing from my point of view about this idea is how simple it is to implement. This blog is running a custom Django based blogging engine called Train.
The posts on this site exist at urls like the following: morethanseven.net/2009/04/04/mixing-it-programming-language-choice/. With only a small view function, a change to a template and the addition of a url this blog should now work with Kellans new url shortener.
I decided to use the ids for the articles on the blog as the key for the short versions. So if you were to visit morethanseven.net/284 you would get the article above. I decided to issue a redirect from the short version to the long version in the end rather than serve duplicate content with the canonical link, not sure which way is probably best though.
The markup for each article contains the required link in the head of the document:
And the django view looks something like this:
All in all, incredible simply to implement, especially in something like Rails or Django which make this sort of wire up urls to view stuff easy. So what’s stopping you adding this to you site or current project? If enough people just do it we can make the web a slightly better place in reasonably short order.
So the Register article about Twitter seems to have kicked over yet another Ruby/Rails doesn’t scale debate – mainly it seems from people who haven’t read any of the back story or the real meat of the story. For anyone catching up I’d suggest reading this recent interview with three of the Twitter developers. Ikai Lan made some particular good points about people who don’t RTFM and the comments are well worth reading too. Tony Arcieri, of Reia fame, took another approach and wondered why non of the open source message queues every got a look in
What it really all comes down to is that Twitter are using more than one language to write their systems in. What I don’t understand is why this is a shock to anyone, or why it’s a bad think? Google appear to use Java and Python for most things. Yahoo! use Java and PHP (and C and Perl I think?). Microsoft use VB and C/C++/C# and probably F#. At work we use .NET and Python for different things.
Big companies have been using multiple languages and platforms for good and bad reasons for ever. Sometimes it’s about legacy systems, but often it’s about using the right tool for the job. I think lots of people jumping in on the debate do so from a point of view that everyone uses one language for everything, mainly because most personal projects or small agency style jobs do just that. Why overcomplicate smaller projects with the need for people to know more than one language? In a small general purpose team it’s also going to make recruiting and getting people up to speed much harder.
But for startups doing interesting things it’s potentially both more efficient and more interesting to use multiple platforms. I think Dopplr might be mainly Ruby with a smattering of Erlang, all built around an ApacheMQ message queue, and Matt has talked about that architecture at various conferences without being called out on it.
So for your next personal project why not pick a handy message queue (personally I like RabbitMQ and StompServer), and at least two complimentary languages and see how it changes the architecture of what you build? Mix PHP with an Erlang backend or go for Twitters Ruby/Scala mix. It might very well be overkill for that blog or todo list application you had in mind, but it just might  teach you more about picking the right tools for the job when you come across non-trivial problems.
Looks like I’m being experimented on. I just got a strange Web button appearing on a search today and decided to click it. It revealed a host of new Google features (at least I’ve never seen em before), including various filters and visualisation tools.
The entertainingly named Wonder Wheel was my first click. It’s a visualisation which shows related search terms. I’d love to have access to that data via an API as well.

The timeline view, and time based filters, look more useful for most search activities. Just getting hold of recent content on a particular search term is the sort of thing blog search engines have sended to do well in the past and that Twitter search has been showing off.

I hope I keep these features during whatever testing is going on and that they all go live soon. It’s a pretty big improvement if you ask me.
Just saw this and thought it was cool. You can link to a specific line, or set of lines on GitHub. All you need to do is append something like #L17-24 to specify highlighting lines 17 to 24.
I’ve been playing again with App Engine, and going back to an on/off pet project that I’ve build variations of for a while.

It’s basically a pretty straightforward aggregation platform, taking content from a number of feeds and creating relationships between the items. It’s mainly an experiment in creating a decent size site on App Engine – it can be surprising how many urls you can get out of a good corpus of data:
So starting off with 1500 or so pages isn’t bad. The site also grows over time as new items are posted or I add more feeds. Everything else I’ve done with App Engine has been more application focused so seeing how a content site performs is interesting in and of itself.
The data in question I used for this first experiment was the feeds I could find from the top posters on Hacker News. Hence the name Hacker Posts. That left me with 35 feeds:
I have a couple of other communities or events that I’d like to do the same thing around, as well as a few features I want to add to the underlying software. The nice thing with App Engine is rolling new instances out is as simple as running a command.
The webapp framework wish ships with App Engine uses the Django templating system by default, but without Django apps doesn’t support the same mechanism for loading template tags and filters. This is how to do it though using a few webapp.template methods.
For Ruby people WSGI is the Rack in Python. In fact it was one of the inspirations behind Rack. Rack descriptions itself as:
Which I think is a clearer explanation, except in WSGI’s case we replace Ruby with Python.
As well as being able to write WSGI middleware for Django or Pylons we can also write WSGI middleware for App Engine applications – which is what I spent some time doing today. For the most part I found the examples and documentation interesting but overkill for what I needed to do. Specifically I wanted a piece of middleware which modified the response content, adding extra content into the response. Most of the examples I found didn’t focus on middleware, or where full blown examples making them hard to follow.
So for anyone looking for a simple example of WSGI middleware which adds content into the response here goes. I used the WebOb framework because it provides a nicer interface to the request and response objects and it’s included in the standard App Engine SDK. The following sample middleware simple adds Hello World to the end of every response.
In reality you might want to append something to a specific place in the response, or introduce conditionals. This is easy enough to do by parsing the initial value of resp.body in the example above.
To use the middleware in your application you simple wrap your current WSGIApplication instance with the middleware class.
WSGI middleware is both a useful place for common functionality to live in your App Engine application as well as being a handy tool for anyone working across multiple Python frameworks to share code.
So JauikuEngine, the open source, App Engine based, version of Jaiku is now available for everyone to look at. I found the repo a couple of days ago but it was restricted to project members. The main reason I want to hunt through the code is to have a look at what I’m guessing will be API’s available in a soon to be released version of App Engine – with specific interest in anything to do with XMPP, queues and offline processing.
Well it looks like I’m out of luck for the moment at least. In the settings file I found the following two snippets though:
And another for queues:
The only problem appears to be that the page referenced, code.google.com/p/jaikuengine/wiki/im_support, currently says:
So termie, or anyone else on the inside, I’d love to know how to get this up and running?
I’ll be having a better look through the code when I get a chance. This was just the first thing I jumped on before heading out the door. I love Open Source.
Building larger applications tends to mean splitting your codebase up some how into manageable chunks. I’m quite interested in what I see as different approaches in the Rails and Django communities:
Django tends to recommend building Reusable Apps and we have sites like Django Pluggables to catalog what’s available. You then grab a few of these applications from the web or write your own, add them run them all together as part of a single application. Pinax is probabaly the poster child for this approach. The  0.5.1 release for instance appears to have 41 individual reusable apps, many written by other people and projects.
The Rails community tends to talk more about RESTful service orientated architectures, with things like ActiveResource making this sort of thing easier. So rather than your manageable chunks being within your application they’re separate instances in their own right.
I’d be interested in hearing from more people about their experiences, in particular if you’ve gone against the grain so to speak.
Not sure how I missed this but apparently App Engine (as of 1.1.9) supports remote access to your live data store. This means you can create administration applications more easily by running them locally, rather than within the limitations of the live platform. You can even run a local python prompt with access to your live datastore which is pretty neat.
RewiredState was awesome. 100 or so geeks plus a smattering of government types gathered in the shiny new Guardian offices in Kings Cross on Saturday to hack (the Government).
Some events like this are more productive than others, and the end of day demos included some realy impressive stuff. See for your self on the projects page
My own little project even won a prize (an invisible bottle of Champagne no less). My complain was that if you want to report an issue about the over 7000 government websites you have to do it per site. All of the sites do their own thing, which might be a nice contact form, maybe an email address or in some cases a postal address hidden on a page that’s not linked to from anywhere.
My solution was pretty simple – a centralised issue reporting and navigating tool. So you go along to the site you have an issue with and hit a bookmarklet (or a badge on the site if the site in question have been nice enough to add one). You fill in a very simple form which appears before your eyes and everything is tracked on a nice shiny website.
The advantage of all that is transparency. You can see which sites people have issue with, and also ideally which issues get addresses or at least acknowledged by the support staff for the site in question. The hack had comments so others could follow up on individual issues. It would be simple enough to have league tables and the like as well, or add tagging for a little bit of categorisation – I did only have a few hours though.

After a few nice comments I’m going to have to finish it off and get it up somewhere I think. All I really need to do is clean up the bookmarklet code (which was a hack in more ways than one) and add a bit of sanity checking to data entered into the system.
A massive well done to everyone involved is in order as well, especially James who I remember talking to the idea about ages ago. Congratulations all – and hopefully be back next year.
Stoyan Stefanov just released an excellent little bookmarklet to calculate a content to markup ratio
It’s interesting browsing around a few sites and comparing ratios:
I gave a short presentation about App Engine last night at the Cambridge Python User Group and as always it’s available on Slideshare.

I find App Engine a great way of just writing (Python) code and putting it online quickly, and the presentation was aimed at being something of a fanboy ode. During questions afterwards we talked about some of the limitations and cases where App Engine isn’t as well suited, and at least a few people were heading off planning on having a play.
The event was good fun and I picked up a few Python related tips and tricks from the other presentations and general conversation. Thanks for everyone who organised and attended the event and to RealVNC for hosting the talks.
I’ve finally been getting round to doing a bit of work on dmigrations, the Python based database migration tool we developed at work.
I still have some work to do to merge in a number of modifications we’ve been running internally for a while but I’ve triaged the issues list that had been left alone for too long and merged in a fantastic collection of patches from a few people including Alexander Robbins and Chris Lamb. Thanks hugely to everyone for these.
Everything is available in trunk in SVN at the moment ahead of me cleaning a few thinks up and updating the download version. I’ve added a setup.py file as well ahead of hopefully adding it to the Python Package Index.
I obviously wasn’t the only one who had decided to do some work on the project in the last couple of days. msaelices has just submitted a patch which adds a PostgreSQL backend. I don’t tend to use PostgreSQL personally so if anyone does want to have a look at running dmigrations with the patch I’d love to hear about it.
I always enjoy reading things by Paul Graham, even if I’d like to be able to disagree with his conclusions on occasion. It’s not just that I love Hackers and Painters, but also he often says things like the following that make me smile:
His latest short essay concerns startup hubs again, something that I’m actually pretty interested in, end up talking to people about, but hardly ever writing anything on. Mike over at Techcrunch Uk elicited some good comments last year with a post on the subject. As usual Graham focuses on the US, but what about translating his idea of buying a Startup Hub and applying it to the UK? He sets several criteria for success:
Lets assume for the moment that the idea would work. Take £15-20million and invest in 30 or so well chosen startups on the condition they move into town. The question is then: where in the UK would this be most likely to work? And are their any UK specific criteria that would increase the success rate?
Some people might be thinking “but no UK city is going to invest £20million in a scheme to encourage startups”. Leeds is doing just that. Or rather Yorkshire Forward are putting up £35million along with Leeds University putting up £31milion. What are they doing with all that money? They are going to build a big building with it – which is exactly what Graham says won’t work.
I’ve been busy building and playing with various HTTP clients recently, mainly due to more playing with RESTful web services. I took a couple of hours out to build something to make my life easier – namely a very simple logging HTTP server in Django.
All the application does is accepts HTTP requests and log the results to a file. I’ve been using it to make sure the requests I’m sending from elsewhere are correct, before pointing the client at a web service that actually does something useful. It supports POST, GET, PUT, DELETE, etc. So far so simple.

I ended up using Django mainly because I did most of the work on the train, and I know my way around it pretty well by now. I did originally thing it might have been better to use something simpler but I did end up learning a few new tricks that I’ll use for future projects.
For something that took only a few hours to write it was both good fun and a useful learning tool. And Django proved itself more useful that I though for smaller services like this.
It turns out the guys at GitHub publish the commit messages from their work on GitHub itself. If you really have to keep up you can even subscribe to an Atom feed.
OK, so this might not be hugely useful. But it is funny, and a just a nice sign of a small company being open in an interesting way. A few of my favourites:
I quite like Epydoc for generating Python API documentation, even if the interface looks a little dated and could do with a lick of paint.
For most project I use an Ant build script to generate documentation when needed. You could wrap the basic commands in a make file or a bash script if you prefer that sort of thing though. The only trick is to make sure you have everything you need on your Python Path. In Django projects, or App Engine projects, you’ll probably find the default runner script plays with the path somewhat. The following example adds the current directory to the path along with an ext directory where I stash external modules, which should see it run cleanly without any import errors.
As I mentioned before, App Engine is getting an XMPP API at some point soon. But if you just can’t wait to start adding IM interfaces to your applications then you can do it now, by using a nifty third party in IMified.
IMified provide an incredibly simple HTTP API for interacting with your own IM bot. If we want to be buzz word compliant we might even call it a webhook. It’s also currently a free service while they work through the beta. The documentation is short and to the point but only contains examples in PHP. It supports multiple step conversations as well as authentication.
So, armed with a little time on the train over the last few days I got to work knocking together a quick demo application as a proof of concept. You can find the site on imified-demo.appspot.com and if you want to chat with the bot you can add appengineimified@bot.im to your contacts. The bot uses the Jabber protocol so is available over Jabber or GTalk. IMified make it easy to use MSN or Yahoo IM accounts as well, which is something the App Engine API might very well not do I would imagine.

As always you can find the code on GitHub. Most of the code is actually just the site itself or settings to make local development easier. The following is a slightly edited version of the live code (logging and caching removed to make it easier to follow). All we need to do is accept a HTTP Post request with a list of arguments and return a plain text response. All being well the response is sent as a IM message to the sender.
IMified can obviously be used outside App Engine as well, and in fact it’s not just about working around limitations in existing systems. Running the long running processes required for bots, and potentially even running your own XMPP server, is fiddly at times and requires at least some setup, monitoring and configuration to get working. Not having that as a barrier for entry for simple experiments or applications is a good thing.
Aral spoke at the last DJUGL about App Engine and mentioned a wide range of third party services that you can use to get around current limitations. IMified definitely fits into this group of support services very nicely indeed. I’d love to see them do really well as it really makes it much easier to get started with XMPP applications, even if what you can do is limited to a few simple APIs. I’d love to hear about other services that people are using in this way to build these distributed applications.
I’m working on a small project involving using RESTful APIs and wanted a simple HTTP client, something that sat a little higher in the stack than httplib2 or similar. I turned initially to the Django Test Client which now supports all the required methods but it turned out that I’d have to unpick it from django a little.
With a little bit of looking around I found the python-rest-client which certainly sounded like it would do what I wanted. It lets you make HTTP requests in as straight forward a manner as possible and fitted the bill perfectly.
It supports authentication, nice helper methods and gives you the response in a nice format.
As an added benefit it also comes with a Google App Engine compatible version.
As I think a few people are aware I’ll be leaving Global at the end of May.
It’s going to have been a pretty good year all told. I got to work with some great people and we built some pretty good stuff from the group up. The Capital Radio website was part of it but it’s the systems behind the scenes where all the interestingness lies. But as the team as a whole was just getting started the economy got bored of growing each year and went south. Global is first and foremost a radio company that makes money from advertising and, in need of cash, it decided to cut lots of the interactive department. All of that meant lots of redundancies and less interesting work and I decided I’d rather go now than wait around.
It’s an odd situation to be in but I’m not going to dwell on that and I’m not bitter about any of the goings on. I got to play with Django full time in what, I think, was the biggest Django development team in the UK. I got to work, and argue, with smart people every day. We had two internal hackdays where I got to play with XMPP and messaging apps. And I’m definitely a far better programmer than a year ago.
I’ll be sticking around at Global for the next three months to finish off a number of projects that I’ve been involved in and tidy up a few loose ends, as well as to work out what to do next. Which is where you (might) come in. Think of it like a job vacancy in reverse.
So if anyone knows of something that would be up my street I’d be grateful. Even better, if you or your company are looking for someone at the moment and don’t mind waiting a few months then I might be what you’re looking for. For those that don’t already have an email address you can find me on gareth@morethanseven.net
(There is obviously a chance that you might be a recruiter, rather than someone who wants me to work with or for them. I won’t ignore you out of hand but please don’t try and ring me unless I ask you to. Email is both quicker and easier for me to deal with. Also if you just want my CV for your files, have only a vague job description, or you want to tell me about a job in a company you can’t tell me the name of please don’t be offended if I don’t get back to you.)
The previous Django settings tip seemed to go down well so I thought I’d jot down a few more over the next few weeks. Most of these have come out of working with a decent sized Django team at Global so I can’t take credit for anything but writing them down for the most part. For this example I think Alex Knowles did the original version.
I was talking with out friendly sys admins on Friday about a new application and whether they were happy with some application specific logging (using the Python logging module) I’d build in. Nothing fancy, just application logging to a rotated log file for system events. Their answer was yes, as long as they could control where the log files ended up and the maximum file size, ideally without having to play around in the code or to redeploy the application if they wanted to move the files elsewhere.
These things were already specified in the settings file rather than hardcode into the application but that only gets us half way. The standard Linux way of setting this sort of thing is with a configuration file stored in the /etc directory. So we ended up with the following snippet of code in our settings.py file.
Then in the yaml file you can simply clobber any of the settings using a simple name value pair approach.
It lets us keep production paths that might change out of the developers code, at the same time as giving the sys admins a familiar way of managing the production environment.
It does have one downside, if you’re not aware of it’s presence then it can make debugging settings related issues a pain. With that in mind you could wrap it so as to only work this way when DEBUG is False, or take the approach here which is to leave extensive comments.
Django settings files are pretty interesting. Rather than being written in some sort of purely declarative markup they just use Python. This brings both lots of power as well as the ability to do things in the settings file that you probably shouldn’t do.
One area where I find this capability particularly useful is when specifying file system paths. Lots of the settings concern where Django can find templates, images, or stylesheets for instance. The examples given in the default settings file are all of the form /this/directory/structure/. If you plan on only working on your own, and never running your applications anywhere except your local machine this is probably fine. The moment you want to deploy your application, or want to collaborate with others this becomes a problem. You either have to agree upon a fixed directory structure between all developers (annoying) or have distinct settings files for each machine, which probably means them being outside source control (also annoying).
A better approach is to have those paths dynamically ascertained at runtime. It makes the application much more portable, making local development and production use easier. Using the standard library os module we can do just that.
Here we set a couple of useful constants, one is the path to the site folder and the other the path to where django is stored on this machine. settings.py contains a number of places where these constants are useful. For instance the MEDIA_ROOT settings which specifies the file system location for assets like images or stylesheets. The default settings file even comes with an instruction and example showing a non portable path.
Other examples include setting the path for a SQLite database:
Or specifying directories in which we can place templates.
I actually think this should probably make it’s way into the default settings file. I might very well be missing something but I can’t see when it’s not much better to do things this way.
Quick Django pop quiz. Can anyone spot the deliberate mistake in the following url definition? We’re trying to define a view called log_viewer and instructing a specific url pattern to render it.
In this case our regex matches /log or /log/ using the /? optional pattern. This is because even if we only link to one format we know people will probably visit both, either by entering the URL manually or by linking from an external source.
As far as HTTP is concerned though /log and /log/ are separate URLs, even if they display the same content. The main reason this matters for public facing websites is that our friendly search engine spiders are likely to index both separately, leading to splitting the page rank as well as accusations of duplicate content which might see further erosion of rankings.
The solution is generally to issue a 301 redirect from one format to the other. This tells search engines and people alike that the canonical location for the requested content is elsewhere. You could specify the redirect manually, but this is going to get irritating quickly once you have a few more definitions.
Handily Django provides a mechanism to do exactly what we want to do by setting APPEND_SLASH to True in your settings file. Even better it’s switched on by default. So if you don’t know much about the intricacies of HTTP you still get the correct behavior. That is unless you specify your URL patterns in the format above.
You see APPEND_SLASH only works if the URL doesn’t match a specified pattern. If no pattern match is found it appends a trailing slash and checks for a match again. Because the above pattern matches the pattern without the trailing slash (/log) the desired behavior is never triggered, and the view is rendered at both URLs. So although we want to catch /log and /log/ on the front end, our urls.py definition should actually be:
Django has lots of useful bits of magic for doing the right thing, but unless you know what they actually do you either end up recreating functionality yourself, or find features don’t work in quite the way you thought. It’s a good argument for keeping frameworks small whenever possible, and for developers to at least know their way around the code of their respective framework.
Three weeks ago I pondered whether XMPP and offline processing were coming to Google App Engine?. It was a hunch based on the upcoming release of Jaiku on App Engine. I reasoned you couldn’t really do it without XMPP and offline processing APIs. Looks like I was right.
Today Joe Gregorio announced on the App Engine Blog an update to the roadmap for the next 6 months; including
Colour me excited. This could be the point were we start seeing more and more interesting IM interfaces. And this ticks off several of my must haves for App Engine.
It turns out App Engine breaks the default behaviour of the Python debugger PDB by sending STDOUT to the browser. But with a little bit of python you can put it back in.
Via tagging a new release on GitHub I see a new version of Radiant (0.7) has been released. Radiant is a really nice CMS for smaller projects, it’s used on the official Ruby site and I used it here at one point.
Go check out the blog post for the full list of the changes.
I like adding images to the occasional blog post but don’t do it as often as I want to. The reason being I can hardly ever be bothered to resize images. It means opening up a memory hogging application, fiddling around for a few minutes and then saving it out somewhere, then uploading said image to my server. All of those things bore me.
Appengine to the rescue. As an excuse to play around with the image API as well as add more pictures to this here blog I decided to build myself a small image hosting application. I really only care about the two things noted above; resizing and uploading. If I can do that in one go I’m happy.
A couple of train rides (all the best code gets written on train journeys, fact) later and I’ve deployed the first version of Image Host which looks a little something like this:

(And yes that image is an image of image-host hosted on image-host. If you’re thinking what image? then it means this experiment isn’t working at the moment.)
The application is designed in such a way as you could have as many people uploading and managing their own set of images as you want, all within one instance. The only reason I don’t just open it up to any google user account is that it would eat into my quotas if it got popular and I have no interest in policing whatever dubious images anyone might upload.
I’ve been playing with App Engine quite extensively of late. I really appreciate the SDK and the limited, but well designed and thought out APIs and testing stubs. At the moment I’m happy with where it’s at. It’s obviously early days but in my mind all I really want are APIs for offline processing, some sort of message queuing facility, XMPP support and a payment model for raising the limitations. And several of those are already slated for the near future.
If anyone wants to host their own version you can grab the code from GtHub. You’ll need your own App Engine account and your own snappy name but that’s all. If you really don’t want to do that, have a good reason to needing image hosting and ask nicely I might add more people to my version as well.
Their are a few bugs I’m going to fix when I get a moment,  a few documented limitations I know about and I’m still fleshing out the test suite to cover all the functionality. But all in all a worthwhile few hours spent hacking. I’ll probably get round to writing up some pointers for testing App Engine code as well as their are a host of gotchas and the only real documentation at present is comments in the code.
The Python core developers are currently discussing whether to move away from SVN to a distributed version control system. It’s a worthwhile read for anyone involved in this sort of decision in any capacity. It features hands on examples of each of the contenders (Bazaar, Git and Mercurial), some interesting observations about all of them as well as  some benchmarks against a mature codebase. Lots of conversations keep cropping up on Twitter about why bother switching to Git or other distributed systems – this is a good place to start whether you use Python or not.
I’ve been playing around with the Sinarta Ruby web development framework recently and building a larger than usual Hello World Example. It’s describes itself as a DSL for quickly creating web-applications in Ruby with minimal effort (what is it about Ruby people and their obsession with calling everything a DSL?). In reality it’s a great little web framework. It deals with a minimal set of the things you really need to do as part of any application – URL handling and routing, HTTP request and response handling, etc. It reminds me of web.py in it’s minimalist approach which is definitely a good thing.
The following example is the hello world given on the site
Which isn’t a million miles away from a web.py example:
The only real difference is the separate mapping of URLs to views in web.py, which is closer to how Rails or Django do things.
Their is quite a bit of documentation already for Sinatra, including the start of a book. The code (as with all good code these days) is on github for your forking pleasure.
As for what I’ve been up to I have a more advanced Hello World example up on GitHub. I’m wanting to get a running application that demonstrates all the basic features (except HAML and SASS support). So far I’ve got a simple bit of Rack middleware, several views demonstrating different url handling techniques, basic erb templates, before methods, configuration settings, error handling, decent unit test coverage, a rake file with a tasks for documentation, code coverage, etc. I’ve also got configuration files for running the application with Thin and using God. I’m going to add some simple database connectivity in at some point (either DataMapper or Sequel, I haven’t decided yet) and play around with writing spec tests and a capistrano recipe file. All in it’s a nice way of learning something new at the same time as producing something that might be useful. Once I have the rest of the bits and pieces I might even right a full tutorial.
I think the sweet spot for these sorts of mini frameworks are small services or little applications that just sit their are run. Both Integrity and IRCLogger both use Sinatra for instance and I think it’s used for the GitHub WebHooks as well. It’s exactly the sort of thing Google AppEngine is useful for in fact and Sinatra would likely be a closer fit that Rails if Google ever feel like adding Ruby support. Although it does depress me a little that the top four items in the public issue tracker are I want my own language.
Jsonpickle is a Python library for serializing any arbitrary object graph into JSON. The advantage over something like simplejson is the arbitrary part, simplejson throws errors when you try and serialize some types of objects. I also prefer the jsonpickle API (encode, decode) over simplejson (dump, dumps, load, loads).
TicGit looks great. I love command line apps and have been looking for something like this for a while. It’s described as a:
Perfect for my pet projects or working with like minded folk.
I’m a big fan of the Ant build tool. Their I said it. Nearly everyone else I end up talking to about build scripts (more people that you’d think, but OK, it’s hardly the most exciting topic of conversation) either hates it or treats it with disdain.
I’ve been using it for a few years on and off, in several jobs and for personal projects as well. I’ve used it while writing Python, .NET and PHP. It might be somewhat unfashionable (it’s written in Java and you write your commands in XML) but, for me at least, it’s incredibly handy to have around.
Ant is a build tool. It lets you define tasks in a config file (called build.xml) and then execute them via running the ant command line application. It supports dependencies between tasks as well as defining properties that can be used by multiple tasks. It supports a lot of Java specific stuff as well but also has the ability to simply execute commands on the host OS.
As a really simple example of a few tasks I use on more than one project involve simple backups and deployment.
First I set up a few properties including details of where my site files live and the SSH access details for the remote site.
The first example task simply runs a backup of everything in the target directory using scp.
If I make local changes and want to push them to the live site I have another simple task which shells out to rsync.
I know some people hate this separate arguments as individual elements. Yes it’s excessively XML but it makes everything incredibly clear to anyone who might sneak a look. And build scripts change little compared to project code so the verbosity never bothers me overly. If you really want you can put everything on one line, but I find that harder to follow and maintain.
For bigger projects I tend to create more complex backup and deployment tasks, or more often than not add in various dependencies. But you hopefully get the idea. Even for simple commands like this that would be a single line bash script I tend to use ant. I find by putting things together into a build script I’m more likely to add useful functionality to it later, and to remember and therefore run the commands more often.
A good reference for finding out more than is in the manual is the Apache Ant Wiki. More than anything it features real examples that you can learn from which with Ant is definitely the best way to discover new tricks.
I know their are a number of other tools in languages I like more. On occasion I use Rake, Fabric and Capistrano. I’ve looked at Vellum and good old make. I know others who swear by just writing simple bash scripts or using straight Ruby, PHP or Python (or not writing build scripts at all and doing everything by hand. But I like having my build scripts separate and simple. It might not be pretty or fashionable, but Ant does almost perfectly what I want it to do.
The second Django User Group meeting was last night and was a great success as far as I could tell. Our boardroom at work was chock full of budding Django developers and interested parties – 70 people or so in all. Good work by Rob to get everything back on track at the start of the year and the next event is already planned for the back end of February so I hear. More news soon on the mailing list hopefully.
Rather unfortunately Simon had to pull out at the last minute and I stepped in to do a quick 20 minutes or so talk on writing tests for your Django applications.
I was rather surprised that only about a third of the people coming along were using Django in some work related capacity, and when I asked who was writing tests for their code a few more hands went down. Very few of the people not using Django at work are writing tests which is unfortunate (and contrasts rather interestingly with my experiences of the Ruby community).
For a presentation done in less than a day while at work I felt it went OK. testing is a hard sell at the best of times and trying to talk to both those without experience of writing tests and at the same time trying to get in a few of the more interesting things we’ve done at work was maybe a bit of an ask.
It not being a blog post round here these days unless I include a link to GitHub I did mention a small project last night in the talk – Django Test Extensions. It’s more just a collection of useful common testing code: additional assertions, custom test runners, etc. Thanks to Ross for the last minute patch with a couple of bug fixes as well.
The other two presentations went great. Andrew talking about the South migrations system and Aral talking about real world Google App Engine were both up my street of things I’m interested in. We have video of all the talks as well so hopefully that will make it out into the wild at some point once it’s been edited.
I’ve been knocking together various little instant messaging bots recently, partly as a way to play around with XMPP. As well as using the low level xmppy and XMPP4R-Simple libraries I’ve been having lots of fun with the JabberBot framework.
Jabberbot lets you write simple bots incredibly quickly, using simple conventions to determine what commands the bot exposes. It’s easier to explain with a simple example. The following bot lets you send the command time to it and returns the current time on the server on which the bot is running. The magic is in the name of the method bot_time_. Any methods that start bot are automatically exposed as commands for the bot to accept.
You’ll need an xmpp server for the bot to connect to but you could always just register an additional google account and use it over gtalk if you wanted to. Personally I’m running ejabberd on a local Ubuntu VM as well for testing.
The JabberBot site has a few more examples as well with fancier features. At the recent Last.fm hackday I spent a bit of time knocking together a bot which talks to the Last.fm API (using the PyLast library for the API backwards and forwards.) I did this mainly as a demonstration of how simple it can be to create a useful command line interface to your API using an instant messaging client.
The code for LastBot is on GitHub. It has a few limitations and doesn’t intent to cover anywhere near all the API. When up and running you should be able to talk to it with your IM client. Simply send the user specified in the settings file a message like so:
So if you wanted to search for “astley” you would type:
Which would probably give you:
If the first result that comes back isn’t the one you wanted you can ask for the next result by simply sending next in another message. You can use prev as well to come back through the set.
You can always send it a call for help at any time which should return the instructions to you via an IM message.
Which should return something like:
More instructions, and the code behind the bot, can be found in the README.
As developers we spend a lot of time using command line interfaces – mainly for speed and because you can cram a lot of functionality into a small amount of screen real estate. We’re increasingly spending time debugging API calls as well and exposing your API calls for use by instant messaging clients has the potential to make development easier. Think of the Python interactive shell or of IRB, but for APIs.
Google just announced that, once the port of Jaiku to App Engine is complete they will be Open Sourcing the code and stopping official development. I only used Jaiku sporadically in the heady days when everyone had to sign up for a new web 2.0 service every week or be mocked by their colleagues. What really interests me about this move though is what it means for App Engine.
If memory serves Jaiku had an instant message interface. Does that mean App Engine is going to get an XMPP interface?
I’m also presuming the original Jaiku application had at least some features powered by offline processing or used a message based architecture behind the scenes? Or used scheduled batch jobs for reporting or data mungling?
All of these issues are high up on the App Engine issues list and solutions to these two outstanding problems would move App Engine, in my opinion, to being very cool to being very useful. I’ve been impressed with the simple, well designed and well documented current set of APIs and I’d love to see a few more added for XMPP and offline processing. If the Jaiku code is open source but relies on Google only APIs it would seem a little odd.
So does anyone have any more information on this? Any passing Googlers who can get us an answer?
One of several little projects I have up and running on GitHub at present is LocalBuilder. It’s a pretty simple little script which watches for changes in a given directory and when they occur runs a given command. I knocked it together to use to trigger the running of a test suite each time I save files in a project. It’s written in Python but you could use it to run commands in any language you like.
It’s all pretty simple Python really and is hopefully reasonably commented and tested so if you’re interested you can follow along with the code. If you just want to use the damn thing then:
It turns out Ruby has a much nicer tool to do this and more in autotest, part of the ZenTest suite of testing tools. I’m finding more and more very nice bits of code written in Ruby of late, but that’s probably a whole different blog post.
It’s the start of another year so that means it’s time to start thinking about conferences.
I just found out at the end of last week (via Twitter no less) that I’ve been confirmed as one of the speakers for Bamboo Juice. Thanks to Jon and Rich for having me. I’m really looking forward to that as it’s a new event outside London (at the rather fabulous looking Eden project in Cornwall), which should hopefully mean a chance for a new set of people to go along to what looks like a decent event. I’ll have more information about what I’m going to be talking about some time between then and now I would imagine, but the emphasis is on practical learnings for working developers and designers.
Apart from that the only other conference I’ve confirmed I’l be attending so far is Thinking Digital in May. It was great last year even if I spent the majority of the time running round organising barcamp, moving house or leaving Newcastle. This time I’m not involved in any organising (sorry Andy)  so I plan on making the most of the talks and general experience. Rumour has it that their will be another barcamp as well so I’ll look forward to that too. It will be good to get back up to Newcastle and see people again.
I’m also looking forward to what Patrick is up to for @media. It was the conference that started this whole thing rolling in the UK back in the day and now it appears it might be set for a change.
As well as these three I’ll be keeping my eye out for any news about Xtech, The Highland Fling and any other new upcoming conferences. If you know of anything else leave a comment. I didn’t find out about Think Visibility until it had sold out but I like the idea of smaller, specialised events.
Overall it already looks like being a good year for conference goers in the UK. I’ll be on the lookout for barcamp style events as well but I do like a good formal conference as well. Hopefully see a few people at some of these events?
Everyone has to have a post with a year in the title at the start of the year so I thought I better write something. Rather than one of those personal retrospective emails I thought I’d go for something different – a look at what I’d like to see in APIs in the coming year.
I’m pretty interested in the idea of applications exposing Webhooks at the moment. It’s a pretty simple idea. As a user of a service you can register your own HTTP end points to receive information whenever events occur in the service.  Both Shopify and GiHub have pretty nifty hooks for extending their capabilities for instance. When someone pushes code to a git repository you could send a ping to trigger a process to update the documentation for example.
With the rise of hosted application development environments like AppEngine writing small, nearly throwaway, apps to subscribe to these hooks could become incredibly powerful. It’s a lot like how unix programmers think, by piping lots of small applications together to get to the expected end result. It’s not a replacement for the more standard read/write API, but it’s a potential solution to the need to constantly poll that API for some types of applications.
The idea of public webhooks would be hugely powerful, but would also likely be a scalability nightmare. Imagine if Flickr exposed a hook that you could subscribe to whenever anyone added a public photo. Or Twitter added a hook for when anyone tweeted. These sorts of hooks would quickly be swamped with subscribers. The number of HTTP requests being sent by a service like Flickr under these circumstances would be rather large to say the least. Which is where another technology that’s designed for this sort of problem becomes useful.
XMPP or the Extensible Messaging and Presence Protocol has been around for a while, although originally under the name Jabber. It’s predominantly being used at the moment as a instant messaging protocol, but in reality it’s far more general purpose than that. Or rather, IM is generally considered to be between two or more people. But their is no reason that either or all the participants in an XMPP session can’t be programs.
On my local machine I’m a big fan of the command line for all sorts of simple, and sometimes complex, tasks.  If applications expose their APIs via an XMPP bot then you basically have a ready made command line interface to online services via your IM client of choice. Combined with a solution to the public webhook problem mentioned above and you can hopefully see why I find XMPP pretty interesting.
I’ve been playing with writing XMPP bots recently, both at our internal hackday and at the recent Last.fm event. Their are various libraries and code examples lying around the internet that make getting started easy enough. With services like Imified getting setup last year I’d imagine it will get easier still this year.
As for what I’m going to be up to along these lines, we’ll have to wait and see. I have a few pet projects that I’d like to get off the ground which might be good test beds for this sort of thing. Apart from that I’d like to write some getting started style tutorials on some of the technologies involved or maybe a full blown article on the theory if I can find somewhere to publish it. A barcamp presentation or two might also fall off the back if I do get the time to play around a little more. But that’s just me. If you’re building, or planning on developing, an API for a product at the moment I’d suggest having a look at these two areas. They might turn out to just be potential extensions to what you had planned. Or they might turn out to be just the right approach to the problems you’ll face getting people to use your API.
Integrity is ace. I’m a huge fan of working under the ever watchful eye of a Continuous Integration server. I’m also becoming more and more of a fan of Git, and GitHub, for my personal projects. At work we run CruiseControl and it does it’s job well, but locally I only use it for larger projects. It comes with a little overhead and if I’m just hacking on the train I rarely check on it’s status.
Integrity is an unashamedly lightweight and straightforward continuous integration server written in Ruby. It comes with Git integration as well as a nifty notifications framework. There are already notification plugins available for jabber, IRC and email. If I get time and inclination I’d love to hack together a webhooks plugin too. It’s a simple app to get up and running with and you can always browse the code if something isn’t clear. All in all it’s perfect for my type of smaller project.
So, with a local CI server up and running you’re left with one problem; having to click the _Manual Build" button whenever you want a build. Now Integrity comes with a mechanism to allow pushes to GitHub to trigger a build. But this only works when you have an internet connection and are using GitHub and are pushing frequently. Personally I often make lots of local commits and then push at a later date. Also not all of my projects are on GitHub for various reasons.
Well it turns out that all the build button does is sent an empty HTTPpost request to a URL of the following format:
That means with a little bit of Git magic we can have our integration server run a new build whenever we commit our code. All we need to do is write a very simple post-commit git hook script. I’ve written the script in Python but you could write it in anything. This script is from a real project so adjust the server address and path as needed.
All you need to do is drop this script in your .git/hooks folder as post-commit. Make sure you set the executable bit with chmod +x as well, otherwise you’ll be wondering why it’s not working and probably blaming me.
I’m pretty interested in computer games. Building them represented a big technical challenge and with that comes interesting parallels with larger web projects. Andy Budd has talked about User Experience learnings from games previously, and I’ve heard Aleks Krotoski talk about similar themes, in particular the design of social systems and user generated content. What I’m interested in however is tools programmers.
Computer game development teams generally have a decent number of people solely dedicated to building and maintaining tools. They aren’t working on a specific title, or just on maintaining existing systems, but on programming tools for other developers in the company. Now I’m not sure how much sharing of these tools goes on between the different computer games developers, but from a cursory look around I couldn’t find any examples.
Tools programming in web development teams seems to be a different kettle of fish. I’ve never seen a job position advertised specifically for tools work, nor do I know anyone who would describe themselves as such.  I am however seeing more and more of these sorts of tools make their way out into the world recently thanks to GitHub. Build systems and scripts, documentation systems, testing harnesses, linting tools, etc. All can be found if you look closely. So it’s definitely not that we don’t build these tools, though it might be that we don’t do it as part of the 9 to 5.
One of the differences that might cause this difference between games developers and web developers is scale. For the most part games development teams are bigger than your average web team or agency development team. But not always and as I said I’ve never, not once seen a web tools job going. The other reason might be Open Source. Speaking from the web side of the fence their are lots of great open source tools for building web sites and applications. If your needs are pretty basic you should be able to get away with stringing some of these together, but you’ll probably still need to do the scripting to do so. Maybe this is the same for games development; I’m afraid I don’t know.
Maybe the other reason is that as web developers we all also do a little bit of tools programming? From personal experience this is certainly true, but then I like and think about this sort of stuff more than most anyway.
So a few questions for anyone also interested in this sort of thing:
I also have a feeling some communities are happier knocking out little tools than others. The Ruby community in particular seems busy at the moment and their is a great deal of good stuff in Java.
One last thought. How do you go about finding new tools that might work nicely in a web development world? I read far too much of the internet on a daily basis and I still miss interesting stuff more often than not. I only came across Integrity yesterday for instance.
Spotify is great. It seems stable, the desktop interface is simple and straight forward, and it has an entire album of emo bands doing covers of other emo bands. One thing that’s not quite clear just yet in the interface however is how to search for all the Rock tracks, or more specifically how to search for all the tracks in a given genre.
Turns out it’s quite simple. Just do a search for genre:Rock. My guess is there are various other textual shortcuts hidden in that search box. If anyone knows of any others post a comment.
I’ve been looking at different ways of using simple sessions on App Engine, in particular for one shot flash messaging after redirects and the like. Their are some issues with Cookie support at the moment but  Gmemsess solved my problem perfectly.
I’ve been thinking recently about what happens when we all get older. Now, I’m not actually referring to everyone here but more specifically what I’m going to call second generation web developers. I don’t mean Tim and friends here, or the early entrepreneurs of Yahoo! and Netscape. I mean the people who came along when the commercial web design and development industry had settled down a little bit, lets say 10 years ago. People like me.
Work seems to exist in lots of places; big in-house teams, small in-house teams, agencies, startups and freelancers. Where the jobs are at any given point seems to be tied to economic conditions and location. Most people I seem to speak to have tended to work in one of these areas, but I’ve not really got anything but vague memories to back that accusation up. I’m a little odd here in having previously worked mainly for agencies followed by a stint of freelancing, and now work for a decent sized in-house team at Global Radio.
As I see it the industry rules are predominantly made up by people involved as they go along. Their is no real impetus behind any trade body that I’m aware of, no real understanding within education and no consensus on organisational structure and jobs within it. That means, unlike accountants or people in many other professions, we’ve very little idea about what will happen in the long term.
So. What happens in 10-20 years time to the now quite large number of professional web developers.
Do we all just do the same thing we’re doing now. Just with higher version numbers? The problem with this is maintaining the challenge. If the core problems remain the same will it be much fun? I fear their are only so many times you can learn a new programming language and then solve the same problem you worked on at your last job before the world catches on.
Do we all become managers? The problem then is who do we manage? If the industry just gets bigger and bigger this works. But that sounds unreasonable. So if their are a limited number of managerial positions who gets them? The people their first?
Is their another industry that will have us? Computer games are starting to look to the web for ideas about community and collaboration, pretty much at the same time as the web is looking at games for thoughts on experience and engagement. But long term I can’t see a mass exodus or a huge cross over of people, just a huge overlap of ideas.
Do some of the areas like agencies, in-house teams, etc. disappear, or at least employ less skilled people? Experience costs money, and not always because it’s worth the extra expense.
Or is it simply that the world in twnety years time will be so different to now that we don’t really have a clue. And that a large group of computer savvy problem solvers will find something to do?
The real question I guess is what does the demand for skilled professional web developers look like in ten to twenty years? In reality most of the people currently doing the job won’t be getting to retirement age any time soon. That means every newly trained graduate or kid with a computer getting their first job adds to the size of the workforce. And how big do we think that workforce can get before it reaches a nice equilibrium? Certainly in London at the moment their are jobs aplenty. Many good friends have just left Yahoo! rather promptly and I’d be worried for them if I didn’t know how many people will be knocking down their doors.
I’d love it if their were numbers somewhere on this that you could graph. Maybe spot a plateau coming. It’s the sort of thing the Institute of Physics or the Chartered Institute of Marketing do. But as I said, we don’t have anything similar.
So maybe their is another option when the workforce has expanded as much as it’s going to do. We could get all work for The Institute of Web Development.
Stuart at work has been playing with a nice Python web crawler recently. I’ve used Harvestman before but it’s not the most straightforward thing to work with. Spyder has a really nifty callback based approach and a couple of hooks which allows you to write code like:
On a side note I wish Launchpad was as clean and tidy as GitHub though. I can see GitHub adding some of the features that Launchpad has eventually, but I hope they fit them in around the existing features.
It’s probably old hat to those who have been using Google Appengine for a while but I just found some sample apps on Google Code. I prefer learning from actual code like this, at least until the App Engine books make it out next year.
I upload the work I’ve done so far on testing CSS. It’s still work in progress obviously but if you’re interested do let me know. I’ve stared with the image approach but will hopefully have something up demonstrating the selenium/rendered DOM position approach as well.
I’d been meaning to write a quick article about GitHub, but then Mike and Neil beat me to it and stole most of the best bits. Read both of those articles then come back if you want. I agree whole heartedly.
Now I’ve used public hosted source control before. But Google Code, Sourceforge or Launchpad never felt like this. Their were always their for people to download your code if they wanted. Maybe you collaborated with a few people on a specific project. But GitHub, through a combination of neat visualisations and social features, is encouraging people to just upload all their code. All those lines of throw away scripts. All that code written on the train learning a new language. All that configuration stuff that normally gets missed.
Talk of emerging programming languages often shifts to the need for a killer application. I want to learn more about Git so I can do more with GitHub. For me, that makes GitHub a likely driving force behind a sea of Git adoption in the coming year or so. I’m also interested to see what comes out of a reasonable sized network of like-minded people absentmindedly hacking away. Already I’m seeing people branch other peoples code within my network of contacts. Often just small tweaks and changes – like a code fairy cleaning up a few rough edges or adding an extra line here.
So, if you write code head over to GitHub and signup. Create a few repositories of the things you’re hacking on at the moment and jump through the pre-portable-social-network dance just one more time. I promise it’s worth it this time.
CSSDOC looks like a good idea. I’m sure a few people mentioned this last year at @media after the talk by the people from The Guardian but nothing came from it. Hopefully tools will start to come about soon.
I’ve been doing some performance profiling of this here blog. Not because I really need to due to huge amounts of traffic unfortunately, more because I’m planning on releasing the code and wanted to give it a good once over before I do.
The above profile is from the home page which seemed a good starting point. The majority of the other pages are more straightforward, generally displaying just one article with a few related bits and pieces.
What jumped out was that the majority of the slow traces (the list is ordered with the slowest calls first) were to do with the use of the textile filter packaged with Django. Textile is a simple markup language which converts to HTML, I’ve used it for all my writing activities since I used Textpattern on this blog many moons ago. It’s not just the traces that explicitly mentioned textile either – the regular expression calls are also to do with template filters.
I quickly added a hidden html field to my model and a custom save method. On saving an article I now automatically transform the textile version of the content and save it in the html field. In the template where I used to have article.content|textile I now have article.html. A quick check of the profile on the same page showed a dramatic increase in performance:
From nearly half a second (0.479 CPU seconds) to 0.081 CPU seconds – roughly a 500% improvement in performance!
If you are using text transforming filters in Django that make use of regular expressions I would look to try and get rid of them and move the conversion into a save method. You might not see quite as much of a performance gain as I did here but I’d be surprised if it’s not taking an awful lot longer than you might thing.
Their are a few tools that are handy to have around if you’re wanting to profile your own applications. Django Snippets has a handy profiling middleware and the really rather nice Command Extensions provides a few more tools including profile graph generation.
I was always a fan of Rail environments and as part of some work upgrading this site to the latest version of Django I decided to clean up my whole deployment process. Part of that involved replacing everything in settings.py with the following snippet of code:
I now have two settings files stored in an environments module containing all the usual django settings; one for my development environment and one suitable for live. The settings.py above is for my local development environment, with only one small change for live (this file doesn’t get deployed along with the source code for the site, so doesn’t get overwritten).
This isn’t quite the same as the Rails implementation obviously. I run completely different server setups so I’m not too bothered about a flag on the runserver command like the -e flag for mongel. I could also probably do something to autodetect the environment but this works fine for me.
Talking today on Twitter with Tomasz got me thinking again about one of those problems that I come back to once in a while. Unit testing CSS. CSS development is a pain, even with some sort of system. Admit it. I actually like CSS most of the time but it’s still painful at times. Hopefully with that out of the way you feel better.
All testing past simple validation of CSS seems to be done visually at present.
Thinking about this from the point of view of CSS seems straight forward, but turns out not to be so for a variety of reasons. The problem lies in the cascading and compounding nature of the beast. Each individual CSS rule might do something which is self contained, but the chances on a real site are probably slim. For instance:
What is the size of the font size of a paragraph? It turns out it depends. Not just on more than one unit of source code (we have two rules here) but also on things like the browser. And how do we get this font size from a browser in the first place? I generally dislike Selenium but does it provide a mechanism for getting at the calculated DOM attribute values? Do we have to interface directly with a browser at a lower level?
wxMozilla, wxWebKit or maybe pywebkitgtz might prove useful, but I’m not sure at what level they operate. What I’m imagining here is maybe something like (excuse the Python, hopefully you get the idea):
So we could use CSS selectors (ie. p) to find elements and then assert various DOM properties (ie. fontSize) are equal to values we specify. The magic is in getting access to those calculated DOM attribute values from an actual browser engine.
Another approach would seem to be looking at visual rendering and comparing against a known good version. This seems to be something that the Mozilla folks got up to a while back to test different browser versions. Their are a few tools that might help us out here too; BrowserCam provides a paid for service, Webkit2png is a handy command line script I’ve had fun with in the past and IECapt appears to be a similar beast for Internet Explorer. CutyCapt is another cross platform webkit based utility. I can see a few gotchas lurking here. Animations or slow loading javascript would obviously throw this into disarray. But disabling these in the browser might get up somewhere. How to compare images  produced I’m not yet sure, but I reasons someone reading this might have a good idea?
As the title would suggest this post does not contain the answer, only a few useful links and two possible approaches to the problem. The questions at this stage are:
I reason their are a fair few things that would be needed to make this first practical and later standard; nice APIs, run times in various languages, and working out whether or not it actually helps CSS development to name but a few. But right now I’d go for a limited proof of concept that works on my machine. If anyone has any links to thinks that might be good starting points please let me know. Other ideas welcome as well.
One last thing; Mozilla’s latest employees are looking at the whole spectrum of developer tools. I’d love for them to start with something like this.
I’m starting to play around with using App Engine again for small projects. It’s great for simple, somewhat throwaway apps as long as you don’t need anything too fancy. Actually all I want really is long running processes but I digress.
I’m increasingly writing test suites as well for even small projects and was missing the convenience of the Django test runner for running them against App Engine code. So I’ve spent a little time writing a simple test running script to use for non-Django Python projects. I’ve posted it over on the App Engine Cookbook for anyone else who might want to do the same.
I ended up writing something myself as I couldn’t find anything else which quite met my needs, so it’s the typical programmers itch code and as such is provided as works for me software. The other approaches out their didn’t quite meet my needs but might be useful to know about if you want to start testing your apps.
I’m busy experimenting with various blogging approaches at the moment, hence the short links I’ve been posting recently. Another type of post I thought I’d give a try to was the list of interesting things. I find this sort of thing strangely cathartic – if nothing else by writing down the things I’m thinking about I won’t forget to spend time playing with them.
Interesting testing And coverage reporting write up by the Google Engineers behind Update Engine. This is the sort of thing we keep discussing in the office.
The latest version of VMWare Fusion lets you run virtual machines in headless mode. Which is pretty handy if you’re using a Linux VM to mirror your live environment. The strange thing is that it’s not enabled by default. To enable it you need to run the following on your console: defaults write com.vmware.fusion fluxCapacitor -bool YES
Working on a decent sized Django project at work means I’ve found myself delving into Django’s admin interface more than a few times. Although it’s always possible to just use a custom template and do everything yourself it’s nearly always easier and often quicker to use the generated admin views. One of the problems with that is, even with therecent 1.0 release, some of the options are not that well documented outside the source code or in posts buried on mailing lists.
I’ll assume a little bit of familiarity with the new-forms-admin way of doing things which is now the default in Django 1.0. If you’re just getting started with building Django sites then you might want to first have a look at a tutorial or two. It’s quite different to the examples found in the original Django book or older online tutorials but it’s also much more powerful and flexible with a better separation of concerns.
We’ll start off with a very simple model in models.py which defines a simple Article class with a couple of fields.
Django 1.0 introduced the concept of admin autodiscovery. By playing your admin declarations in admin.py in an application (most likely next to models.py and views.py) you can tell django to find these automatically. To enable auto loading of admin modules you can add the following to your urls.py.
.pre from django.contrib import admin
admin.autodiscover()
This will load the module admin.py for each of the apps in the installed apps list. Now Lets add an admin class in your admin.py to go with the above models.py. We’ll call it ArticleAdmin:
The important line is the last one, in which we register the admin for the Article class. This will display the relevant admin views in the Django admin for that model – allowing us to add new articles, list existing ones and delete old ones. But by default the admin is quite sparse.
Once we have a few articles in the system we’ll find it hard to find them again. Lets add a few more lines to our admin.py file:
Lets step though each of these statements and see what we’ve done:
The simple example above hopefully demonstrates the ease of which the admin can be configured. Knowing about these capabilities already built into Django can save you quite a bit of time when it comes to producing production ready admin interfaces. Except for more complex systems this should suffice. Below is a table of the Django admin options I’ve been using. If anyone has any more let me know and I’ll add them here, along with a brief description.
As you can see you can customise the default admin views a great deal even without creating your own templates and defining custom admin views. The best part is still that as well as being useful for demonstrations and prototypes these interfaces are useful on a live production site. Quite an achievement I think.
Open Microblogging looks pretty interesting. An open standard built upon other open standards for the purpose of passing information between micro blogging services like Twitter or Facebook.
Message Queues are cool. It’s official. Now, banks and financial institutions have been using big Enterprise Java message systems for years. But it’s only really over the last year or two that the web community at large have got interested. Wonder what all the interest is in Erlang, Scala or Haskell? Distributed systems and a lack of shared state – hopefully leading to some sort of scalability nirvana – that’s what.
Matt Biddulph of Dopplr has spoken at varying levels of technical detail on the subject over the last year or so. At barcamps and more recently at dconstruct. But you still don’t find that many people actually starting to use any of this stuff. Looking around the internet I couldn’t find that many examples of how to get started. Their are some pretty mature standards, good libraries, server interoperability, but few tutorials aimed at people who don’t know all about it.
The first problem is looking for a simple use case that most developers will have experienced problems with. The example I like to give is sending email. If you have a simple form on your site that sends email you probably just submit the request to the backend, it sends the email and then renders the success page back to the user. The problem here comes with scale. How many connections can your mailserver sustain? How many emails can you send from it before you start looking like you’ve been turned into a spam factory? At what point does the time taken for the mail server to respond to the web server cause the web server to time out or respond so slowly the user left or pressed refresh? If you’re sending lots of emails you need to think about this sort of stuff. For your average site this might not be a problem, but for the newer breed of applications or social networks this might bite you sooner than you think. You can gain more control over this process by introducing a message queue. Submitting the form simply adds a work task to the queue. A listener reads from the queue and sends the email. The advantage comes when you realise by removing the rendering of the page form the same process as sending the email you can throttle the system without affecting page rendering time.
So onto a simple working example. I’ve decided to use Python as that’s my language of choice at the moment. It’s also easy to read in a -sudo-pseudo code sort of way. Writing these examples using equivalent libraries in Ruby or PHP should be straightforward enough. As for the message queue itself I’ve opted for stompserver which is available as a Ruby gem. So assuming you have Ruby and gem installed (good instructions for this on the Rails wiki) you can just run:
Starting the queue is as simple as running:
This will get you up and running quickly. Stompserver has a number of arguments you can pass in to use different ports or backends but for the purposes of getting started it’s enough to just run it. This ease of use is the thing I love about stompserver. ApacheMQ is something of a tricky beast to setup, though you might want to use that in a production environment.
So now we have the server up and running we can get on with talking to it. I used the Python stomp.py library to deal with the heavy lifting. All the other modules are in the standard library. Their are equivalents for PHP and Ruby available as well.
The first script is a listener. Its job is to listen for activity on the queue and then act upon it. You pass the script an argument of the name of the queue to listen to.
This example simply prints the messages from the queue to the console, but in reality the on_message handler would be were you act upon the message received. In our email example above it would be were you parse out the email address, subject line and message and actually send the email.
Stompserver currently exposes a queue for monitoring the queue server at /queue/monitor. You can use this script to subscribe to that queue and get information about the current state of the server. It will tell you which queues currently have items in them and if these are currently being processed.
You can run multiple instances of this script subscribing to a single queue. This is one of the real advantage of message based systems, two listeners should clear a queue in half the time. This sort of horizontal scaling is hugely useful as you grow a site or application.
The second script allows us to send messages to the queue:
The script takes a couple of arguments, the first one is the name of the queue, the second is the message you want to send.
Both these scripts are pretty simple examples. In the real world you would probably want to make them a little more robust and user friendly. Both could probably do with checking they have the relevant arguments and providing help information if you run them without. I’d also probably move the hosts into a config file as it’s currently hardcoded into the scripts. I’ve also not tested them with other stomp compatible servers like ApacheMQ. In theory they should work fine assuming stomp.py works as advertised.
Overall, it’s surprisingly easy to get started with message queues. If you’ve been hearing about the advantages of distributed message based architectures but assumed you had to be Matt Biddulph to use them, think again.
Imified looks like an interesting way of getting started with using instant messaging bots in your applications. (Via)
A short break from blogging ends with a new site design. As with all these things their will no doubt be a few kinks still to work out and I’ll be adding to the design a little over the coming months. The main reason for all this change? A move to a custom CMS build using Django. This was something of an excuse to play around with Django outside work and I’m pretty happy with the results. I have all the bits of wordpress I actually used, plus a few bits I didn’t have before. More importantly I have something I want to hack on. Wordpress is a kick ass blogging tool, but keeping it updated or adding new features never seemed much like fun.
It seems like it’s the week for Django related site launches in the UK. We released the new Capital Radio site on the world last week and Nat beat me by a day or so with her new Django powered site. Look for more in the future too I would wager.
I have a whole range of posts brewing about what I found out along the way, both building a personal blog and building a large site with a big team. Keeping up with the bleeding edge of Django ahead of the 1.0 release took some doing (I’m running the latest Trunk release here at the moment). Deciding to use a combination of Spawning and Nginx for serving is a nice break from Apache as well. But that is all for later when I have a little more time.
I’ve not written anything here for a good few weeks, my tweeting has slowed down some and I’m behind on my feed reading. I’m going to blame the new job and the daily commute from Cambridge to London I think. I’ve definitely not been any less busy that usual:
We had an internal hackday on Thursday and Friday last week where lots of us over at GCap/Global downed tools and build cools stuff for a couple of days. This is exactly the sort of reason I took the job in London – for the opportunity to build interesting things quickly with other smart people. I got to play around with an event driven, music orientated, API hack and a more useful but less sexy documentation hack. Yes, I said documentation hack. I might be able to release the latter all being well but I need to finish it off first and kick the tyres on some internal projects
As a development team we’re using Django for everything which is proving to be huge fun. It’s a decent size project and we’re pushing Django (and in particular new-forms admin) in interesting ways. Having worked previously with PHP, ASP.NET and Rails I’m loving lots of bits of Django. I mentioned the template system before but their are lots of other things to appreciate. Some of this just comes from working with people like Simon and Rob who know Django pretty well. Some of it just from being able to write Python every day.
When not working on Django, or writing HTML, CSS or Javascipt (now mainly using JQuery), I’ve been busy pushing the benefits of Continuous Integration. As someone who is actually pretty bad at writing unit tests I like the process of working with Cruise Control as a gatekeeper. I also like automation in general so have been busy with Ant scripts and some Twill scripts as well for more functional testing. Django’s test suite is pretty nifty and easy to use. The official documentation is a pretty good starting point but I’m still on the lookout for some more in depth best practices articles.
So, between writing lots of code I’ve had less time to write words other than internal documentation. I’m finding the change pretty refreshing at the moment but want to keep up with writing every now and again. Who knows how that will work out? I have a feeling that I might blog more geeky code related stuff but time will see how that plays out.
In maybe a more constructive manner than yesterday I started wondering where the rock star web project managers hang out? I think we’re all aware of something of a celebrity culture within web circles. Their are a hardcore of people who’s blogs, books and conference appearances we’ve all seen several times over. And in the main I think this has had a positive effect on everyone involved. People like Jeremy, Molly and Simon have at different times acted as pretty useful barometers and yard sticks for lots of people. But these people are invariably designers and developers – not product managers or project managers.
The only person I can think of who has talked a little about the topic of project management is Meri with her new book Principles of Project Management. The topic occasionally comes up in conversation, or is mentioned by the designers and developers noted above. And their are lots of blogs (more often by developers it seems) about Agile, XP and Scrumm. But what about the practice of web product management? You can point to countless blogs written by designers and developers at the likes of last.fm, flickr and Yahoo. But where are the managers?
So, my question to you is: do you know of any great web product or project managers that blog about the discipline?
Another successful @media conference comes to a close and as usual interesting things were said and hopefully everyone learned something. As usual I have a few more things on my must find time to play with list. More on those if they happen.
But one part of the conference I felt needed addressing straight away was the first days panel. The Hot Topics panels bring together a few of the days speakers to answer questions posed by the audience. This year each day had it’s own panel discussion, with the first days session having a design theme. So far so good, and before I getting going I have to say I think Jeffrey Veen did a sterling job of prodding and prompting the session along. The rest of the panel composed of Andy Clarke, Dan Rubin, Bronwyn Jones and Indi Young.
The panel fielded a few interesting questions (and The Beatles in-joke was highly entertaining) but the whole thing took an odd turn part way through – odd in the sense that everything suddenly became very anti-developer.
Andy Clarke seemed to be the ring leader here saying things like (I’m paraphrasing here) “I hate daily stand-ups, we should do Agile”. Dan Rubin was involved too, seemingly saying all developers prefer the rigid nine to five because they’re logical people, while designers want to be able to work whenever they choose. This and other comments caused a few good friends and web developers on the front row to literally shake their fists in anger. The audience got involved too, cheering the anti-developer or anti-engineer statements on. In my mind damage had just been done to out our industry.
Buidling web sites or applications is a pretty multidisciplinary exercise, and in any team environment good communication is often the difference between executing well and failing badly. Conferences like @media are a great place to come together with people from different disciplines and similar jobs in different organisations and share stories. I’m not saying either Dan or Andy haven’t had bad experiences with developers or stand-ups. I’m not saying bad individuals or bad process don’t exist (I have scars from both). I’m saying that generalising these experiences and then promoting them to impressionable designers is dangerous.
I have the good fortune of working with a great designer, Alex Lee, at GCap at the moment. He’s involved in our stand-up meetings every morning and without him there we would waste untold amounts of time and effort. If he turned up on monday and said “I’m not going to come to stand-ups anymore, Andy Clarke says they aren’t cool”. We have a massive problem. My fear is this is exactly what’s going to happen somewhere tomorrow.
I’m something of a hybrid; I’ve run the whole gamut of design and development activities during my career to-date (I’ve even touched on project management) and have worked in small and medium sized agencies, as an independent freelancer and now in a decent sized in-house team. All of these roles pose different communication challenges which require different solutions. What works for three people in a distributed team doesn’t work for fifteen people working in-house. Some types and sizes of team work best with stand-ups, others work best being in the same small room together. Sometimes you need a centralised audit trail of everything that has happened on a project, other times it’s overkill. Somethings everyone has to be working on the project at the same time, at other times as long as tasks get done it really doesn’t matter.
I’m not saying I like all of those working conditions – I’m saying that if you understand where they work best you can work in the environment that suits you. Andy Clarke runs a boutique agency doing high quality work, working with a very small team where individual flexibility and minimal process is definitely the best approach. I now work in a cross disciplinary team of maybe sixty people and it’s a different ball game completely.
This isn’t an attack on Andy, or Dan, or even an anti-designer rant. The web is a young industry filled with young people. We’re learning as we go and stealing what we can from other disciplines. Also not that many of us really like the idea of project management, never mind the reality. Agile methods like Scrumm are hot at the moment but that’s not to say we won’t find something that better suites our discipline in the future. Iterative development, while often annoying to designers, appears to be producing better results. If designers, and developers, want to change how we work together for the better, then get interested in project management and lets have a discussion in public. If you want to complain about individual experiences then that’s why you have a blog.
The North East is stealing a march on London this week with all sorts going on for those who like a real world get together. Although not quite as busy as London Web Week next week we have the Thinking Digital conference kicking off tomorrow, followed by a Geek Dinner on Friday night and then BarCampNorthEast over the weekend. I’m even planning on popping out tonight with a few people in town for the festivities. As if fate somehow got involved the weather is also pretty darn nice today too.
If you didn’t already know about these events and you’re in the area their just might still be the chance to come along. We have a few places left for barcamp in particular, you just need to register over on eventwax. And the Geek Dinner on friday night at The Pitcher and Piano is open to all, at least until it fills up.
It really is great to see thing happening at this end of the country, and a real shame I’m leaving the area so soon afterwards. I really hope the local community gets behind all these events and they act as a catalyst for interesting things. I’ve always been of the mind that if you can get enough smart, interested people together in one place then good things will happen. It’s why I enjoy going along to conferences, hackdays and barcamps. It’s also one of the main reasons I joined GCap.
Once I’ve recovered from the next two weeks (which may take a while as next week I’m in London for @media, BarCampLondon4 and anything else I can get along to) I’ll hopefully write more about trying to organise a barcamp and, if I really get time to think, about knowledge workers outside and inside London.
But in the meantime leave a comment if you’re going to be around in Newcastle over the next week. It should be a great opportunity to meet new and old friends alike.
I just finished my presentation on the last day of the Xtech conference in Dublin. I’d chosen to ramble on about the advantages, problems and a few solutions of building applications atop of lots of APIs. The presentation is now up on slideshare at slideshare.net/garethr/design-strategies-for-a-distributed-web/.

Lots of interesting conversations have been occurring all week and a few people have mentioned a near barcamp feel at times. It’s a pretty small, clued-up, technical audience and as always some of the best bits have been conversations in the corridors.
Jeremy has been live blogging many of the talks which should be a worth a read later on. Although with three tracks most of the time that’s only some of the great talks on offer.
On departing pubstandards in London a few weeks ago Dirk Ginader said something along the lines of:
I’m not saying that had anything to do with it, but I’m happy to announce I’m moving to London in a permanent manner very very soon. I’m taking up a job at GCAP, working with quite a few people I know from various web goings on; Hi to Simon, Ross, Brad, Ed and Stuart amongst others.
Freelancing for the last year has been great. I’d been able to work on the variety of projects I’d wanted after leaving my previous job, and I’ve had more time to myself when I wanted to get involved with other projects. WaSP, SXSW, Highland Fling, BarCamp, Thinking Digital to name a few things I’ve been up to in the past 6 months. But something about working with a super smart team of people won me over. The fact that I’ll get to focus on Python and Django is an added bonus.
Patrick apparently closed the book on this happening some time ago. I’m just hoping someone got the date about right and won the money. I’m sure I’ll write more about the whole thing as time permits and lots of specific topics spring to mind. I have a number of events in the next month to get along to before all this happens however; if you’re along at Xtech, Thinking Digital, London Web Week, BarCamp NorthEast, BarCamp London 4 or @media then remember to say hi. My addiction to attending web events can only get worse with me being nearer London I fear.
Build scripts. I think I’ve probably mentioned before; everyone knows they should but not many actually do. I’m not talking about your large in-house development teams or your sexy web startups; both probably have a good-enough build process for different reasons. But smaller teams, web design agencies or freelancers often rely on FTP and a prayer. One of the problems is definitely finding suitable documentation. It’s not that their isn’t a lot of good quality documentation – it’s the suitability of it for those with only a passing interest and a limited systems administration experience.
Capistrano is a tool for automating tasks on remote servers. It can be used for all sorts of useful things but we’ll concentrate on it’s utility for web site deployment. It’s been popular with the Rails crowd for a while and is written in Ruby, but it can be used for deploying sites in Python, PHP, Django, whatever you happen to be using at the time.
You’ll first need to install Ruby and Capistrano. The Capistrano site has installation instructions which should get you up and running. You’ll need to be able to use a console on your machine and have a passing acquaintance with your webserver. I’ve also assumed that you’re using source control, mainly because if you’re not you should start their. Automation becomes much more useful and practical on the back of a good source control system.
Right, lets get started. Create a file called capfile somewhere on your local machine. The following two code snippets should go in that file.
Just replace the variables (marked with angle brackets) with your own values. The path is the full path on the remote server where you want to deploy your files to. Note that I’ve abstracted out the webserver restart command and the source control checkout command. The above example values assume you’re using apache and subversion (and want to do a subversion export). It should be easy enough to change these for your preferred combination of source control system and webserver.
If your platform of choice doesn’t require a server restart (say PHP) you can always remove the ; #{restart} portion of the command.
Now we’re all set up just bring up a console in the folder containing the capfile and run the following command:
After prompting for a password to access the remote machine this should display the output from the commands being run. Depending on the size of the repository you’re checking out
Of course for most projects you would want to expand this basic recipe. Maybe you have database updates to run or need to deploy to multiple machines at once? You’ll might have machine specific configuration files which aren’t in the source control system. You will also probably want to be able to revert back if something goes wrong and you might want to only change files that have been altered. Capistrano has lots of powerful features built in that will let you do just that, and once you get started with this sort of build scripting you’ll find lots of areas to improve. And you’ll find lots of good tutorials online that will take you further.
The difficult part with project automation and build scripts is often the getting started in the first place; especially if you’re not the server admin type. Unfortunately the documentation and articles written on the subject tend to be somewhat arcane and aimed at the more hardcore developer. Hopefully this simple, usable, example convinces someone else to give it a try.
I often end up pondering URL design given a moment and something that keeps coming up is hackable search queries. But first a very quick primer on the idea of resourceful design.
REST is a series of architectural principals more than a defined architecture. The Resource Orientated Architecture builds on those ideas with a series of concrete guidelines put down by Sam Ruby for designing RESTful systems. The simple version is that you try to design your system around resources represented by URLs.
I’d thoroughly recommend reading RESTful web services whenever you get a moment as this subject is covered in detail.
Flickr isn’t a truly resourceful design but it does have many of the hallmarks. For example the URL that describes me is at:
When it comes to searching on flickr we have:
The pattern of using a query string argument named q to pass a search string is pretty common. One of the guidelines mentioned as part of the ROA discussed query strings:
Search is definitely algorithmic. Now you could maybe argue that a global search should be done on the root of a site, with specific resource searches on the resource in questions. eg. /people/?q=. This would likely work fine but require some behind the scenes complexity as well as probably not being as obvious to the end user. Global searches are in many cases much more common that restricted searches and even in resourceful designs the root of the site (ie. the home page) might not act as a list of available site resources. A notable exception might would have to be the excellent BBC Programmes site which is basically one big semantic catalogue.
But we have another kind of URL that’s cropping up for search results, one that treats the URL much more like a fundamental part of the user interface for search. An example from a site I use all the time is The Accessible UK Train Timetable which allows for URLs like the following:
You can basically squash all the search parameters from the form into the URL, meaning you can easily bookmark search results. Note however the actual content is likely to change. The above example for instance would use the current time to get a list of trains from Newcastle to London. In an hours time the results will be different.
Another good example would be the new Yahoo! UK TV listings which has URLs like these:
Again this is really a search query, or at least specifying the time and date is. In some ways it’s the return of the command line – allowing searches to be run very quickly from a textual interface.
Now, both these approaches treat URLs with the respect they deserve. But they do have the potential to clash somewhere in the middle if care isn’t taken. The Accessible Train Times site is a single purpose site which just does searches while BBC Programmes does feature a search engine but it’s just the global BBC search which takes you off site. And if that wasn’t enough potential competition then a question raised by Simon at The Highland Fling regarding URL design and the search engine optimisation crowd go me thinking too. From being a somewhat niche area of interest URLs might just become a sort after part of a good website design – fought over by the varying disciplines of modern web design and development.
After my previous post about Django and the web standards community a number of the comments picked up on the fact I mentioned haml under the title Other Craziness. Ok, so I was being a little over-poetic but I decided this warranted a closer look.
Haml is a markup language that’s used to cleanly and simply describe the XHTML of any web document without the use of inline code. Haml functions as a replacement for inline page templating systems such as PHP, ASP, and ERB, the templating language used in most Ruby on Rails applications.
A quick example should help. The following haml code…
…is compiled to the following HTML:
Depending on your application this could be at runtime or as part of a build step. Although primarily associated with Rails because haml is also available as a command line utility you could in theory use it with any framework or language.
My initial take on this was to call haml an abstraction of HTML but Nathan Weizenbaum, one of haml’s developers, put me straight:
Lots more examples for anyone interested can be found on the haml documentation site.
After some research and some playing around with the command line version of the haml engine I decided to see what Twitter thought about the situation. Little did I realise what I was letting myself in for:
Tom Morris kicked things off:
Brad Wright thought:
And followed with:
And Mark Norman Francis chipped in with:
A few people echoed Ross Bruniges sentiment that haml and sass are just:
I have to admit to this being my initial reaction on hearing about and looking at haml, hence the remark from the previous post. But that’s not to say everyone was negative.
Mike Stenhouse stepped in and said:
Some of the comments were about how the use of haml might alter the dynamic of a team, to either positive or negative effect – depending on your point of view.
Mark Ng saw it as a cunning way of getting rid of the front-end guy.
Where as Olly Hodgson say it maybe as a route to get the dyed in the wool back-end writing decent markup.
At present haml is very much pitched at the Rails community from whence it came. Many of the examples demonstrate benefits compared to ERB, and haml is of course written in Ruby and available as a Rails plugin. Being perceived as part of that community has obvious benefits but also some subtle costs, in particular regarding those people that don’t like Rails very much.
I’m not really convinced of the benefits in all fairness. The something else to learn barrier only gets magnified when working within a team environment. You now have to train new recruits of whatever skill level in another syntax. One that they might be able to write quickly enough but can they understand from the briefest of glances at a template? HTML might not be great here but it is familiar to everyone. Their is also the programmers abstraction. What if I can’t get the markup I want out the other side of the black box? Yes it’s open source so I can hack the box open but that causes even more problems. And while I quite like meaningful whitespace (for instance in Python), in templates which fail if it’s not quite right I see a major problem for those whom a text editor is not their best friend.
I am however interested to see whether the problems people have with haml are with haml in particular or with the overall approach of alternative syntax’s for HTML and CSS. Are DSLs (Domain Specific Languages) needed for CSS and HTML? and if so is this a possible avenue for innovation on top of slow moving standards?
I finally got round to making use of Git, the distributed source code management tools much loved by Open Source projects like the Linux Kernel and now Ruby on Rails. It had been on the long list of things to have a look at for quite a while but for the majority of my personal projects SVN is just fine. The reason that led me to finally run sudo port install git-core was the new Ditz command line issue tracking software release on Gitorious by William Morgan.
Now, a command line issue tracker is maybe kind of niche and a little geeky. But most desktop or web based issue trackers just seem to wind me up and I like small tools that fit in with my command line centric workflow. I managed to clone my own copy of Ditz to have a play with. I ran into a few initial problems but nothing a good tutorial and a helpful mailing list couldn’t fix.
I’m not just playing around either; Ditz comes with the nice ability to generate a set of static HTML files representing your issue database. I’ve committed and pushed a few modifications and enhancements onto my clone – mainly validation, semantic markup improvements and a smattering of microformats (work in progress) in the templates and a couple of markup generation helpers in the Ruby code. I plan on working up the interface a little as well but like any good web designer I’m starting with the underlying markup structure.
A few other enhancements I’d love to see in Ditz or a clone I could pull from include the ability to use  SQLite rather than the default YAML for larger projects, automatic generation of RSS feeds alongside the HTML (although I’ve started implementing hAtom) and maybe automated deployment to a remote host over FTP or SSH.
The distributed nature of Git appears to be pushed as the main advantage to those developers taking a look. But I think this is, in many cases, is not that important compared to the familiarity that comes from having used SVN or similar for a while. What I think just might be it’s real strength is that Git is a social source control system. For Open Source projects like Ditz this is a potentially game changing move. The ease with which anyone can contribute and take things in strange directions without affecting the overall effort is fantastic. It removes barriers to entry for contributors and means making small quick changes is much easier and more immediate. With both Gitorious and Github wrapping Git in a shiny web based interface that focuses on these social aspects it will be interesting to see if this leads to more or faster Open Source collaboration efforts.
I’m just back from another great few days in Scotland for The Highland Fling. The biggest difference for me this year was I was lucky enough to be one of the speakers. Amongst such internet luminaries as Simon Willison, Norm, Chris Mills, Christian Heilman, Aral Balkan and Paul Boag I presented on a few web building blogs (HTTP, URLs) and on some suggestions for API design. The presentation is available on slideshare for anyone who would like a peek.

I think the topic went down well and it triggered some good discussion around the nascent issue of API design. One thing is clear – we need some good resources filled with examples on the subject. More and more people are going to be extending various bits of software with an API and it would be nice to think they will all be a pleasure to use. In the meantime is you have any features or guidelines your like to see in APIs then suggest them over here.
The rest of the conference was great. I particularly enjoyed Norms little history lesson and Aral has me at least thinking about installing all the Flash and Flex tool chain. The highlight for me was probably the format. After each presentation we got a little grilling by Paul. He came up with a few thorny questions for each of us as well as fielding questions from the audience. Hopefully my stint on the sofa made sense to a few people, certainly everyone else threw out a few interesting titbits that probably wouldn’t have been talked about otherwise. Also, Chris Mills nearly managed to make Paul cry with laughter by striking something of a rock star pose for most of his interview.
I’ve just uploaded a handful of photos from the event and surrounding geek gatherings too. All in all the event was great (again). With the close venue, interviews mixed in and general friendly atmosphere The Highland Fling had an intimate feel often missing from events. Huge credit goes to Alan and hopefully we’ll all be back next year.
Quick post to say BarCampNorthEast tickets are now available. We’re starting off releasing 50 tickets and we’ll see how that goes. We’re really not sure how quickly these will disappear so get them while they’re hot. More tickets will be available later as well.
barcampnortheast.eventwax.com/barcampnortheast/register
The event is looking like its going to be great. I’m really happy we managed to find a venue that would let us do the whole sleeping over thing. And with Thinking Digital the week before and rumours of a geekdinner on the friday night it’s going to be busy few days in Newcastle.
We’re still on the lookout for a few sponsors as well so if anyone is interested in sponsorship opportunities, or just has a few questions feel free to drop me a line.
I’ve been noticing an interesting trend recently, not one I have any empirical evidence for mind, but one I though interesting non-the-less. Parts of the webstandards world appear to all be playing with Django. Part of this has been the odd mention down the pub, at barcamps or at SXSW this year. But the main source of information on the topic has been twitter. To name but a few I’ve seen tweets from Steve, Ross and Aral recently and Stuart and Cyril literally wont shut up about it.
What’s interesting is that this didn’t happen with Rails, not in the corner of the pub that generally talks more about markup, javascript and CSS anyway. I’ve worked on a couple of Rails projects both personally and commercially, and I’ve just launched a little pet project build with django called doesyourapi. What follows is, to my mind, a few reasons why I think this trend exists and also why I think it will continue, at least for the time being.
You can’t ignore the personal touch, and in Simon Willison and Stuart Langridge we already have two people who bridge the Python/Django community and the web standards crowd, at least in the UK. Personal technology choices at least are often driven by personal correspondence.
Django’s templating introduces a very simple syntax and nothing else. Rails lets you have the full power of Ruby to do with as you will within your views. Rails also makes heavy use of helpers, further adding to the complexity of views. Now I have mixed views here, based on my own skills more than anything. I know I’d feel much more comfortable throwing someone with good markup skills at a project using Django than Rails. For the most part with Django you use the html you’re used to, Rails often wants you to change this to helpers – in much the same way as ASP.NET does in fact. I think some of this comes from the Rails don’t repeat yourself philosophy obsession. Sometimes this leads to programmatic complexity which makes working with templates more akin to programming, even if it means less duplication. I’ve yet to work on a particularly complex Django project so maybe this simplicity might become a limitation to work around? Always a possibility.
Some of the bits and pieces that come bundled with Rails are just plain wrong, the Javascript helpers being one example. The abuse of HTTP by default in some of the scaffolding code being another. Oh, and the markup coming out of various helpers as well. In trying to help the application developer Rails gets in the way of the professional webstandards types. Django does next to none of this for you. Programmers coming from Rails might see this as missing features. Frontend types prefer this clean slate approach because it means you don’t have to fight the backend (sometimes including people) for control of the output. Note that you can work around much of this (in the same way as you can work around ASP.NET if you have to), it’s just nicer to not have to.
Rails people love Ruby. After all it’s better than Java (it’s also a pretty loveable programming language too). But like computer science departments everywhere many Rails people also dislike or simply put up with HTML, CSS and Javascript. If they can find a way of not having to write these and write something else (Rails people are also obsessed with domain specific languages) instead. Hence we have the likes of HAML and SASS. The problem is that us frontend loving folk quite like writing CSS (well, sometimes) and absolutely love writing HTML. Most of the time for good reasons too – just look at microformats for an example. Frontend developers tend to like using a mix of tools, predominantly backend developers not so much it seems.
Personally I find it interesting. You could quite easily flip many of these arguments around to support why so many people are using Rails. For two frameworks with similar goals and uses it’s interesting to see the early philosophical differences playing out in the real world. It might be interesting to see what happens with frameworks like Merb as well which seems to be set out to avoid many of these perceived issues with Rails. So, have anyone else noticed anything similar? Or even the complete opposite?
BarCampNorthEast is go. A few “people”:"http://www.agm.me.uk/blog/ have finally got together and sorted out the long promised barcamp in Newcastle upon Tyne.
We’re going to be holding the event in the middle of Newcastle, at The Art Works on the weekend of the 24th/25th of May. That’s a whole two months away, ample time for everyone to make arrangements hopefully. From the early discussions we were always set upon going the whole hog and having a two day event. The venue is big enough for people to sleep over as well if they want which is great. It keeps the cost down for anyone visiting as well as meaning we can play Werewolf all night. We’re piggy backing on the Thinking Digital conference as well so hopefully some of the people from that will stick around for the barcamp.
We have the requisite Barcamp wiki page as well as a listing on Upcoming. Feel free to indicate your interest on either of these. Registration isn’t open just yet, but we should be opening that up next week on Tuesday 1st April at 11:00am. Look for the link on here, on twitter, on the upcoming and barcamp wiki pages and anywhere else I can think of to shout about it.
Feel free to contact me with any questions. If you’re not sure what all the fuss is about then have a look at the barcamp site. And if you’ve never been outside London before then maybe this is your chance. Newcastle is only two and a half hours away by a train with free wifi. We’re also on the lookout for a few sponsors; if your interested drop me a line.
I’ve been playing with Twill a little recently. It’s a Python based DSL used for functional testing of websites. From the official website:
A simple example might make things clearer. You’ll need to install twill first – the instructions are available on the site. We can write tests directly into the shell so we’ll start their. For our first test we’ll write one that will hopefully fail – a test to check whether this website is down.
First we fire up the twill shell then enter two simple commands. The first command, go, sends the browser to the specified URL. The second command is an assertion, in this case a check on the HTTP status code. If this website is available then it should return the HTTP code 200, if it’s unavailable then it will probably return a 404 Not Found. This test will hopefully fail, indicating that this website is up and running. In reality you’re more likely to test for the 200 status code and fail on anything else but for this example it’s useful to see what a failing test looks like.
Although pretty powerful the twill scripting language is nice and small. I’ve listed most of the commands below just to give you an idea of the sort of things that you can get up to. You can type help at the twill shell to get more information on the available commands and the individual commands themselves.
You can also store tests in individual files as well as run a batch of tests at once. I have a couple of tests that I can run against any URL which might be a useful starting point for anyone else starting to look at testing their sites or applications. You can download these tests here.
If you unpack the zip archive and then open the folder in a terminal or console. You can then run all of the tests like so, note we’re passing the starting URL into the scripts which makes using the same scripts for multiple sites easier.
The tests included do a few things; from checking for the presence of several required markup elements and checking for an XHTML doctype to checking that all the links on the page are working.
All these examples are pretty simple and non-site-specific. For more complex form based applications you can write application browsers which fill out forms, create user sessions and do everything a user might do. Twill is also particularly useful when it comes to testing RESTful webservices with all the URLs and HTTP status codes floating about.
If URLs are people too then you better make sure you control your URLs.
Although some people use blog hosting services like blogger the majority of serious bloggers, web designers or companies generally use their own domain name to host their site. Controlling your own domain name is increasingly important when that URL is a representation of you on the internet.
With all these social networks we’re starting to have pieces of us scattered all over the place. I’ve joked previously about the utility of domain names over real names for interpersonal communication but this breaks down a little when not all the urls that represent me are owned by me. I control morethanseven.net/photos but I only have some influence over flickr.com/photos/garethr. With external hosts you also need to be aware of cybersquatting. I own morethanseven.net so no one else can use it. However with services that give you your own URL as part of registration everywhere people can cybersquat in hundreds of new domains that you might not even know about.
It’s not just web pages you have to worry about. Feeds are another example of URLs you just might want to keep control over. This last one is also something I see lots of people handing off to others – specifically Feedburner. Now I’m a big fan of feedburner and use it for the feeds on this site. But I don’t use the feedburner URL, anyone subscribing to the feeds here does so using morethanseven.net/feed/. If I decide to stop using feedburner I can, without having to upset the few people who subscribe to it by moving my feed address. It’s the same with email addresses; I love gmail but rarely actually give my gmail email address out.
So, start looking after your domain names a little more carefully. They are pieces of you scattered around the internet, and losing control of them is going to become increasingly socially painful.
So, while sat with a few people at the WaSP panel at SXSW (of which more later when I’m fully caught up) I got a nice email from the folks at Xtech accepting my presentation idea. The abstract is below. If any of that sounds interesting or up your street I’d love to hear other peoples experiences or ideas on the subject.
Everyone is making use of mature and stable web application or javascript frameworks these days. That means weâ€™ve stopped reinventing things such as routing and object relational mapping, but weâ€™re all still building very similar components again and again. Frameworks might allow us to solve lots of fine grained problems but APIs could let us solve common course grained problems quicker.
Their are already a few examples in the wild of APIs designed to be used as part of your application development process. Amazon has been leading the way in providing remote services such as S3, EC2 and SimpleDB. Their are also options when it comes to hosting these services yourself, the mint analytics software and the CouchDb database service are both good examples.
The real value of outsourcing a discreet portion of your application to a third party API lies in quality. You could always use local storage and your programming language of choice to deal with a large volume of file read and write operations. But do you really think youâ€™ll beat Amazon for reliability, scalability and speed?
Itâ€™s not just high quality functionality that we could leverage from other providers. Weâ€™re all fed up with entering and re-entering our personal data into each new service. With advancements like OAuth and Microformats and lots of focus on data portability at the moment we might just be able to share data too.
Sometimes itâ€™s not enough to just change the server. The rise of specialised browsers such as Joost and Songbird allows for functionality that would be impossible otherwise. Site specific browser, along with advancements such as local storage, may prove
Problems
Itâ€™s not all in place just yet. The reliability of your application is likely to be important, and making use of a distributed set of APIs could leave you in the unenviable position of being less stable than your least stable partner. The issue of lock-in could also raise itâ€™s head, without a vibrant ecosystem of different providers that is.
The use of third party commercial APIs has the potential to change the development landscape â€“ bringing high quality middleware to the web. It could be the original web services dream realised. But without critical mass and an active market it could also be a new achilles heel for successful startups.
I had need recently to produce some nice looking charts and immediately turned to the very nice Google Charts API. Just before Christmas Brian had written up a great introduction on 24ways and ever since I’d been looking for an excuse.
Chris wrote up a pretty nice approach to enhancing well marked up data tables using the Charts API with a dash of Javascript and I decided to start with that. I made only a couple of changes to this approach based on personal requirements and preferences.
For a table like this:
I prefer to use the table header element to mark up the table row and the original script relied on these being td elements. A couple of changes to the javascript fixed that. I also decided to display the caption as a title on the resulting graph.
The original script only supported the 3D pie charts and in a couple of cases I wanted to generate bar-charts from the data or flat pie-charts. A few modifications later and you can pass a type parameter into the script via a class on the table.
The default if the parameter isn’t set is the flat pie-chart, which can also be specified via:
For the 3D pie-chart:
The horizontal bar-chart is created with:
And finally the vertical bar-chart is set based on:
The complete modified version of Chris’ script is below:
I’m starting to get excited about Thinking Digital. But first a bit of back-story.
I started out making the trip to @media 2005 and since then have been a regular attendee of more conferences than I can shake a stick at. Lets say I caught the conference bug. But most of those have been within the web standards community niche and more recently I’ve been looking further afield for conference kicks, party due to the eclectic nature of BarCamp style events. Anyone in that positions inevitably takes a fancy to going along to TED. The only problem is the fact it’s already booked up until 2010, costs $6,000 just to for a standard membership and is invite only.
This year the folks over at Codeworks, which is a regional body set up to help us North East digital folk develop an industry, are organising their very own full size conference. The Think and a Drink events have been running for a number of years and I’ve spoken a few times on web related bits and pieces. But these are purposely local events mainly for members and tend to crop up at the rate of once a month. Thinking Digital is a full size, 400 person, conference being held in Newcastle between the 21st and 23rd of May this year covering a similar set of wide ranging themes to TED; technology, design, mobile plus a bit of philosophy and futurism thrown in for good measure.
Now this is good for a few reasons. Only one of which is I can see the venue out of my apartment window. Costing only Â£475 and being in the UK would be a couple of other reasons to get excited. As an aside; I’m helping out as a board member along with the likes of Ian Forrester but I’d gladly pay my way if I wasn’t.
The real reason for the excitement though is the calibre of some of the speakers. Ray Kurzweil, Greg Dyke, Dan Lyons (a.k.a. Fake Steve Jobs), Matt Locke, Aubrey De Grey and Tara Hunt to name a few I’m particularly interested in. And remember folks, this is in Newcastle. Which for the few Londoners reading is a couple of hours North of you. On a train with free wifi even.
Now this might not be a web conference but I’m pretty interested in the take of the likes of Ray Kurzweil on the web and what will happen in the near future. I’m also interested in the types of people who will make the trip – which is really the reason for this post. As everyone knows, one of the best parts of any conference is the chance to chat with like-minded (and not so like-minded) folks. So, who fancies coming along?
After SemanticCamp me and Rey popped in to see Paul and everyone at Osmosoft towers. A good few interesting conversations ensued, including one about the difference between mashups and integration. All good fun basically. Simon also had an interesting take on the topic as well.
What has all this to do with the topic of this post? Well, Simon says:
Now I’m not really picking on Simon here, more that I’ve been meaning to write something on this topic for a while and this proved a good catalyst for a little rant.
The flickr API is pretty darn cool. But it’s not RESTful in pretty much any way you want to shake a stick at it. It’s a well designed RPC (remote procedure call) API. Now even flickr get this wrong. They even have a page which confuses the issue and makes REST out to be a response format based on XML. Bad flickr.
flickr states that:
This turns out to be completely against the whole RESTful principles. Lets try and explain. You can define most of what you want to do with an API with the use of nouns, verbs and content types. REST is based around limiting the set of verbs to those available in HTTP; for instance GET, POST, PUT and DELETE. For any given application or API you’ll also specify a set of content types (representations); for instance HTML, JSON or XML. All you get to play around with are the nouns, in the case of the web these are our URLs.
In a typical RPC style API you have one URL (like flickr does) which acts as the end point to which all calls are made. You then probably define a few functions which you can call. Lets look at a simple book example.
The RESTful way of designing this might look a little bit more like this:
We mentioned content types or representations earlier. Lets say instead of the default response format we might want to get a JSON representation of a given book we might do something like the following.
The advantages of using URLs as the API nouns in this way include more than just sane URLs for the site or application in question. The web is based around many of these architectural principles and that seemed to scale pretty well. The idea is that fell envisaged RESTful applications have an advantage here too. For me one of the real benefits of RESTful APIs are in the simplicity they bring to documentation. I already know the available verbs, all I need to know are the set of resource URLs and I can probably use CURL to work out the REST (sorry, bad pun).
This misunderstanding is pretty common. Even WikiPedia appreciates their is a problem:
This isn’t just pedantry on my part, well not quite. I’d recommend anyone involved in designing and architecting web sites read RESTful Web Services as well as Roy Fielding’s Architectural Styles and the Design of Network-based Software Architectures. But if you just want to get the general principles and don’t fancy wading through technical documents then you have to read How I Explained REST to My Wife hilariously written by Ryan Tomayko.
And remember, just because an API makes use of HTTP doesn’t make it RESTful. I’m looking at you twitter.
I had a great time at SemanticCamp over the weekend which was not too much of a surprise. What was a surprise was getting back home to find out that I’ve been invited to join the Web Standards Project (WaSP) on the Education Task Force. I even have my own page.
Thanks for inviting me guys. Anyone who has had the misfortune of having me present during a discussion of the current state of web education knows it’s one of my favourite subjects. Hopefully I can make myself useful around the place and help with getting a few things done. More on this when I know more but in the meantime feel free to pester me endlessly if you have a particular axe to grind.
Yahoo! Live launched recently along with a nice RESTful API. I’ve spoken before about the beauty of REST being in lowering the barrier to hacking and when I wanted a quick feature for Live it was simplicity itself to put together.
A few friends are using it far too much it seems, Ben has 7.6 hours and Si has already clocked up 15 hours. But for the most part I keep missing their no-doubt highly entertaining antics. One thing that Live misses I feel is a favourite users or previously viewed channels list. Basically I want to see which of my friends who use the service are broadcasting right now. Something like:

The API request we’re interested in is the /channel/PERMALINK method. This lets us get information about whether the user is broadcasting at the moment.
I’ll add a few more people to my list when I discover other people using the service. If you have an account leave a comment. I’ve added a touch of javascript as well so as to avoid having to reload the page manually. This way I can loiter on my little aggregator until someone I know starts broadcasting and head over to Live for whatever Si has been spending 15 hours doing.
Most software developers, especially those with a grounding in Agile development methodologies, will probably be familiar with Continuous Integration
The emphasis above is mine, purely as it’s at the heart of what I’m going to ramble on about. A little closer to home Ryan King just posted about a new site; inursite. The premise is simple; enter a few of your sites and inursite will visit them once a day and run a markup validation service over the page. You then get a feed of the pass or failure status. It’s simple but brilliant. For example, I have this very site added to the service. If I put some invalid markup in this post, tomorrow morning I’ll get an item in my feedreader telling me of my mistake. I’ll get that every day until I fix the problem.
This green/red (pass/fail) type approach to simple tests is what I find most powerful about continuous integration systems like “Cruise Control”:":http://cruisecontrol.sourceforge.net/. Ryan asked over on his site in one of the comments what I’d like to see, so lets see:
Lots of this is front-end best practice, some coming from the YAHOO! exceptional performance work. It’s something I’ve touched on before too. Can anyone else think of other things you’d like to see when you’re working away crafting markup and CSS? Once you have all these tests running you could display them in widgets, gadgets, twitter, firefox extensions, large displays, mobile devices, the works.
Now that sounds like an awful lot of stuff for one person (or even for one application) but I have something else in mind. If inursite allowed you to hook up external webservices which accept a URL as an argument, along with any service specific parameters, and return true or false then, in theory, anyone could add their own custom checks to it. This becomes particularly useful for larger teams than are likely to have internal quality tools already. On top of all that I’d probably pay for a service like this that let me run it on demand (rather than once per day) – or maybe even better, pay for a downloadable version (a.l.a. Mint) I can install locally.
As you can probably tell, I think the general idea of continuous integration for front end web developer is one for which time has come. It’s simply part of our discipline growing up and becoming more professional. Whether Ryan looks to extend his fantastic simple service in this direction or not I hope something will come along that does all this and more. I might even work on it myself – but then I always say that!
Another approach to deploying web apps is to use Phing. Phing is at heart a PHP clone of Ant,  another common build and deployment tool. The main advantage of using Phing, at least if you’re already using PHP, is close integration with other PHP specific tools (PHPDocumentor, PHPLint and PHPUnit to name a few) and ease of install.
Speaking to installation Phing has it’s own PEAR channel. I still think PEAR is great in so many different ways – it always makes me frown when I see people cussing at PEAR (especiallly the installer). You can install Phing as follows.
The Phing documentation is nothing if not comprehensive. Unfortunately, unless you are pretty familiar with Ant or you’re trying to do something complex (and willing to invest the time) the chances are you’ll be a little lost. More and simpler examples for common problems would be useful for those beginners.
I might publish a few little recipes down the line or even a full commented production recipe but for the moment lets start simple. The following build script is designed to be run on your remote web server and relies on a subversion repository (hopefully you’re using source control, if not that’s another post I’m afraid). It simply exports the specified repository to the specified export directory on the server. Note that you need to set your own SVN details and replace the capitalised properly values. The following should be saved as build.xml on your web server, outside the web root.
Let’s break some of that down. Property definitions allow you to specify variables for use in your build script. In this example we only have one task specified so maybe we don’t need the extra abstraction but the moment we start reusing scripts or adding more tasks it’s a good idea.
The other major point of interest is the task itself. Here we make use of the properties we have already specified to run a subversion export. The svnexport task is build in to Phing.
Also of note is the setting of a default task, main, as a property of the project element and specifying that the main task depends on another task, svnexport. Again, we could avoid that at this stage but the moment we add another few tasks then we’ll want more control over execution order.
Phing should be run from the directory containing the build script. You can run the command without any arguments, in which case the default task will be run (in our case main) or you can pass an argument to specify a specific task. For our simple build script  the two following commands do the same thing:
Given the above build script simple runs a subversion export command you might be wondering what use it is. The answer is not much at this stage. You do get a simple build script which can be stored in your source control system and used by the whole team. The real advantage is where you might go from here. If everyone in your team deploys in the same way (ie. using the build script) then anyone can add extra tasks and everyone gets the benefit. Simple example might be generating code documentation, running Unit tests (and not running the export if the tests fail) and creating a zip file of the deployed source for backup purposes.
Build scripts can get complicated quickly, but if you start out small and add tasks as you need and understand them, you should be able to raise the quality of your application and avoid easy to make mistakes.
Along with an awful lot of noise, the whole X-UA-COMPATIBLE IE8 issue is also bringing out some well rounded and thought through arguments from people I admire.
I still disagree with Jeremy Keith on the default issue but agree with everything else. Jeremy says:
At the moment it comes down to a sense of stubborn realism on my part. All the people who know about the issue and the need for the new header (that would be me, Jeremy, probably you and the whole web standards developer crowd) are probably also in a position to add it fairly easily. All those that don’t know about the header don’t have to know about it. Ever. I would prefer if this wasn’t the case but alas I think it might be.
Mike Davies has one of the most well argued points out their. Throwing into the pot the issue of content not served via a web server raises even more issues that make changing the default behaviour difficult. I think Mike sums this up pretty well:
Whether we like it or not this issue is about Microsoft, and I (maybe naively) believe Microsoft would like nothing better than to get back up from the mat and innovate in the browser space. But they can’t because of the ball and chain.
Who hasn’t written at least some code that, in hindsight, turned into a horrible maintenance nightmare? You might dream of jumping in and rewriting it from scratch but if that code has already been shipped to a client then what is that client doing to do? Turning around and saying actually, what we gave you originally was pretty rubbish isn’t going to go down well – even when it’s the right thing to do.
Mike is right in that the IE team are actually going to be shipping two browsers. That has got to be a massive issue for them – certainly a bigger issue than me adding a new header to my site or a meta element to my pages. But if it lets the second of those browsers support CSS3, HTML5, XHTML, Canvas and anything else we dream of then I’m all for it. We all know the first of those browsers just won’t be up to the task.
One particular thorny problem I wasn’t aware of was issues surrounding accessibility. Bruce Lawson has some of the details but in short by locking a large number of sites into the ways of IE7 we essentially freeze the (poor) state of accessibility on the web. Patrick Lauke also raises a related issue in the comments regarding further alienating screen reader and assistive technology developers. These points for me are ones I want addressing by Microsoft and one’s without resolution which would make me change my mind on this whole issue of defaults.
So, the jury is still out in my mind. I hope it is in the minds of those working closer to the heart of this problem at Microsoft. Like Molly I’m glad this issue has been raised now, rather than simply revealed at launch. I just hope the reason for that was to get the sort of high quality feedback we’re seeing in some areas of the community in order to test the waters and come to a consensus.
If you work on the web you’ll probably have already seen or noted the existence of the latest issue of A List Apart:
My twitter feed positively exploded with negative feedback on this issue but I’ve only just got round to reading (and re-reading) the various interesting articles and (some half through through) comments. I though I’d have a think through who this affects.
What we’re disucssing is either adding a meta element to your documents or alternatively sending the relevant HTTP header. The code for that looks like:
Users of Internet Explorer get a bum deal here I think. Their browser is going to get bigger – potentially a lot bigger. With all those rendering engines rolled into one it’s also likely to need lots of memory. How this affects the mobile version of IE is anyones guess. Memory and file size are even more important here.
I have a feeling they will all go Meh and move along as if nothing happened. Certainly my experience of upgrades to Firefox, Safari and Opera (including recent bleeding edge versions) doesn’t break any of my bits of the web or the bits I frequent.
A really cool feature for us designers and developers has gone unmentioned I feel. We’ve been asking Microsoft for one application in which we can test all their browsers in. IE8 will be that browser. Think about it. By changing the contents of the meta element (or the header) we can trigger the rendering engine to use IE6, IE7 or IE8.
I’m a big fan of the whole progressive enhancement approach, using new browser features that not everyone supports (generated content, CSS3 selectors, etc.) in a way that they don’t affect older browsers. If I were to use a feature not yet supported by IE7 but expected in IE8 for instance in this way I would expect it work in IE8. If I include an IE7 X-UA-Compatible declaration this won’t happen. Which is why I probably won’t include one. The problem is if I don’t include it at all I still have the same problem. I’ll come back to the solution (and the problem with the solution) in a moment.
Not everyone is using Web Standards, and even less get progressive enhancement. Most of the web is broken and we are not going to fix this. Virtually no one thinks XHTML2 is a good idea because it starts with this fact and says “Let’s start again, but get it right this time”. The fact that all those websites (and more importantly for Microsoft the users of those websites) standardise on IE7 is fine by me. IE7 isn’t that bad remember.
So, all in all I don’t see a major problem for me, or other savvy web designers and developers. I have one issue; namely the strong language around the use of edge.
What this is describing is how things work now. It’s also how I want to work – using progressive enhancement to make my websites better when people have browsers that support them. I’ll be adding the relevant edge headers to my sites by default as I get a moment to tinker.
In my view this should allow the web to move forward faster with newer features hitting IE sooner, leaving a ghetto populated by IE users who don’t upgrade quickly and designers and developers who don’t want to be professional about their craft (or tools that aim for the lowest common denominator). Yes, I’ll need to add a header to all of my sites in the future but in return I get the latest version of IE that lets me test all previous versions of IE.
After a break over Christmas the web world is back in full swing with a few upcoming events.
BarCamp Scotland – February 2nd.
I only just found out about this one but it looks like a few people from Refresh Newcastle and Refresh Edinburgh are going to make it along. The short notice does mean I have to think of something to talk about sharpish though.
Think and a Drink – February 7th
Bit of shameless self promotion here. I’m speaking at the monthly Codeworks get together on something to do with the Web. Also speaking is Paul Downey from BT and another regular BarCamp attendee. If you’re around Newcastle try and get along (it’s a membership event but let me know if you want to go along and I’ll speak with the organisers).
Semantic Camp – February 16th – 17th
A whole BarCamp style event brought to you by tommorris. This looks likely to be particularly interesting event I think.
Future of Web Design – April 17th – 18th
The Future of… rolls into town again, this time with a focus on web design. I’d be tempted to go along to this solely for the Photoshop tennis doubles match with Andy Clarke commentating. The rest of the content looks pretty good to.
Thinking Digital – May 21st – 23rd
Another event I’m involved in, this time on the board along with Ian Forrester from The BBC and Mike Butcher from Techcrunch. This one is a little different to most of the events I go along to – being less web centric and more about all sorts of interesting ideas. With the likes of Tara Hunt, Sean Phelan (MultiMap), Matt Locke and Dan Lyons (a.k.a. Fake Steve Jobs) speaking it should be good fun. Oh, and did I mention it’s in Newcastle?
And look, only two of those are in London!
It’s already looking like 2008 will be even more jam packed than last year. I also have a sneaking feeling that their will be lots more events Up North this year than last. It will be interesting to see how this works out. Will more people come along that might not have travelled down to London? Will the event happy northerners still go to London as well? Will anyone from London travel north of Watford? Time will tell.
I consider myself something of a hybrid, flirting with both the design and development side of things. I also like to think of myself as something of a student of the discipline; reading and consuming as much knowledge as possible on the process as well as the practice. Something I took to pondering while reading My Job Went to India was why the majority of theory books tend to be from the software side?
It’s appears the same when it comes to discussions of process, and in turn innovation within process. Their is lots of talk around Agile processes and practices amongst software developers, but I don’t see lots of designers discussing the how. User Centred Design is maybe a slight exception, but again it seems to be the software people and information architects, as apposed to those who consider themselves designers, who talk about this. I’m just wondering whether others have found this too?
The Design Council published a pretty good resource About Design but I haven’t seen much other real literature on a modern design process. Communicating Design is a fantastic book on design documentation, but doesn’t frame this in terms of a discussion of process. If anyone knows of any books or blogs out their on the subject I’d love to know about them. I’d love to be proved wrong and end up with a good reading list.
My Job Went to India (which is excellent by the way) contains 52 hints and tips for your career as a programmer. In reality most of these ideas apply to any creative discipline, only the examples and analogies need changing. You could probably argue the same for books like The Pragmatic Programmer, Practices of an Agile Developer and Peopleware. I’m also thinking whether or things like the mythical man month apply? Or if other truths of the design practice exist?
The importance of design in any discipline is becoming increasingly well recognised. But where will the next generation of managers learn about how to manage designers and the design process? One of the problems with front end web development and design work has been the difficulty of learning the craft. Is this the same for managing the web design process?
So in case you hadn’t guessed project automation is the new black. I’ve been getting back into some development work recently with Is it Birthday? and getjobsin and trying to automate as much of the repetitive and boring work as possible.
I’m not absolutely sure that that many people outside large or particularly organised teams realise that large web sites are not generally deployed by someone with an FTP client and crossed fingers. This sort of effort, along with other repetitive tasks like running tests or generating documentation, can be automated. This is a win-win for everyone. It’s more reliable (scripts don’t forget to restart a service before they go home), quicker and removes the need for having one person in charge of deployments.
Even where people know about this build process idea they might not use it for their projects, probably for similar reasons why not everyone uses source control. I think the reason is probably that most web designers and developers (even particularly geeky ones) thing this software engineering stuff is maybe a step too far. Their is also something of a barrier to entry, knowing where to look and how to get started without reading lots of documentation (often filled with XML examples) and trial and error. Also project automation is apparently not sexy?
Anyway, if you’ve been working with Rails you will have come across Rake, which is a build tool used to automate various tasks. Well the nice symfony people have written a PHP version called Pake for use as part of the framework. It’s used for all the command line action, from running tests to clearing the cache and automating deployments. Pake is however a separate tool that you can use in your own projects, whatever framework or hand rolled codebase you are using.
Pake can be downloaded using PEAR on the command line:
The documentation for Pake is pretty much non-existent as far as I can tell, but it is a really handy tool so worth a little effort. The best source of knowledge is to look through the default Pake tasks that are provided as part of symfony. One of my favourites, which we’ll look at now, allows for incremental deployments via Rsync over SSH. I’ve been using this with non-symfony projects too.
Rsync is a command line tool for syncronising two file structures. The Rsync command that does most of the heavy lifting for the deployment looks like the following. Note I’ve used {} to denote placeholders in the following examples.
The sync task I’m using is straight from symfony, but the licence allows for distribution so here is an example zip of all the files needed to follow along. You’ll need these to follow along as I haven’t printed the full sourcecode for the pakefile here.
First a little configuration. Using YAML we define an environment, staging in this case_ and specify a host, port, user and the full path on the remote server. You can of course specify multiple environments in this file, we’ll see how to use them shortly.
You can also include an rsync_exclude.txt or an rsync_include.txt file. This gives you control over the files being synced when you run the Pake task. The following example is a good starting point, it stops you pushing those pesky .DS_Store files that OSX creates to you web server, as well as avoiding subversion metadata and the configuration files for the Pake task.
We can now run the following command, from the directory containing the pakefile.php script, using Pake. The first example will do a dry run, showing you what will happen. You’ll be prompted for your SSH password as part of the command unless you’re using keys for authentication.
When you’re happy you can run the sync command with the go option which will instruct Rsync to do it’s thing.
Pake has a handy flag to find out what tasks are available.
This should give you a list of tasks and a brief description, useful to find out what you can do if you’ve been away from a project for a while.
This is a pretty simple example but one I’m already finding useful. Rsync is but one way of deploying apps but with Pake has the advantage of being simple and in lots of situations good enough. It’s certainly better than a manual deployment process. It would be simple enough to build into the task a simple logging system so you have a log of all deployments; when they happened and who did them for instance.
If that has whet your appetite then their are other deployment tools you might want to look into; Capistrano (Ruby), Ant (Java), Maven (Java) and Phing (PHP) spring to mind. If anyone knows of a Python equivalent that would be useful too? I’m also using Phing for a few tasks on projects at the moment, mainly for some nifty Subversion tasks (and you can use Phing with Pake as symfony does), but that will have to wait until another post.
So, what are peoples experiences of build tools? Any good pointers? Or maybe reasons why you don’t use them in your projects?
It turns out that their are now nearly 40 Refresh groups scattered around and about. Here’s a quick map of them all. It’s pretty interesting how all these groups have picked up on a simple word.

For anyone in the Newcastle are we’re having a meetup next week, on Thursday evening, as it happens, the details are on Upcoming if you fancy coming along.
This being a blog and it being the start of a new year (2008 for those not following closely) I feel obliged to list of few predictions of sorts. Rather than concrete this will happen things here are a few bits and pieces I think will be interesting over the next year.
A few people have already waxed lyrical about Site Specific Browsers and after trying out “Prism”:Prism for a few weeks I can say I really like the idea. Their are a couple of teething problems, both with the idea and the current implementations, but all seem solvable with a little focus. Security (sandbox everything), memory footprint (as small as possible) and integration with the desktop (local storage, Google Gears?).
I think their are already a couple of extreme examples of this idea in the wild, both of which I’m already pretty interested in. Joost are doing some pretty cool things with their bespoke application and web site backend and Songbird just hit version 0.4 and is looking pretty nice.
We’ll see more and more people playing with mobile apps next year I’m thinking. Mobile browsers (special mentions to Opera Mini and Mobile Safari here) are getting better and better and the iPhone (at least in the UK) has hopefully started something with regards fixed cost unlimited data. With some good materials already in print, next years Future of Mobile likely to be bigger and better than before and half the web developers I know pondering how they get to work with mobile I see more cool stuff happening here.
Chris has made a call to arms
 (of sorts) and I’m hoping this comes to pass. It’s becoming more and more apparent that the best teams are agile and multi-disciplinary and both sides of this divide have lots to gain from each other. For starters most front end people could probably do with learning some good old fashioned software engineering stuff; build tools, source control, deployment, testing, etc. As for the backend guys; how about unobtrusive applications, accessibility and the importance of good quality markup? Apologies for the huge generalisations here, but hopefully you know what I’m getting at.
Simon Wardley and others have been talking about software systems as infrastructure for a good while and I think we’ve seen glimpses of this in the last year. Whether it’s people using Twitter for interesting projects (Foamee for instance) or Amazon and their web service platform (S3, EC2, SimpleDB, DevPay) we have a start. I think we’ll see lots more of this sort of drop in web based software over the next year. The key seems to be thinking about things in a RESTful manner, and throwing in as much publicly exposed JSON, RSS and Microformats as possible. It will also be interesting to see if the Atom Publishing Protocol takes off next year too.
Education is coming up more and more in conversation between people involved in web design and development it seams, both off and online. I had a great conversation with Rachel and Mike after @media and then another good discussion with Dan Dixon, Andy, Tom and others at barcamp and know other smart people are thinking about this too. I’m hoping to get something off the group here in the next month or so but would love to know about any other industry driven initiatives anyone might know about?
Ok, so that’s more a list of things I think I’ll end up focusing on (as much as I ever focus on anything) over the next year. We’ll see how that goes and if other things crop up that are more interesting. What about you? Anything you think is going to be play worthy next year?
Did you love the simplicity of Is it Christmas? but feel left out, wondering why Christmas should be more important than, say, you? Well here’s your answer; Is it Birthday?

I mentioned previously that I’d been playing with the symfony framework on a couple of projects and I have to admit to being more and more impressed as I wade in looking for things. I’ve worked with enough MVC-like frameworks to know they are all quite similar in more ways than they are dramatically different. So it’s the little bits and pieces that win you over.
One of my favourite little bits in symfony has to be the Web Debug Toolbar. When running in development mode you get a little menu floating above your application like so (top right hand corner).

This gives you all sorts of useful information as you’re developing your application – from the number of database queries to the rendering time, with a full logger built in. It’s also been particularly useful in learning symfony – being able to follow the entire request through the stack from the logger is really handy when you don’t know what is going on.

With the increasing use of object relational mapping in web frameworks it’s quite easy to end up with a working application on your development box that turns out to absolutely hammer your database when used in the real world by only a few hundred people (I’m looking at you tagging). The database utilities, showing the number of queries and the SQL for those queries, is particularly handy for finding bottlenecks here.

I’ve not come across something similar in other frameworks, PHP or otherwise. I’d be interested to know if similar things do exist for Rails or Django in particular.
I like small. I have a macbook rather than a macbook pro because it’s smaller. Especially now I’m wandering about more often with work (and play) lugging a machine around is less and less appealing. So, armed with that excuse I felt justified in going out and buying a new Asus Eee PC. I managed to just walk in to Micro Anvika in Newcastle and buy one from a bemused staff member who  was pretty shocked to find one in stock and not allocated (sorry Rey).
For anyone who hasn’t seen one before, the Eee PC is a tiny linux laptop a little like the One Laptop Per Child machine but aimed more at traditional computer users (at the moment read early adopting geeks). The distro is setup to be pretty user friendly for those pesky Windows users but a little hacking turns it into a pretty darn nice machine.

And yes, I have stuck an apple sticker on back. I’m sure that should get some entertained looks next time I’m at a relevant event! I think the machine itself is actually pretty good looking, but the default desktop themes are all pretty uninspiring.
I’ve stuck with the appliance like IceWM window manager and the launcher application as it makes starting things up from the keyboard with one hand pretty fast. I then jumped into various hidden away files to get the desktop setup how I like. The wiki over on eee user has a number of useful tutorials; customising easy mode and the debian repo from a ruby tutorial were particularly handy.

Still lots more I could install (this is Linux after all) but so far have installed Ruby, Rails, Python, Django, Erlang, MySQL, SQLite, Apache2, PHP5, PEAR and symfony all without a hitch. I do love apt when it just works. I’ll try and get Mono and Java on their as well just for completeness when I get a moment.
The only thing I can think of that I’ll really miss at the moment is Keynote. It has a VGA out port (which not even the macbook has) which will support a decent resolution for presentations. But Keynote is so pretty and easy to use compared to the alternatives. We’ll have to see next time I have to do a presentation.
I can see lots of places (past the obsessive tech geeks like me) where a machine like this will be really interesting. Obviously the low cost of the machine means that schools might just get interested in Linux. For people who prefer a desktop machine as their main computer and only occasionally need a laptop it’s a pretty cost effective option. As a machine to experiment with Linux it’s pretty useful too – I’d recommend being familiar with basic commands on Linux to anyone doing web development work for instance. With so many possible audiences, and what is already a pretty nice machine, Asus might be onto something here.
For those not following along closely I thought it worth highlighting some interesting goings on around the web. Andy Clarke put the CSS cat amongst the W3C pigeons with a couple of posts; CSS Unworking Group and a following up CSSWG Proposals
How I’ve had a few ideas myself previously around what I’d like to see from the CSS3 working group. I’m now avidly following along with the comments and I’d urge anyone working even on an occasional basis with CSS to do the same.
Maciej Stachowiak said a few very interesting things in the comments I thought worth commenting on:
I think their-in lies an opportunity. The weight of comments on this issue, and the heat of the argument, would suggest people are interested. But I think a-lot of those interested parties are those working day-in with CSS (rather than on it) and lack this critical information. As a web designer how do I find this information out? Who are the interesting bloggers in the area of standardisation? Is their are good reference site? Or a wiki? Or an active friendly mailing list of people willing to answer simple questions? The web (standards) design community is used to open discussion and debate – whether from blogs or projects like microformats.
You could of course also argue that being an expert implementer is not necessarily enough to know what needs extending. A key part of extending a technology is knowing what is needed first in the real world.
I think this depends on the software in question – what about critical military systems, satellites or space shuttles, power stations, etc. – but I agree in principle. Most web designers do not have the experience of working on this sort of software project. But then Andy is not putting himself up as a project management guru.
Their are obviously strong feelings on both sides of this debate. But something I would be interested to hear – from someone with a standards focus – is whether you think their is a fundamental problem at the moment with regards CSS? If not then why not? If so then what are your ideas for moving forward?
And feel free to say you don’t know yet. I think what Andy has done is put down his ideas. If you don’t like them then suggest better ones. If that’s sticking to the status quo then say why you it feel will work out.
(I’ve posted the above in the comments to the original post, but wanted to repeat it here more in the hope of getting more interested people involved in that thread.)
I don’t really expect Andy to have the answers to this issue, but by him kicking the discussion off in such a public way the least we can all do is have the debate we need and come up with the solutions for everyone. Especially on the  list of getting started with standards links and reading materials alluded to above, if someone has this information but not the time to get it online in a coherent fashion, or to present it to the wider community then please get in touch.
As a big big fan of all things HTTP, the new Erlang based database, CouchDB, piqued my interest. With the recent release of 0.7 it’s now intended for widespread use. Now I’m a fan of databases as long as I don’t have to go too near them. SQL, triggers and stored procedures are all a little too close to magic for me.
The reason CouchDB looks particularly good fun was it’s build around a REST API using JSON for data transport. I’m generally not an acronym guy, but REST, JSON, API and HTTP in one open source piece of code? Sounds like fun to me.
The CouchDB community have built up an already excellent wiki with all the information you need to get started and get the software installed. I just used the magic of MacPorts but the page covers all the various dependencies and setups (though nothing on their yet about the N800).
You might then need to set-up a user and some permissions. Again the wiki has more detailed installation instructions. All being well you should then be able to fire up CouchDB:
So first things first. Their are getting started code examples on the wiki in all the usual languages so you can just dive in. CouchDB also comes with a few in-build tools which are both pretty attractive and useful. You have a database browser, a full test suite and a command shell.
Curl, for those not familiar with it, is a fantastic command line utility for throwing around HTTP requests. I already use it for testing and generally prodding sites but it’s absolutely perfect for demonstrating CouchDB. Let’s start with creating a database called test:
That should have created the database. Let’s get some information back about the database:
Everything should be returning JSON strings telling us something about what it finds. Let’s add a blank document to the database:
Note the blank JSON object {} passed as Post data. What about retrieving a list of document from the database?
We’re now done with that database so let’s delete it.
I’ve just done the basics here, but the APIs are simple and nicely documented enough for you easily to find out more. This is the beauty of RESTfully designed APIs, they are just about self documenting. For the most part you just have a predefined HTTP method and a defined URL.
The Erlang base of CouchDB makes for some interesting horizontal scaling possibilities (as well as a good excuse to play with Erlang.) Even if CouchDB wasn’t cool enough already, it comes with probably the best start-up message I’ve seen in software for ages:
As well as the eclectic ramblings on this blog I’ve taken to writing longer bits and pieces for some other nice folk, two of which went up over the last few days.
“Drew” and Brian are doing a great job of gathering together some really good articles in a silly time-scale over on 24ways.org at the moment. My little ditty on minification was day 6.
Chris Mills, in his new position as Opera mascot, has also rapidly started to build up a great collection of articles over on dev.opera. My contribution to what is looking like a great new resource is a piece on using JSON for configuration in your Javascript applications.
Writing for other people is different to just posting interesting stuff on your blog. And it wouldn’t work at all without some pretty dedicated people editing, compiling and chasing to get us all those high quality articles for sites such as alistapart, Digital Web and Sitepoint. So if you meet any of these people on your travels then pat them on the back and buy them a beer. Without the information sharing community we probably wouldn’t be doing all this for a living.
I’ve talked about having fun with jQuery using Jash and Firebug before after seeing Simon throw the Google homepage around at barcamp. I’m no more a one javascript framework person than a one programming language person and recently I’ve seen cool things I want to play with in lots of the main contenders.
I’ve put together a couple of bookmarklets which load YUI or Dojo from their respective content delivery networks and insert them into your current browser context. You can then play around with them in Jash or Firebug or any other Javascript console.
Dojo is particularly interesting here due to it’s cunning loading mechanism for additional parts of the framework. Basically you can gain access to the entire (huge) framework simply by using the bookmarklet and then dynamically loading the rest via the command line.
Insert Dojo
Insert YUI
The code simply inserts a script element into the page which loads the relevant framework. I’ve shown the code on multiple lines for the sake of the example, but your bookmarklet should be on one line. Alternatively you can drag the Insert YUI or Insert Dojo links to your bookmark bar.
I have a new toy I’m quite enamoured with. After a double helping of jQuery first from Jon Resig at @media Ajax and then from Simon Willison at barcamp I just had to take a look. My favourite part was probably Simon showing off his Insert jQuery bookmarklet – hacking around with the Google home page design on the firebug command line. The idea of having a command line to play around with any web page I come across appeals to my geekier side.
I spend more time in Safari than Firefox these days mainly as I find it feels snappier and is generally more stable. The only real downside is the lag of Firebug. A quick look at Firebug Lite revealed it doesn’t work in Safari but a little searching later I found Jash. The Insert jQuery and Insert Jash bookmarklets now have pride of place in my bookmark bar.

So what to do with my new toy? Let’s hide all the paragraphs! (for no other reasons than we can).
So, redesign any web page you visit via a quick to access command line? What’s not to like?
The resent barcamplondon3 was huge fun and particularly interesting. I might even go as far as saying it was the best yet in terms of the sessions I attended.
Particular highlights for me included discussions of (Yahoo! flavoured) web development practices and processes from Norm and Mike, Matt Biddulph on messaging, erlang a event driven programming techniques and Dan Dixon’s panel on What should we be teaching the next gen of web designers/devs?
I tried to kick off a debate regarding automation and software tools and practices in web projects. More from anecdotal evidence and experience rather than pure research I don’t think that many companies or individuals really set their projects up on a solid base. Source control, bug tracking, scripted builds, automated quality testing, etc. aren’t that common for smaller or medium web projects.
In the end I talked though everything in general terms and the audience nicely jumped in with various platform specific toolsets for unit testing, functional testing and performance measurement. All in all good fun and a subject I think will see some activity next year.
Overall a good smattering of my favourite topics at the moment (education, development process, distributed messaging), lots of fun people and the chance to ride round Google on a segway at about two in the morning. Roll on the next barcamp.
Alex from Twitter just got round to adding the ability to export your entire archive of tweets via the API. A few people on the mailing list had been asking for this for a while so good to see it get released.
I couldn’t resist knocking together a very quick and simple Python script to go off and get all your tweets, presented here for anyone else to play around with. Note that simple, fast and works on my machine were watchwords here. Don’t expect fancy parameters, much error handling or artificial intelligence.
As a wandering web designer cum developer and now occasional consultant I’m generally pretty technology agnostic. At some point or another I’ve written HTML, CSS, Javascript, PHP, C#, Python and Rails for a living – often on the same day. But something has me thinking and I thought I’d see what other people thought as well.
That thing is Java. I have books on Java. I’ve done the usual examples (hello world in 20 lines for instance). I didn’t really like it and left it at that. But a few presentations, both from Google, at both Future of Mobile and @media Ajax got me thinking. First you have Android in the mobile space which Dave Burke did a good job of making look pretty interesting. Then you had the guys from Ajaxian talking about using Java for everything.
A few other thinks to throw into the pot. Beyond Java is an interesting read – summing up with a call for more fragmentation and for small nimble languages to be used where it makes sense. Oh, and I think Tom is busy learning Java as well. Mmmmm.
Some good coverage around of both conferences and I know my first impression was basically this way lies madness. But it would have been more interesting had their been a few more Java people around to argue with.
Note that I’m not advocating a Google like disregard for the web here. Markup really matters, I happen to disagree with Douglas and think CSS rocks and I love writing Javascript. But should we just learn Java anyhow and try to subvert some of the existing tools to be more web like?
I spent yesterday afternoon over with Joost in London as part of their UK developer day. The event was mainly for them to show off their new widget platform and to get some input from prospective developers about what they would like to see next. About 20 people or so came along from the BBC, Ebay and a couple of london agencies, with me and Tom loitering around the edges. The afternoon was split into a series of short talks from people on the team. What follows is some of the interesting bits.
Joost describe themselves as The Best of the Internet and best of TV, if you haven’t seen it yet it’s basically TV running on the Mozilla platform. All in all pretty impressive. Dan Brickley had a different slant, seeing Joost as Web 2.0 +/- 1 Built and extended with modern web technology, modern web attitude. The idea of taking an unashamedly desktop piece of software and learning lessons from the web is pretty interesting.
The Joost development site is now public, although not linked to from the main site as yet. Their is some documentation of the various APIs but we can expect more in the future, including some videos from the developer days in a development channel.
The main emphasis for developers at this moment was on overlays and widgets (little bundles of web technology) which integrate with the Joost client. We got a sneak peak (and a chat with some of the developers) of the first commercial Joost widget for Coca Cola – Coke Bubbles and Libby Miller and Jim Ley showed off a few of the default widgets.
The most interesting bits from my point of view were to do with the technology. Joost has an interesting advantage as a platform being a single, dedicated client. No more cross browser testing here. They also keep  pretty close to trunk for Mozilla, which gives them quite a bit of CSS3 support too. You can also try out other new technologies not available in too many browsers yet: SVG, RDF, Canvas and E4X for example. This was the second time in a few days I’d seen SVG pushed as cool, the first being at Future of Mobile which I’ll jot down some thoughts on later.
What was interesting was that the people along from the development team came from a W3C background rather than say an product development or design companies. Joost makes extensive use of Semantic Web technologies under the hood and exposes quite a bit of RDFa if you go looking for it. It also follows along with various other web-like ideas, unique URLs for channels and programmes, including the ability to pass these parameters for instance to deep link into a video at a current time. The widget platform as well is starting to align with standards work going on at the W3C – we saw a quick demo of a widget build for the Opera browser which worked seamlessly with Joost too. The development team are now working on dealing with signed widgets, local storage APIs, interwidget communication, as well as usable, clear privacy models. All pretty interesting.
Some of the event was hampered by a shoddy internet connection but the general mood was relaxed and friendly and no one seemed to mind too much. The beer probably helped. I did chuckle when a senior alternative marketing guy from Coke described as Insane the typical agency setup though (rich guy in Notting Hill → account manager → project manager → developers).
Their will be a Coke sponsored widget competition announced after the rest of the developer days (in Amsterdam and New York) with the title of International widgets champion of Joost up for grabs along with some cash and a t-shirt.
My only real complaint was probably the low level mutterings about Javascript! Audience members and Joost developers alike made a couple of digs during the course of the day. Javascript rocks, people. Oh well, I’m back in London for @media Ajax where I don’t think I’ll have that problem.
If any of this sounds interesting (or you just want to watch more TV) I’d recommend having a look through the development site, jumping on the joost-dev freenode IRC channel, signing up to the mailing list or reading the development teams blog.
Sorry, shameless what’s going on post for historical posterity.
Before that starts thought here are a couple of useful microformatted schedules for Future of Mobile and @media Ajax created by the really rather handy Conference Schedule Creator. Hat tip to Jeremy and Brian for the lovely styles.
I’m spending the next few weeks circling London, first up attending Future of Mobile, on to Pubstandards and then popping in to see Joost. After a short break I’ll be back for @media Ajax and BarCamp London. Throw in a few business meetings and we’re all set. I even have new Moo cards.

If you’re around at any of these events, or just happen to be in London, and are one of the two people who read such self serving ramblings as this let me know and we can drink beer. Actually scratch that. Due to the wonder of Dopplr I already know where you’re going to be.
Earlier in the year I set aside some time to get my head into the mobile web. Most of this was reading and tinkering. Since then I’ve pointed a number of people (including clients and people in the pub) towards some or all of the following links. A good starting point for anyone starting to get interested in the mobile web.
A good starting place is Mobile Web Design by Cameron Moll which is available as a PDF, and now as a hard copy from lulu. Definitely worth your while. If you’re not convinced then you can read the original series of articles to get started.
The Mobile Web Best Practices 1.0 from the W3C Mobile Web Initiative and Global Authoring Practices for the Mobile Web are more good starting points, this time with more of an implementation focus. Blue Flavor also have an intersting presentation you can download on the subject of Designing for Mobile
Once you have a basic understanding, or if you just want the facts then check out The Wireless FAQ. Everything from What is WAP? up to Nokia S60 WAP browser cursor skip some links on my WAP site! What is wrong? is in there somewhere.
If you’re too busy to be reading everything written about mobile, or just prefer listening to the voice of Paul Boag, then have a listen to Boagworld episode 96. Heidi Pollock at the recent Future of Web Apps was also excellent, especially if you let your numbers. You can get the presentation and audio of the talk now.
If you have still have questions after reading a few good articles or books then their are a couple of high quality mailing lists, namely wmlprogramming and mobiledesign. It’s worth being subscribed to both, thought their is quite a bit of audience cross over.
The .mobi site also has lots of useful information regarding mobile. Whether you buy into the need for a new top level domain or not it’s worth reading.
Lots of reading to get you started and not one mentioned of the iPhone…
Have any other good links for getting started in the mobile web? Leave a comment with further suggestions.
The state of CSS has been a common topic of conversation this year. Ever since Andy Budd stirred things up at The Highland Fling about The Future of CSS and followed up with a call for CSS 2.2 we’ve been wanting more.
Well, in response to all this the CSS3 working group have released CSS Snapshot 2007 as a working draft. CSS3, in theory, has a modular structure. The idea behind this was that individual pieces could be worked up, specified and released without having to work on the whole thing at once. Sounds a lot like iterative development to me. The problem is we haven’t, until now, had a stable release one of CSS3.
So (at least when this document reaches Candidate Recommendation at some unknown point in the future), we can get on and use all those selectors we keep eyeing up. Colors and Namespaces are not particularly interesting, at least to me, but useful non-the-less. CSS2.1 is important as a baseline going forward, especially for the browser makers, rather than being particular interesting for the jobbing web designer.
The problems come when we look at some of the features we want, and the modules they are part of. Andy specifically references text-shadow as something that’s already got more than two sample implementations but is part of the Text module which we won’t see for a while due to it’s complexity. (As a side note text-shadow was formerly part of CSS2.1 but got dropped due to lack of implementations at the time.) Dave Storey summed this problem up nicely over on CSS3.info
From a quick look at the current work of the CSS working group I count six modules at candidate recommendations, six at last call, twenty three at working draft and six not yet started. If modules really are discrete blocks then this to me looks like a recipe for disaster. If modules aren’t discrete blocks then why are they divided into modules? Rather than focus on shipping modules it appears that the effort of the working group has been spread out across nearly the whole spectrum of CSS3. This pretty much invalidates the idea of a modular approach.
CSS3 is a big project. And specification, like software, is hard. In well organised and managed agile software teams it’s common to split things down into features, or groups of features (modules). The CSS working group did this. In a well managed agile team shipping something early is hugely important. Everyone focuses on that goal, rather than on their favorite part of the project that might not be needed until later on. The CSS working group definitely didn’t do that. But it’s what I think is needed now.
The working group are now starting to talk more openly about what they are up to. And the CSS Eleven are looking to add some support from a web design industry point of view. I’m interested to see what they come up with as suggestions and support. The problem I see is that, unless the working group back down on the strict nature of the modules and define sub-modules or simply go to specifying features then we could hit an impasse. The best course of action (from a complete outsider anyhow) would be to get all the people from the working group and from the CSS Eleven working on one module at a time. Only once a little momentum has been built up maybe they could work on more than one at once, just not thirty five this time. All of that would take some hefty project management and some setting aside personal and organisational politics to get done. Fingers crossed.
I actually think things are looking up, but time is certainly a factor here. Lets assume IE.next is released late next year or the start of 2009 (wild speculation). If the Internet Explorer team implement to the current specs at that time – that could easily not include CSS Snapshot 2008, which would be reaching candidate recommendation around about the same time as they ship. Which could in theory mean it’s four years or more before we see text-shadow well supported. Never mind elements of CSS3 which currently have no reference implementations at all! Anyone else with any bright idea? Or insider info that you want to share?
Fab. It’s that time of year again. Time to start thinking “wouldn’t it be cool if” when it comes to the BBC (even if it’s not your day job).
I had the fortune of going along to this years BBC Innovation labs back in March and they’re back on the road again next year, with applications open from the 1st of December.
The basic concept is simple. Through a simple but competitive process of ideas submission the BBC find a handful of companies to dump in nice settings in the middle of nowhere for a week to work on their ideas. They are running four labs this year; in Wales and West Midlands, North East England, North West England and Scotland. If you live in any of these areas I’d strongly recommend finding out more at one of the local events.
Been couped up in a stately home or nice hotel with other odd nice people is great fun. It’s all pretty intense and last years event really got me thinking. It’s also a pretty good business opportunity. You own your ideas whether you get on the labs or not, you get paid for your time attending them and you get to pitch your ideas to the commisioners at the end of it.
For an idea of what goes on, you spend alot of time doing thinks like this:

And if you get really into it you might end up strutting around like this:

It’s not all fun and games though. You might have to spend your time watching BBC employees play snooker on work time or playing the odd game of werewolf:

The briefs are already up that will form the core of the submission process in order to get on one of the labs. I’d recommend having a good read, picking a few you think are interesting and tie in with whatever your interests are at the moment and go from their.
With all that in mind I’m looking for people who fancy partnering up to brainstorm ideas, put together a few submissions and see if we get anywhere. I’m now working full time on freelance and consultancy gigs but the nature of the labs was that you need a team of at least two people. And anyway, ideas flow more freely with a few smart people sat around. So, if anyone fancies sitting around talking through a few of the briefs (and who knows, maybe drinking a glass of wine?) at some point over the next few months before the deadline let me know.
Performance used to be something other people thought about. If you were working on a high traffic site for a large company, chances are they would throw inordinate amounts of expensive hardware at the problem. If you had a personal site only if you got really popular would you need more than a shared host. But the number of web applications being launched by small companies or individuals from their bedrooms is raising the awareness of the importance of performant websites.
Credit to YAHOO! here. Google and Amazon might be the first companies that spring to mine when it comes to thinking about infrastructure but YAHOO! have been publishing lots of useful, practical and interesting information in this area. Steve Souders, the Chief Performance YAHOO! has a new book going into detail about YAHOO!s performance rules. The recent release of the YSlow extension and another upcoming book from Stuart Colville and Ed Eliot cap things off nicely.
According to YAHOO!
While all that has been going on I’ve been working with a few people on a Rails project and come across a great plugin; asset_packager from Scott Becker. Inspired by the Vitamin article by Cal Henderson (of Flickr and another good book fame), asset_packager compresses and merges CSS and Javascript based on a prescribed configuration.
I’m also working on a few PHP projects and wanted the same experience in these as well, so  I’ve gone a put together a quick release of php asset packager. At the moment this is a CSS only release, with Javascript features coming later, along with all the rest of the features of the Rails plugin and anything else I can think of or need.
First off you define your stylesheet setup. I used YAML as my goal was to stick as close as possible to the Rails plugin.
<pre>stylesheets:
- base:
  - screen
  - header
- secondary:
  - foo
  - bar</pre>
The above simply states that you want to merge screen and header CSS files together to form a base stylesheet, and merge foo and bar together to create a secondary CSS file.
All you then have to do is include the following:
<pre>// define("ENVIRONMENT", "production");
echo Asset_packager::stylesheet_link_merged();
If the ENVIRONMENT constant is set to production the line includes the merged stylesheet links, otherwise each file is included seperately. This again mirrors the behaviour of the Rails plugin. A command line script is also included in the source to actually merge (and tidy) the CSS files.
The vague plan is to add the rest of the features and then maybe package up as a plugin for whatever software or frameworks I’m working with at the time. At the moment that probably means wordpress, symfony and code igniter. Everything is open source and available from svn over on the Google Code site. Contributions, in the way of code, bugs, documentation or thoughts welcome.
I’ve been busy really getting to grips with Trac recently and thought I’d post up a few details. Trac for those that haven’t come across it is a wiki, issue tracking systems and source code browser all rolled into one. It’s open source and written in Python.
I’ll start off describing my current ticket setup, along with the code I use for reports. In future posts I’ll hopefully describe thinks like setting up users and permissions in a flexible way. I’ll leave describing installation to others, mainly because it’s a pain. I’ll also assume you’ve installed the WebAdmin plugin, or are familiar with trac-admin.
By default Trac comes set up with severity, milestones, ticket types, priority and components and a few default options in each. The problem with all these options is that it takes more time and effort to add and manage issues – so they don’t get logged. Unless you know you absolutely need them I find it easier to remove most of these options in the first instance.

I prefer using Trac soley for issue tracking. It suffers badly in my opinion if used for more project management related activity. Other tools do that job better (and also do bug tracking poorly).
I keep priority in, but by default it’s blank and only has one other options – High. I only assign High priority to issues which are causing major problems, either preventing a successful build or stopping something happening that’s meant to.

Trac reports are simple (or not so simple) SQL queries with a few bits of special syntax. I have four reports set up; Open, Closed, Important and My Issues:
Open, not too suprisingly, displays all the open issues. As mixing different components here would be confusing we divide the list with headers for each components
<pre>SELECT
  component AS &#95;&#95;group&#95;&#95;,
  (CASE priority WHEN 'High' THEN '1' ELSE '' END) AS &#95;&#95;color&#95;&#95;,
  id, summary, owner,
  time AS created,
  component AS &#95;component, 
  priority AS &#95;priority,
  changetime AS &#95;changetime,
  description AS &#95;description
FROM ticket t
LEFT JOIN enum p ON p.name = t.priority AND p.type = 'priority'
WHERE status IN ('new', 'assigned', 'reopened') 
ORDER BY priority DESC, p.value, t.type, time</pre>
Closed is the reverse of Open (I’m starting to state the obvious here) but with a few differences in terms of the listing. I’m more bothered here about seeing a timeline of closed issues. Good for keeping a eye (or a feed) on to see progress.
<pre>SELECT
  (CASE priority WHEN 'High' THEN '1' ELSE '' END) AS &#95;&#95;color&#95;&#95;,
  id, summary, owner, component, resolution,
  priority AS &#95;priority, 
  changetime AS modified,
  description AS &#95;description
FROM ticket t
LEFT JOIN enum p ON p.name = t.priority AND p.type = 'priority'
WHERE status NOT IN ('new', 'assigned', 'reopened') 
ORDER BY changetime DESC</pre>
My Issues lists only those issues which are assigned to me (or the currently logged in user) and open.
<pre>SELECT
  component AS &#95;&#95;group&#95;&#95;,
  (CASE priority WHEN 'High' THEN '1' ELSE '' END) AS &#95;&#95;color&#95;&#95;,
  id, summary,
  time AS created,
  component AS &#95;component,
  priority AS &#95;priority, 
  changetime AS &#95;changetime,
  description AS &#95;description
FROM ticket t
LEFT JOIN enum p ON p.name = t.priority AND p.type = 'priority'
WHERE status IN ('new', 'assigned', 'reopened') AND owner = '$USER'
ORDER BY priority DESC, p.value, t.type, time</pre>
Important is an overview of critical issues. Software can have small bugs and be perfectly usable in the vast majority of cases. But some issues either effect lots of users or prevent the system working at all. These High priority issues need addressing first, before anything else.
<pre>SELECT
  component AS &#95;group&#95;&#95;,
  (CASE priority WHEN 'High' THEN '1' ELSE '' END) AS &#95;&#95;color&#95;&#95;,
  id, summary, owner,
  time AS created,
  component AS &#95;component,
  priority AS &#95;priority, 
  changetime AS &#95;changetime,
  description AS &#95;description
FROM ticket t
LEFT JOIN enum p ON p.name = t.priority AND p.type = 'priority'
WHERE status IN ('new', 'assigned', 'reopened') AND priority = 'High'
ORDER BY priority DESC, p.value, t.type, time</pre>
Interesting bits here include the following line which makes all High priority tasks have a red border:
<pre>(CASE priority WHEN 'High' THEN '1' ELSE '' END) AS &#95;&#95;color&#95;&#95;,</pre>
As a side note, I hadn’t written proper SQL for a while (all that hacking on APIs). Suprisingly good fun as it turns out.
The main benefit of this setup is simplicity. Too many bug tracking systems are either way too complicated, or come with defaults which are on the complex side (I include Trac in this last category). Garret Dimon has been writing about the development of a new bug tracking system with a focus on simplicity. Fixx and Lighthouse are other products looking to fill this niche. That’s not to say larger teams don’t need to store more information than this, just that by starting small you can actually find out what you’re missing – rather than guessing and making the whole process of adding and fixing bugs a particularly painful one.
Continuing the catch up, as mentioned, here’s some thoughts on the recent Future of Web Apps Expo conference.
It’s fair to say the last Future of Web Apps (FOWA) event got a little stick due to what was seen as too many paid sponsors on stage lacking interesting things to say. Some designers and developers on the coal face also felt all the talk about startups just wasn’t relevant to their day jobs. So you have to hand it to the gang at Carson Systems Carsonified for coming back with an even bigger and better event which addressed most of these concerns.
Simon and Brian came in to help with the speakers and at some point the event went for two tracks – the developer stage and the entreprenuers stage. At the same time the sponsors (supplemented by lots of smaller uk startups) moved into a large expo area outside the two stages. All this meant a move to ExCel which is a bit big, soulless and out of town for my liking but hey, how many places can deal with that many people with space to spare?
In the interests of fairness I have to say I ended up helping out, getting there early on the first day and standing behind the desk (with Cristiano and a few other nice people) for the mad rush at registration. I hadn’t worked on a large event for quite a while, I’d forgotten how much fun it can be! I don’t think that makes me biased though.
Highlights for me included Matt Mullenweg on all sorts of interesting architecture bits and pieces learned from wordpress.org, Heidi Pollock with practical tips for mobile web development, Paul Graham on startups in general (and why we all need to be in Silicon Valley apparently), the endlessly entertaining Simon Wardley on ducks (and commodisation of online services) and Matt Biddulph of Dopplr on practical tips for playing nice with social networks. Chatting with Steve Sounders and hearing about how he got the title of Chief Performace YAHOO! was also good fun. The book is on it’s way as I write.
Dave Morin from Facebook was a disappointment. He basically showed everyone facebook (which I would imagine 99% of the audience are already on), talked up some huge numbers and then dodged questions about walled gardens and open data.
I didn’t make it into the entreprenuers stage as much as I should have done. Mike Stenhouse spent most of his time in there I think and had nice things to say about most of the talks. I caught Leisa Reichelt and Dick Costolo, both of whom gave particularly interesting talks to round off the second day of the conference. Although at least some people I spoke to don’t like having more than one track (the same topic of conversation cropped up at @media) I much prefer the choice of sessions. Missing the odd clash is a small price to pay for the larger number of interesting talks.
Overall; lots of interesting content from lots of interesting speakers – many of whom I hadn’t seen speak previously (always a good sign). As for constructive criticism I’d have to say that the half hour slots of the developer track were probably a little too short in some cases and the conference was lacking in real, hands on, practical tech sessions.
One final thing that can’t go without mention; Diggnation. I had no idea what this was all about and quite frankly I’m not sure I do now. I’m still carrying the mental scars. I don’t think anyone can explain that in words so, here’s the video.
Long time no post. Lots of excuses, time away (including a holiday) and work catching up with me. Hopefully expect a few quick fire posts to catch up and then back to normal.
I was in London a couple of weeks back, mainly to help out at FOWA (which can get it’s own post) but also to generally catch up with lots of people about interesting stuff. After using my Monopoly knowledge of London to find the apple store (something some Londers took issue with) I heard about an event round the corner thanks to the magic of the internets (in this case Twitter, Upcoming and Dave Stone).
Mashup Demo was billed as “a simple and effective way for startups, growing businesses, enterprises and corporates to present themselves to an audience of professionals who would include:- investors, media journalists and bloggers, potential partners and industry influencers”. When first chatting with one of the exhibiting companies they looked me up and down and said “so, are you a blogger?”. Apparently I didn’t look much like a VC. Who knew?
Fifteen lightning presentations made up the bulk of the event, with me taking notes using my handy N800 (another blog post probably), and then followed a good amount of time to wander about and chat with everyone, probably about 100 people all in all. The companies presenting represented lots of the hot topics at the moment shall we say. Anyone not mentioning mobile, or social networking (or both!) had come to the wrong party.
Local Search had a strong showing from welovelocal.com, Rumble, and tipped.co.uk. The thorny problem of local was one of the really interesting discussion topics at the BBC innovation labs earlier in the year and it’s good to see interesting ideas of how to solve it. Although personally I don’t need another social network and also want to be able to define what I mean by local (not just be given a location, or have it based on a pool of friends, or whatever the latest looking glass is).
Identity was suprisingly popular with both Bondai and meecards talking up their applications. Obviously with all the cool kids playing with OpenId and the alpha geeks tinkering with OAuth identity is technologically interesting, but both these companies have spotted commercial opportunities as well. Be interesting to see how this all plays out. I’ve rambled on before about what I called Banks for Data (as apposed to money, which is just data anyhow when you think about it) but not really considered early movers and entrepreneurs in this sector.
A couple of interesting takes on agregators dropped in as well.
fav.or.it was pitched as trying to popularise feeds for everyone else, but looked to me to actually be more of a useful tool for the information hungry crowd. It’s innovative features (inline commenting looked nice) would likely not be that interesting to people with only a few feeds and a few minutes but for people with hundreds of feeds look interesting. Brandwatch was similar to an idea a friend and me kicked around a while back, but never put together. Simply put it finds stories relating to your brand (company, product, whatever) and ascertains the importance and whether it’s positive or negative. You can then track all that over time with useful graphs and charts and the like. At the moment they are indexing about 500,000 sites and it still uses some manual filtering I think. Even so I know of clients past who would definately find the service offered interesting.
A few others caught my eye; an ad network for bloggers based on their blog rolls called rollSense, a service simliar to slideshare but for business documents called “Edocr”:"http://www.edocr.com/ and testcard.tv. Testcard.tv does some really pretty impressive agregation, stitching and searching of online video. Sort of a glimpse of the future, but probably not as pretty. Optimising that for the Wii or PS3 (or keyboard/remote control) would be awesome.
After chatting with a few of the companies, and other people in the crowd, I popped for sushi with the aforementioned Dave for discussions of the freelance life I’ve got myself in to. The event was pretty darn interesting. That the companies presenting were, for the majority, proper startups was interesting. And the fact the whole event was geared around product centric companies and the surrounding eco-system interested me. I get to similar business events up and around newcastle, but these tend to be more service company orientated it seems. All in all Mashup Demo kicked my stay down in London off nicely.
I’ve tinkered with a rather large number of these web application frameworks, in Ruby, Python, PHP and C#, at some point over the last few years. But I’ve never really settled on any of them – mainly because my day job didn’t need me to and also because I like playing with new toys. I even tinkered with my own homage to Web.py in PHP which a few people have picked and are running with.
Right now though I’ve been doing some work with a few people on a Rails project. Their are some seriously nice things in Rails that I don’t see elsewhere; really good testing support baked in for one, and some serious deplyments for another.
Symfony is another PHP framework but one that only supports PHP5 (a good thing in my book). It’s basically a slick integration of lots of existing mature PHP tools plus some good old MVC and tried and trusted Software Patterns. I also discovered from a few YAHOO’s that they have started using it on some levels within YAHOO.
My interest piqued I found a couple of related presentations on Slideshare (my new favourite web app), and the site has a high quality online documentation which can also be purchased in dead tree format.
Having a play around with Symfony it took me a little while to get everything up and running, but I’d kept some handy notes for later. Hopefully they might be useful to someone other than me.
I’ve dropped in a few variables denoted by braces where relevant. The paths are from my MAMP and PEAR based environment and I’m using a MySQL database, though you could just as easily use Sqlite.
First we set up a virtual host in Apache. You have to do this manually at the moment. I already have a script for setting up virtual hosts under apache on OS X which I might extend to do this for me if I end up using Symfony alot.
<pre><VirtualHost 127.0.0.1>
  ServerName 
  DocumentRoot "/Applications/MAMP/htdocs/{project}/web"
  DirectoryIndex index.php
  Alias /sf /Applications/MAMP/bin/php5/lib/php/data/symfony/web/sf
  <Directory "/Applications/MAMP/bin/php5/lib/php/data/symfony/web/sf">
    AllowOverride All 
    Allow from All

<pre>cd /www
mkdir {project}
cd {project}</pre>
Now we dive into the actual symfony commands. We’ll just setting up both a frontend and a backend for this app which I think is likely to be pretty typical. I quite like the idea of seperate entities for these.
<pre>symfony init-project {project_name}
symfony init-app frontend
symfony init-module frontend {module_name}</pre>
Next we have to configure our database connection by editing /config/databases.yml
<pre>all:
    propel:
      class: sfPropelDatabase
      param:
        dsn: mysql://{username}:{password}@localhost:3306/{database_name}</pre>
That done we can set up our data definitions in /config/schema.yml. Read the documention for all the details but you can probably get the general idea.
<pre>propel:
 item:
   id:
   title: varchar(200)
   description: longvarchar
   created_at:</pre>
And yet another quick change in /config/propel.ini
<pre>propel.database.createUrl = mysql://{username}:{password}@localhost:3306
propel.database.url = mysql://{username}:{password}@localhost:3306/{database_name}</pre>
The next set of commands build our model files, create SQL for out chosen database and then create the tables in the database.
<pre>symfony propel-build-model
symfony propel-build-sql
symfony propel-insert-sql</pre>
We’ll quickly create a backend using the scaffolding. Note that item here is the model name which is taken from the schema.yml config file above.
<pre>symfony init-app backend
symfony propel-generate-crud backend items Item</pre>
All in all not bad going. And that’s basically the same for most of the apps you’re likely to develop. It could probably be a little easier in the database and apache config areas in fairness. Rails’ use of an inbuild webserver really helps here. But the use of YAML to define the model is, in my mind, more straightforward that using Ruby in Rails migrations. I’ve an app in the works at the moment and I’m looking at using Symfony. If that pans out I’ll blog more about the bits I’m interested in – the inbuilt unit testing framework high on my list.
Back from a madcap weekend in Brighton for the dconstruct BarCamp Brighton double header. If I was to write down everything I can remember we’d be here for a week. If I was to write down simply everything then we’d be here longer still. Information packed just doesn’t cover it.
I’ll likely sort some more detailed posts on some particular topics that piquet my interest in the coming weeks or months, for instance user centred design vs agile methodologies (thanks to Leisa Reichelt), some more rambling posts on web application frameworks, microformats for write APIs, OAuth and countless other things I’ve no doubt forgotten.
One thing you can get now is my BarCamp presention which I’ve uploaded to SlideShare. I’ve not used this much previously but I’m smitten and will be uploading a few other past presentions in the next few weeks as soon as I can find them. If you’re interested in RESTful architectures or Rabbits then this might float your boat:

I also took lots of photos (some with Norm’s very nice 1.4f 50mm lens, thanks Norm) whick I’ll get on Flickr when I get a moment. Expect lots of portraits of people (maybe including you?) for once instead of odd abstract architecture and textures.
Final thing I’ll say for now is a big thankyou to anyone or everyone who sorted all this out. The 600-700 geeks descending on brighton all got fed, watered/beered and educated without anything appearing to go wrong (except when the hotel bar ran out of beer – I blame Steve Marshall). I’m looking at you clearlefties and Glenn and everyone from Madgex. Good job everyone involved.
More interesting Newcastle centric news, although this time partly releveant to everyone else – well, as long as you live in Oxford, Brighton, Leeds or Manchester that this.
Natalie Downe has spent some of her copious spare time (having left the agency way of life and gone all freelance too) cooking up Geekvenues. It’s an attempt to map (using Google Maps) any and all geek related venues around the UK. Local meetup places perhaps, regular venues, or just geek friendly pubs with wifi.
I’ve been adding some venues for Newcastle and they can be found over at newcastle.geekvenues.com/. If anyone knows of any other ones let me know. And if anyone is thinking “I want it to cover my neck of the woods” then get in touch with Nat, ideally with a link to a KML file of locations. I’m looking at Alan and John from Edinburgh in particular here.

OpenCoffee Newcastle is a new, open, informal and regular meetup for technology entrepreneurs, designers, bloggers, developers, geeks, investors and anyone else whoâ€™s interested.
We are going to meet every Thursday morning, between 10am and 12pm, at The Side just off Newcaste Quayside.
The first event will be held on September 13th. So come and share ideas, demo, get to know each other or just have a coffee.
OpenCoffee Newcastle is part of the global community effort over at opencoffeeclub.org. Here’s the blog post that started it all
With all that out of the way a little bit of background and some geekier links. I’d been meaning to get an OpenCoffee event up and running for a while and after a couple of chats in nice pubs with Sam and a trip to The Side we had a date and a vague plan. Worst case is it’s me and Sam just meeting up but hopefully a few more people will make it along. We should have wireless as well for any demos.
Note I’ll likely be somewhat wired. I might be up in edinburgh the night before for FOWA roadtrip and in any case I’ll have been up all night at barcampbrighton.
You can register for the event on upcoming or even register for the group. Or you could just turn up. See you there! Any questions post a comment (either here on on upcoming) or drop me an email.
When I’m writing code, be it CSS, Javascript, Ruby, whatever, I don’t like big bulky IDE’s. Give me TextMate or Vim any day. Going all freelance I now have the occasional need to produce nice documents and being in charge means I get to decide what I use to produce them. I never really liked working in Microsoft Word so decided against it. But rather than sensibly going for an obviously alternative I decided to just use HTML, or rather more specifically, Textile.

As an old time textpattern I learned to love the simple text transformation language and it’s inclusion in the 37signals apps made me smile and more likely to use them. Why use it over HTML you may ask? For me I think about HTML like a browser, as a tree of nodes. I end up spending quite a bit of time writing content then going backwards and wrapping what I just wrote in markup. This is fine for your typical site, but for writing documents it’s hard to get into a flow. With textile I can flow in one direction.
So far I’m liking it alot. I can seperate content from presentation (Word styles are a myth), easily repurpose my content (text, html, pdf) and write documents in anything I like, including Word if I chose to.
It’s been a pretty good excuse to play with various bits of CSS3, including multi-columns, advanced layout, generated content and the various print offerings. Given I can control the rendering engine then produce a PDF to distribute I can use anything that the latest browser builds can deal with.
I’ve not got all the workflow steps automated yet, I’m using Firefox to print documents to PDF (via Preview) at the moment which works fine but is still a manual process I can get rid of using something like css2xslfo and fop. I’m also pretty sure the PDFs won’t be accessible, although the original markup certainly should be.
As I produce more complex documents I’ll likely miss things like automated table of contents generation, footnotes and page numbering but, then again, I can write that in Javascript (or on the server) if I so choose. I was impressed listening to HÃ¥kon Wium Lie at @media about the process they used to format his last book using CSS and PrinceXML. I’m half tempted to spend time rolling all this together into a web app. It would be pretty good fun really; APIs, XSLT, CSS, PDF, some nice UI problems, etc. Only problem I see would be entering a market contested by Microsoft, Google, Sun and er, lots of others.
Ben Metcalfe just posted a run down of some thoughts on conferences to attend. With all my travels this year I thought I’d join in.
I’ve spend quite a bit of time this year travelling the UK for various get togethers, conferences, meetups and the like and had great fun doing so. I go mainly because nowhere else do I find quite such a critical mass of smart interesting people to have a pint with and discuss the intricacies of the web. For 2007, in roughly chronological order: WebDD, Barcamp London, Future of Web Apps, Highland Fling, Refresh Edinburgh, @media, Hackday. I’ll be at dConstruct, Future of Web Apps (again) and possibly Future of Mobile and a Rails meetup in London as well to finish off the year.
I won’t go on about how cool Barcamp was. I actually think the whole unconference thing sits alongside the more formal conference scene quite nicely rather than acting as a direct challenge. Safe to say if you have yet to make a Barcamp then be on the lookout (although if you could not pick brighton please. I really want to go and it’s going to be hard enough to get tickets anyway!)
Highland Fling was excellent. Alan did a bang up job of picking a pretty focused topic and getting a gang of safe hands in to do the talking. Everything seemed to follow on from the previous talk, or catch a thread only touched on earlier in the day. This turned out to be down to the specific topic rather than some pre-conference collusion which I thing bodes well for other such topic centric mini-conferences. It was all rounded off (as all conferences must be) with a rousing talk from Andy Clarke and a panel chaired by Jeremy Paxman Keith.
@media is still the daddy of the UK web standards scene. Personally I want more advanced materials and I’d love to see more audience interaction, but the scale of it makes that a little difficult I acknowledge. I’ll keep going as long as I learn something new and come away feeling a little bit inspired. I’ll be going next year. Maybe it’s not going to hit the heights of the very first @media a couple of years back but that was my first trip out into the world. Oh, and patrick throws good parties in pubs with beer.
I think dConstruct is basically a grown up Highland Fling. It was great fun last year so here’s hoping it’s as much fun this year. Again, the idea of a particular topic is in place. I’m still thinking about @media Ajax if I can get enough work together to pay for it (note to people trying to keep up – I now work for myself as a freelancer) and I have a sneaking suspicion I’ll want to get along to Future of Mobile in a couple of months too.
My plan for next year is to try and get out of the country and hopefully to some of the events in the US and Europe that offer something different. SXSW is an obvious choice, but I’d be interested in any other ideas on top of Ben’s list. I’d also like to think in a year of so I might even end up talking at some nice event or other – but I need to pick a topic and stick to it I reckon instead of flitting between whatever I feel like that afternoon. Oh and probably practice.
I’m having quite a bit of fun with my rabbit, cambridge, at the moment. Their were a number of projects at Hackday involving the ever so cute Nabaztag and I had a feeling I’d probably go home and adopt one. The idea of a rabbit with APIs was simple too much fun.

The Nabaztag API docs can be found in two places (the rest of the site unfortunately has a number of issues when it comes to finding information easily); in the help section and on the API site. The latter are much better than the former by the way.
Lots of people have already been hacking on the wireless rabbit. Their are some good tools over on the Nabaztag Sourceforge project, a handy Ruby module and a prospective PEAR module for PHP.
I’ve been getting more and more into the ideas behind REST recently and one thing that disappointed me a litte was the HTTP RPC style Nabaztag API (note to anyone thinking REST simply means using a GET request rather than something like SOAP or XML-RPC, it doesn’t). I couldn’t help seeing PUTs to ears and POSTs to the mouth.
Inspired a little by Arals Bunny Talk I’ve started implementing parts of the API in a RESTful manner, and used my API abstraction layer to build a similar application.
Head over to morethanseven.net/presents/nabaztag/ and send me a message (via my rabbit obviously) or, er, play with cambridges ears (he likes that). At the moment the API is configured only for my rabbit, although you can download the source code and run it yourself if you like. It requires PHP5 at the moment as one of the required modules is PHP5 only. But you are using PHP5 now aren’t you?
If you don’t want to use the form you can access the API directly. Here are a couple of examples using curl:
Their’s a litte documentation in the source. I’ll add more methods, more documentation and maybe make an API anyone with a Nabaztag can use by authenticating either as I need it or if people actually want it. Let me know what you think (via the rabbit obviously!)
My latest job has allowed me to get back into playing with Rails, something I’ve been meaning to do for a year or so. Having got everything up to date and installed on my virtual server and on my laptop I couldn’t resist seeing if I could get it going on my N800.
The reasons for this are mainly that the idea of a wireless web server in my pocket is pretty intriguing. Think about it. If you’re at an event with Wi-Fi you can just point people to an IP address and hey presto they have access to your details, or a set of photos, or whatever you want to show them. I’ve also got nginx installed, but I’m not as yet that up on running python or PHP behind it on the N800. So, no need to carry around the laptop. OK, maybe it’s just me and all this going to web conferences? We’ll have to see if I actually knock up anything useful for the next conference and see. Saving that I’ve been travelling quite a bit recently and it’s handy when reading a Rails book to be able to try out the examples.
Anyway, it turns out that you can indeed install Ruby and Rails on the N800, although their isn’t much information around and non of it worked exactly as stated for me – hence this post.
I followed along with a pretty good tutorial, although I already had Red Pill Mode, XTerm and SSH installed. Ruby is also now available in the Garage.
The crux of the post is that you can find the required packages, in particular for Gem, here. But unfortunately that is were the problems start. I ran into exactly the same problem as described in the comments in the above post, on this blog post and on this Internet Tablet Talk forum thread. Specifically, Gem hanging, crashing or returning a segmentation fault in the yaml.rb file on line 133.
The problem appears to be (this is me guessing here) with gem and the N800 being able download all the info for Gem to update it’s package list. The solution, or rather a suitable workaround, is to download the individual gems yourself to the local file system or a memory card and then install them from there. You can get all the Rails gems here. A quick script/server later and I had the familiar Rails welcome screen on port 3000. This should in theory work for oher gems as well. You can either download them directly or use Gem on a local machine and them move the files across via bluetooth or USB.
Being busy is to be expected with this new start-up malarkey but whoa. The problem is I have even more ideas for real posts or articles (the type you research, play around with, then write properly) and, for the next few weeks, not the time to do them justice. I’ll be back with gusto when I find the time. In the interim here are a few things from the last month or so I consider cool.
More (interesting) work beckons…
Apologies up front for the bad pun but I’ve been busy playing with another little feature found in CSS3, namely the :target pseudo-class.
It’s actually pretty well described in the CSS3 Selectors module, (although given that the specs make lots of use of partial URLs the spec style sheet is crying out for use of said selector to I digress).
Put simply it allows you to style elements of a page that are the target of in page anchors, ie. &lt;a href="#target"&gt;in page link&lt;/a&gt;
For example. The following CSS hides all div children of an element with an id of example3.
The we can say “if said element has been targetted then display them all”
So if you visiting #example3 you would see the divs, otherwise they will be hidden away.
I’ve set up a couple of examples over on morethanseven.net/presents/target.
These examples basically remove the need for Javascript implementations but I’m not sure whether I’d actually use either of these. But you get the general idea. One thing about the examples was their simplicity compared with similar approaches though.
It’s already pretty well implemented according to this handy compatability table from CSS3.info and I would imagine an ideal candidate for Andy Budd’s CSS2.2.
A quick, but hopefully informative post on something that took me a little time to work out and I thought might trip a few others up too.
I’m playing with quite a bit of CSS3 at the moment for our new company site. It seemed like a good excuse to put into practice lots of progressive enhancement and the like and so far is going nicely if I do say so my self.
The CSS3 Generated Content Module has some nice little bits in and I decided to use it to put some icons after external link ala wikipedia (but with nicer icons). Actually what I wanted was simply an arrow and I didn’t really need an image for that did I?
Well my first try went a little something like this:
Nope, no luck. By the time it hits the browser it’s not what I was looking for. I didn’t think old style named entities would work but what the heck?
Nope. As I thought. No luck their either. I also tried the character itself (but forgot to put that in the first post – thanks Joe) but again to no avail.
Some digging around and I managed to infer the correct syntax from some hidden away examples about something else.
You need to use the Unicode encoding. Doh. Job done.
For both of you who care what I get up to during my work hours this post is for you. I’ve finally gotten involved in a new startup with a couple of friends. Look out for hedgehog lab related goings on in a Newcastle near you. We’ve got plans being drawn up as I type (multi-tasking) but a smattering of training and consultancy on web standards design and development and a very health dose of some of those new fangled online products (all with as many microformats as I can sneak in) sounds like fun to me.

Bye bye to all the people at TH_NK. It’s been great for two years and I’ll miss the cunning hit and run attacks on asp.net that have yielded some pretty good results. I’m sure you’ll get more done without me being quite so obstinate!
It’s not just the impending job move that has been keeping me so busy. I finally, after much useful feedback and patience from Matt got my first Digital Web article done: APIs and Mashups For The Rest Of Us. As the title suggests it’s a quick run through the various goings on, in particular on what I find so interesting about the concepts of Open APIs and this web of data thing. I even squeezed in some talk of Microformats Let me know what you think. Thanks to Sam as well for a useful early proof read and to Tiff at Digital Web (who also has some interesting things to say on this here industry on her new blog, Lost Cog.)
End of self reverential post. The list of real things I have to blog about is frankly too long to wait around. And it’s not helped by these trips to london which frankly just give me even more to write about. Incase anyone is interested (and more to remind me), expect something on automated site testing, some musings and code (!) for doing useful things with microformats, some cool CSS3 (or is it CSS2.2 now?) tricks and something involving flickr which should actually be quite interesting if I get the time.
As a quick follow up to my previous post on unobtrusive javascript I thought I’d expand it even more, but this time in an attempt to make it easier to use.
As we’ve already mentioned javascript should be treated like any other programming language – not necessarily left to the experts but their should ideally be an expert involved somewhere along the line. One way of doing this is to abstract out configuration from programmatic logic. This massively helps with code resuse as well, leaving everyone to play to their strengths. It’s interesting to note that even programmers who would always store configuration settings seperately when writing PHP or Rails applications don’t always get around to doing the same in Javascript. Hell, I used to be one of them until Chris Heilmann pointed out how patently obvious the solution was while at The Highland Fling.
The answer is JSON. I think it’s nearly always easier with a quick example so here’s the Print snippet from the previous post but this time with a few configurable options:
The main thing we have done is to add a new block:
This is simply a JSON object of name/value pairs. You can easily alter the right hand values and change the workings of the app. You then reference the settings in  the rest of you’re code using dot notation:
In the above example our configuration directives are stored in our Print object. Depending on the circumstances of use you might find it easier to have a seperate Config object which stores all the configuration settings for the entire site.
In lieu of some real posts here’s a quick code sample to keep things ticking along.
I’m sure you’ve had the need to place a nice print button on a web page from time to time. If you haven’t you’re probably alone. A quick google will throw up lots of results for print javascript – the problem is that most of them are horribly obtrusive.
OK, so it’s easy to write something like:
But the problem is What if you turn off javascript? The following unobtrusive script automagically adds a print link to the page – in this case as part of a list item which is added to the unordered list with the id of iNav and a class of has-print.
With something like this you want it to load as soon as the dom is ready, rather than on page load. Brothercake’s domFunction comes to the rescue here. The namespacing is based on Stuarts technique which should make it more capable of just being dropped in to any site.
The only problem with this is that it’s just a little longer than the original. I think this is down to one often missed fact, at least until recently – Javascript is a programming language and should be treated as such.
Yeah, I just got my confirmation email for hackday in a months time.

So, who else is going along and does anyone have any ideas they want team mates for? I’m thinking something mobile or microformats related or alternatively something I know very little about. Any ideas?
Oh, and I know I’ve been quite around the place lately, more on that when I can talk about it.
Andy gave a great thought provoking presentation (exactly what conference presentations should be) at the recent Highland Fling shindig on the future of CSS which he’s written up into a good thought provoking blog post on CSS2.2
As web standards developers we want new toys! I took to CSS because it made sense set against the madness of table based layouts, not necessarily because it appeared easy or flexible. CSS has turned out to be hugely flexible, but some of that flexibility has come at the expense of some weird and wonderful and successful techniques. I love this ingenuity, I’d like to think that the original CSS specification allowed for this cleverness in real world use.
CSS3 has stalled. CSS3 is trying to solve a whole host of massive problems that others are better placed to comment on, but I’d love to hear some comment from members of the working group. If I’ve missed a regular blog by a working group member please leave a comment and let me know.
I’m a real believer in open standards and I think that it’s this issue that worries me more about the pace of CSS development. Yes, I want new toys, or more specifically I want to be more efficient (yes rounded corners, I’m looking at you) but actually, I’d rather have a good CSS3 spec than a fast and loose one. But the immenent fight between Flash/Adobe and Silverlight/Microsoft is showing how productive huge commercial companies can be when it wants to get something done. Both are introducing interesting new tools and allowing for what could be improvements to usability (or horribly confusing interfaces but hey). CSS and HTML should be the glue that binds all this together in a web where everything plays nicely together. But if CSS provides too little or too late then this is put in doubt. And if CSS falls I dont see microformats, rdf, html5 or whatever making a real dent – and I really want a semantic web (upper and lower case).
Well, I’d like to say the solution isn’t Browser Wars II – A good idea this time?
Andy’s suggestion of an interim step (or steps) between now and CSS3 modules raises a problem. If the working group do the work then it takes their eye further off getting CSS3 modules out. If they don’t, will they accept suggestions from outside? Will outsiders be able to grok the whole standardisation process?
What we need is a commercial company with lots of smart people and lots of cash – with a track record of working with standards. A member of the W3C would be a plus. Ideally they should have something invested in the web as a platform, maybe even in a smarter, cleverer, semantic web.
Google to the rescue maybe? If you’re really cynical you could see Silverlight as an end run around Google’s apparent dominance of the web as a platform. Lock content away in proprietary silos and traditional search becomes less appealing? And surely Google has lots to gain from the long term adoption of anything that increases the semantic nature of the web. If CSS is a battleground then maybe they need to come out and make a stand(ard)?
I’ve been pondering authority in microformats and as a result been taking a closer look at microID.
The situation I want to find a solution for is; If their are lots of published hcards for a given person they might be different, either out of date or more sinisterly fraudulent. If I do a search for contact details, lets say using the technorati hcard search engine, how do I know which one is to be trusted?
If I publish an hcard on my site using the following markup, including a microID using the included email and url details to form the hash.
If the hcard search engine reads that hcard on the morethanseven.net domain it could check the hash in the vcard against doings it’s own hash with the specified email address and url (checking the url is the same as the one on which the vcard lives). “Ah ha”. It can say. This looks like an authoritive hcard.
This does however raise a couple of issues. In order for the search engine or external reader to be able to create its hash you have to include the email in your hcard. I’m also not sure it does solve the issue of evil me:
Assuming the above is published on spammer.com then I’m sunk. It’s another authorative hcard with the same name, in this case my name. Of course therein lies another problem, when we name our children we tend to just do it on a whim, we even copy names we like from other people! And we change or subvert these names when we dont like them with nick names and the like. Domain names are unique so we avoid this problem.
So, a possible solution does present itself; using the uniqueness of domain names combined with the fact that most people aren’t evil spammers. Lets postulate a search engine finds lots of hcards for a given name. Most of these hcards probably point back to the url of the entity they are describing (so most hcards on the web describing me point back to this site). We can then check if one of the hcards is from that domain, and if it has a valid microID. In other words we use the lazy web to identify the most likely location of an authorative hcard.
I’ve been pondering lots of potential uses for microformats of late and came up on one thing I thought worth asking what people thought; namely the issue of authority in microformatted data.
Technorati provide a very interesting Microformats search engine that allows you to search for a person and get back their contact details via hcards that it knows about. For instance try doing a search for me and you get seven results (ironic given the domain name), try with one of those famous people and you get lots more. This poses a problem, what is the various hcard disagree?
You can always look at the publication date but this might not be enough. I could publish an hcard with incorrect data for anyone right now on this site and it would be more recent that the other, possibly correct, information already available on the web. Note that this isn’t a problem with Microformats per ce, more an ever present problem with the web. It’s just that Microformats make the matter more of a potential issue.
Is their a need for the ability to provide an authoritative hCard, or other Microformat instance? One that can always be trusted? If I publish an hcard of my details I’d like to think the latest version of that would be the most trustworthy. But how do you make a claim of ownership over a piece of data on the web (microformatted or not)? And for some of this data do you need a way of deciding who has access to it and who does not in a standard manner?
I’m pretty interested in the issue of identity at the moment, spurred on by the likes of OpenID and MicroID. I’m wondering if their isn’t some clever cross over here? Sites like ClaimID are doing some things in this space already so we’ll have to see.
Drew posted a while ago asking can-your-website-be-your-api. The simple idea is that you just might be able to live without a dedicated API in favour of good use of microformats.
It also turns out Tantek has also been on the case with his presentation Can your website be your API? – Using semantic XHTML to show what you mean and
Glenn has spoken on the subject too at WebDD and BarCamp.
This has interested me for a while and I’ve finally gotten around to writing some code, in fact I wrote the bulk of it during The Highand Fling and then the rest before the Refresh event after I got to te venue an hour early.
I used Drew’s hKit for the microformat parsing so everything that follows is PHP5 only I’m afraid. The also means I dont have a working example on this server at the moment but you can grab the code and try it out yourself. It also makes use of a couple of PEAR modules; PEAR Validate and PEAR HTTP_Request.
At the moment I’ve worked on a simple hCard example. It demonstrates the idea of a website exposing methods based on the presense of microformats, plus using that exposed method to extract the information we’re looking for.
First we create objects for the parser and the website:
Then we’ll ask what methods the website exposes:
In this example we get back an array of the available methods, in this case getContacts. Then just as an example lets print out some contact details:
This is a very early release, more of a proof of concept and as such their is no documentation and only a portion of hCard is supported. Hell, their’s not even a name beginning with h! Having said that I have a plan for a little project to kick the tyres and so I’l l be adding to it and hopfully have a proper release at some point.
The plan is to introduce methods like getReviews, getEvents, etc. which allow for the extraction of the relevant details. The part I find most interesting however is the idea of the expose method – asking a web page if it has an API, and if it does then what information you can extract automagically. If you’re interested let me know what you think.
Download example
Ok, so I’ve been shopping again but I’ll try and make this post a little more useful that oh, look at my shiny new toy.
I decided that what I was really missing to help with my twitter addiction and growing interest in the mobile web was a shiny new Nokia N800 Internet Tablet.

I cant really write a fair review at the moment as I’ve only just got my mits on it so that will have to save for later. All I can think at the moment is “Oh, shiny”. What I will do is list the software I installed on it from the start, partly to demonstrate what you can get on it, partly to link to thinks I found useful and mostly to remind me if and when I need to do it again.
And just for Phil it already has GCC-3.4-base on so no need to install.
Oh, and for the nay sayers thinking iPhone I don’t care. From all I’ve read the iPhone is going to be a closed system and this is a full blown Linux OS that is designed expressly to be hackable. Plus the N800 makes people swoon now, not in six months or a year when the iPhone hits the shelves.
I occasionally get some stick for wandering the country attending as many web related get togethers as I can manage but in fairness their are so many good events to get along to at the moment it would be rude not to. So here’s yet another run down of some upcoming events that people really should try and get along to:
Yes! It’s The Highland Fling in only a couple of weeks and it’s still looking as great as when I bought my tickets. A nice, small, event with a host of good speakers (and good guys) and it’s not south of me for the first time ever. Props to Alan for getting this off the ground.
If that wasn’t enought it’s worth staying around an extra day in Edinburgh for a Refresh get together. Again a good lineup of speakers and I would imagine a much interesting discussion and debate.
With all the BarCamp goings on around BarCamp London lots of other UK BarCamps have started to get organised, including one around these parts. Heather, Alistair, Meri and myself (ably supported with some soon to be revealed logo design by Elly) have plans. Watch this space.
While we’re getting out act together another northern BarCamp is about to get going. BarCamp Sheffield is happening in only a few weeks time. I’m hoping to make it along, maybe just for the saturday, but if anyone else is going along let me know.
If that wasn’t already enought (and I haven’t even mentioned @media) their was something of a big announcement from Matt over at the BBC today. Matt, and Tom from Yahoo have put together what looks like being the event of the year over the weekend of June 16th and 17th. Hackday is going to involve 400 geeks (you and me and all our friends in other words) spending 48 hours hacking on APIs and the like then watching some suitably bemused band.
Most of these have details on Upcoming which I’m increasingly thinking could be incredibly useful if only a few more people used it. So after telling you to go to all these events (which would cost money) I’ll leave with telling you all to sign up for Upcoming (which costs nothing) and add me as a friend.
One thing you hit pretty rapidly when you start having a look into all this Semantic Web malarkey is the number of rather silly acronyms and abbreviations. In fairness it’s true of pretty much every technical or academic discipline I’ve come across and you can ask the people I work with what I think about that – I wont ramble on here.
And dont think this is all because I’m not technical enough for ya, I have code on my blog and a faintly scary collection of technical tomes. I just think acronyms tend to breed elitism and make the world less penetrable, especially when they are often described in relation to other acronyms. So, after pointing out a problem here’s a stab at a solution; a web designers guide (with the relevant links) to my understanding of the different things involved. I’m not an expert on this yet mind, so if I’m wrong and someone more knowlegable can provide a better description then please comment. If the descriptions from the relevant links I have are good enough I’ll just use those as well.
Most people will probably have come across XML so hopefully this is an easy one. XML is the eXtensible Markup Language. For me that means it’s a set of rules for defining your own markup language; from simple data exchange formats to whole programming languages. The W3C says (which I think nicely highlights my problem from above)
XSLT is another quite common tool. The W3C again defines it in relation to another acronym but this time I dont mind as much and we’ve got a defintion of XML here anyway.
This is a pretty straighforward description, the only real problem for the outsider is wondering why you would want to do that!
RDF W3Schools says is really where the Semantic Web stuff starts. The W3C start off with:
Ah. It integrates applications… What! But wait:
This is pretty good actually, as long as you’re happy with the word ontology. Their is a good article that takes a stab at the question from Tim Bray. I think, simply put, RDF is common set of rules for defining your own meta data (information about information) so it’s interopable with everyone else doing the same thing.
But how are we going to get all that RDF into our web pages (Semantic or otherwise)? RDFa is one possibility (not going there right now) which involves extending HTML (all varieties) with some additional attributes with the purpose of being able to embed RDF in the document. Or from the W3C:
GRDDL is probably the worst offender here in terms of someone spending too much time coming up with the acronym. It stands for Gleaning Resource Descriptions from Dialects of Languages. From looking it appears to be a standard way of saying:
eRDF, or embedded RDF is a simpler approach to RDFa which involves no additional custom attributes but does not aim to be able to represent all the possible RDF constructs.
Of course their are many more, but at least that’s a start. Let me know what you think.
After the numerous games of Werewolf at Barcamp London recently a few of us got chatting about the idea of custom Werewolf cards using Flickr. Well I’ve just finished a very simple example of this and thought I’d post it up.

Head over to morethanseven.net/presents/werewolf for a set of printable cards based on Flickr Machine Tags.
In the future I may enhance this with a nifty interface which lets visitors select the number of each card they want – and I spoke briefly with Stefan from Moo about if it’s possible to link into printing them on those lovely Moo cards.
If you want anyone to appear just add the relevant werewolf machine tag!
Now we’ve got some eRDF in our pages we need to extract it out in preparation for doing someting with it.
First up we want to try and extract the eRDF in our page into an RDF document. Ian Davis has already created a nice XSL document to do just that. and I’ve implemented a nice service wrapper to extract the RDF from a given URL. Try pointing it at morethanseven.net or iandavis.com for an example of it in action. Next step here is to extend it to allow extracting a simple vcard from the RDF in a similar manner to Brian Suda’s Microformats extractor.
Dan Webb has recently written up his Sumo! microformas parser and boy is it really rather fancy. At the moment he’s only got profiles for hCard, hCalendar and hResume but writing profiles is relatively simple. Accoring to one of Dans comments he’s working on adding support for rel and rev style microformats like rel-tag and XFN.
Although billed as a Microformats parser, in reality what Dan’s created is pretty generic. You can use it to parse out any information marked up with any semantic class names – just like our eRDF.
I’ve set up a very basic Sumo! profile for FOAF and some simple tests to extract eRDF data from a test page. This is very much a proof of concept of extracting eRDF using Javascript but would be relatively simple to extend for the full FOAF spec once Dan or someone else extends Sumo! to support rel and rev.
On a related note the combination of Sumo! and the Firebug javascript console is just perfect. Anyone who hasn’t yet downloaded the latest version should get over there quick.

Following on from my previous post on eRDF I’ve started to play around with it. For anyone bored enought to have read the source of this site today you’ll have seen a couple of behind the scenes changes – specifically I’ve added a dash of FOAF.
The FOAF, or Friend of a Friend, project is:
I sort of see it as a bigger and more complicated older brother to XFN and hCard.
First things first, unlike Microformats, using eRDF needs a bit of setup outside just adding classes and attributes – specifically you need to add a profile to the head element of your document and then add some namespaces links into the head like so:
Then you’re onto the more familiar ground (if you’re used to Microformats) of adding semantic attributes. I already had the following snippet markup up in hCard, i just needed to add a few more classes (-foaf-Person, foaf-weblog and foaf-name) to existing elements.
The full FOAF Specification details a huge range of other elements – everything from foaf:Project for making associations between yourself and projects you have worked on, to foaf:OnlineGamingAccount which is pretty self explanatory.
When parsed out that gives you a foaf document a little something like:
So far I’d agree with Ben. All I’ve done is something people (including me) have been doing already with hCard. But I’ve done it in a way, using eRDF, that doesn’t in any way stop me continuing using Microformats.
Marking things up with Microformats for it’s own sake was only ever really of interest to markup junkies (including me again). It’s only when you can parse that information out that it gets more interesting. I’ve got a follow up post to this on just that subject involving some XSL, Javascript, PHP and some standing on the shoulder of giants. It will be left to a post after that when I try do something a little different.
On a related note; so far I’ve found only scattered and often overly technical and verbose documentation on eRDF and RDF in general. I’ve been adding links to del.icio.us as I find useful resources but I can see how useful Get Semantic has the potential to be if any of this is going to take off in the web standards community.
I’ve posted a list before of events and meetups I know about around the UK but had got playing with the Google Maps API and decided to go one stage further and create a hopefully useful place to put all that info in the shape of a quick mashup.
Have a look over on morethanseven.net/presents/meetup for a hopefully useful tool. I’ll try and keep this up to date manually for the time being, so if you know of any other get togethers please leave a comment and I’ll add them to the list. Thanks to the excellent, if a little technical (well, it is Google) API documentation and Jeremy’s Adactio Austin for inspiration.

I’d like to make this even more flexible, ideally all I should have to do is point the page at the group urls and then parse out the geo-coded microformats data and then display it on the map and page, rather than duplicate this information and maintain it by hand. So if any of the owners of these sites want to geo-code up their sites and let me know that would be great. Some Upcoming integration might be on the cards too.
Hopefully it should be of some use to anyone looking for things going on in their area, or if you find yourself in a strange part of the country for whatever reasons then you’ll know how to find the local geek community without risking life and limb with an odd T-Shirt.
Quite a bit of time at BarCamp was spent thinking, talking and in running skirmishes about the Semantic Web. Or the semantic web depending which side you’re on.
Now I’m a big fan and perpetual user of Microformats. They make sense, are simple to add everywhere (by stealth if needs be) and the potential is pretty interesting to boot. They are designed in the open, allowing everyone to participate and have a strong emphasis on solving every day problems in the real world.
Contrast that with RDF and the Semantic Web. Most of the work here has been going on for years in academic institutions and W3C working groups. It’s all rather grandiose. Everything I’d seen previously horribly broke HTML. But someone seems to have seen the light. Tom Morris mentioned in one of his talks eRDF, which is a method for embedding RDF in valid HTML.
After one of the talks I got chatting with Tom Morris and Tom Croucher, both card carrying RDF folk, and Niqui Merret another interested bystander. Toms argument basically comes down to Microformats solve the 80% problem, RDF trys to solve the other 20%. I sort of think the Microformats group would probably agree with that. I also like the idea of creating your own formats off the cuff. I’ve seen enough to at least have a further look see and hopefully try out a few experiements myself.
Ben Ward had a few things to say on the subject which are worth a good read. I agree with the majority of his arguments but coudn’t resist commenting on at least one:
I disagree. One of the best thinks about Microformats, that draw people (including me) in from the get go, is the unquestionable ease with which you can add hcard or xfn and get on with your day. Weeks or months later, after a few feel good projects you’ll be keeping an eye on the wiki when you come across something in need of structure. And even then you find it’s a composite of something you’ve seen before.
Right now I’d love to find that. A simple example of how to mark up a few very simple examples of eRDF. Hey, if that means contact details and a calendar event then so be it. If all that’s wrapped up in a lovely shiny green coat of pain with an icon logo so much the better. Until I can experiment with simple I cant do anything else, never mind anything new. So that’s two posts for the future sorted then.
Tom has set up a wiki over at GetSemantic to start to collate information together in one place which from my initial research seems like a good idea. If and when I get enough together (and the OpenId authentication is up and running) I’ll make sure to add everything there. If I get carried away and find interesting voices on the blogosphere I might event get a planet up and running. Or I might find way too much XML and leave it at that.
Meeting likeminded people is always good fun and the UK web conference scene provides lots of good opportunities for that. Sometimes it’s meeting up with past aquaintances or friends and other times mainly about making new contacts. Oh, and their are always a few co-worker’s in tow.
Mainly as a reminder but also as a “hello” to some of those I met down in London for BarCamp (BarcampLondon2) and Future of Web Apps (FOWALondon07) here goes with a list of some of the people, mainly colleague and a few neighbor I met on my travels:
Tom Croucher, Tom Morris, Gareth Rogers, Ben Ward, Jeremy Keith, Ben Darlow, Norm, Ian Forrester, Stuart Colville, Andy Hume, Mike Stenhouse, Pete Lambert, Lisa Price, Meri Williams, Elly Williams, Steve Marshall, Niqui Merret, Natalie Downe, Alistair MacDonald.
Between them they now have me interested in doing more with mobile messaging (SMS, MMS, Jabber), finding our more about eRDF, playing with Pipes, organising a Barcamp, getting some Moo cards, playing more with the flickr machine tags api and, oh, about a hundred and one other things including drinking beer (ok, that one was Ben). Consider that a partial list of the sorts of things I’m likely to post on over the next month or so.
If their is anyone I have yet to add to Flickr, Upcoming or Twitter hopefully I’ll find you soon or feel free to drop me a line. If I missed someone sorry, probably all the sleep I’ve been trying to catch up on.
This post brought to you by some of XFN.
Well, I’m now running WordPress. I know I appear to be aimlessly flitting between content management systems on this site but it’s more of a little research project I swear. Hopefully it’s all happening pretty seemlessly for everyone reading my ramblings. This time RSS came to the fore, allowing me to simply export a feed of all my previous posts and import them straight into WordPress. This sort of portability is very very nice for the user. I’d love to see this sort of functionality for hierachically structured data in larger enterprise systems.
I’ve got Wordpress up and running with a few choice plugins and a new theme in a couple of days which is nice. Hopefully any minor issues will be ironed out as I find them. An added bonus is comments are now back, I’ve got so many things to rave about after getting back from BarCamp and Future of Web Apps that letting anyone else point out the errors is only fair.
After a load of research for a couple of projects I thought I may as well add a large pile of links to a post for future reference. Feel free to wander through if your interests stretch to using scripting languages in mashups and the like.
Well, my presentation over at the local Ruby on Rails Usergroup went down OK last Thursday. I could have done with some more time to prepare and do up some nicer slides but most of the presentation was quite hands on with me coding and talking, which was a first for me.
My presentation was basically a whistle stop tour of Radiant. I rambled on for about fifty minutes or so, quickly going through the basic concepts of Pages, Layouts and Snippets and then on to an example site which I built from scratch in front of everyone – with only a few obvious errors and obvious mistakes.
I ended up talking some about ommissions in Radiant, which I’m not sure I got across as the positive that I feel they are (at least for me). I dont want a fully featured blogging system, rather something that’s easy enought to jump into somewhere and see how it works. A few ideas came up from this discussion that, given the chance, I’ll mention on the Radiant mailing lists, or give them a go myself – like the ability to save configurations of pages, snippets and layouts and then load then instead of one of the three default prepopulation options. Though I’m busy modding the Radiant commentable behaviour and pondering a move to the Mental release candidate at the moment.
It at least got me back in to presenting to an industry audience, quite different from the teaching I’d been doing. Hopefully a good thing ahead of BarCamp in a weeks time.
Just back from WebDD down in Reading at the Microsoft Campus and thought I’d post a few thoughts. I’ll get into a few of these topics in more detail down the line when time permits but for now…
I went to a few talks, one on Microformats by Glenn Jones and the other on web standards design by Patrick Lauke that I enjoyed but didn’t learn much. I wasn’t the intended audience really and I’d gone along because I didn’t fancy the alternatives. Both were entertaining and well attended anyhow, and I’m sure the more developer orientated folk got a lot out of both.
The main two things I came away thinking about however were the upcoming version of Visual Studio (codename “Orcas”) and Microsofts new identity platform CardSpace
Now I’m not generally a fan of IDEs, but someone has been thinking alot about how Visual Studio (and the new Expression Web) can be of use to web designers. Some of the CSS tools are very cool, I might do some screenshot from Expression (we all got free copies) if I get the chance. Their has also been a huge effort to integrate Javascript functionality, including full debugging and breakpoint support as well as intelissense. All of which looks really rather nice.
According to Microsoft Windows CardSpace enables users to provide their digital identities in a familiar, secure and easy way. Which should have alarm bells going off and (bad) memories of Passport flooding back. I went in not knowing much and in quite a sceptical mood, assuming the worst. Turns out I was wrong. The presentation, by Barry Dorrans, covered quite a bit of ground from an introduction, through code and working examples and even a big discussion of the issue of identity and OpenID in particularly. I’m suprised that I haven’t seen any mention of CardSpace along with the recent rise in discussion around identity and OpenID. More on this soon.
One last thing on the WebDD site caught my eye: around 350 people with a 50/50 split of web developers and web designers. In reality I think about 20 of us registered as designers which amused me. Microsoft doesn’t (yet) have a pull over designers as it does over developers. More on that too when I get my thoughts in order.
Ok, well, that was interesting I thought. For those that haven’t followed along on the site I thought I’d recap some of the weird and wonderful colour schemes that have been used on this site as part of my experiment with colorburn
Some were just about OK, some were quite nice really and a few were, er, pretty much unreadable (sorry). I’ll let you decide which where which:












Just a few things coming up in the next month that I’m attending that I thought warranted a post.
Now I think about it, or rather now I write them all down in one place, I think I might have got carried away.
I do quite a bit of big picture web site/application design at work (ok, that probably needs more of a description). I get involved in alot of the details that lie somewhere off to the side of the obvious bits that need to be done (mmm, not much better). All those technologies and tools and ideas only tell part of the story of building a successful site or online application. Even something stringing together a good idea, clean markup, good CSS, some fancy backend code (insert prefered option) and a sprinkling of user centred design tools can fall down on the hidden details; good form labels, help messages that actually help rather than just repeating the problem, organising file systems to allow for simple scaling, skinning and internationalisation, etc.
Tom Coates spoke last year about his concept of Native to a Web of Data which really struck a chord at the time, and not just because I still want this on a t-shirt. More recently the idea of REST has really caught on, especially amongst the dynamic languages crowd –  you just need to look at Rails 1.2 for an example their. The big picture is HTTP is back and everyone who didn’t already get it should be doing some reading.
Another notable person talking quite a bit about REST, HTTP and related issues is Joe Gregorio. Amongst other interesting posts, I came across one on URI Templates a good while ago and have been following the specification to produce recent documentation. The idea is simple and so is the specification. A quick example, for a simple CRUD interface might be:
Doing this for a site or application in the early stages of development really helps to highlight what actions are available, and to remove the need for costly changes later in the day. It also helps others learn, mainly through discussion, about the different HTTP methods and the importance of the URI in general.
Please note that Twitter no-longer support Basic Auth via the API so the following code no longer works. Please see the official docs for more info
Like others I’ve found myself becoming something of a fan of Twitter, the impossible to explain social networking site. If you’re reading this, have a twitter account and not already my friend then add me if you like.
Apart from the interesting social aspects I’m also interested in Twitter as an API for all sorts of communications, remember Twitter already deals nicely with SMS messaging, Instant Messaging, subscription and the like and has a nice XML and JSON based API. I’ve been using the Zamano SMS gateway at work for a few projects and Twitter actually lets me some more and doesn’t come with a price tag.
I started out playing with curl to send updates like so (obviously with a real username and password):
curl -u username:password -d status="twittering from curl" http://twitter.com/statuses/update.xml
I then used the PHP curl features to do the same thing from PHP:
Obviously you could do more with the return than print out a success or failure message. The $buffer variable has the returned XML or JSON for you’re parsing pleasure.
I’m going to try out some of the other API methods too, probably play more with XSL or look more closely at the PEAR JSON module in building up a simple library as a quick search didn’t throw up much of interest and the API is nice and simple; making it fun to hack on.
If you visited the site previously since the Radiant move you might notice a new colour scheme. Come to mention it if you visit tomorrow, or the day after you should get a different colour scheme too.
I mentioned that I planned on a few experiments around these parts and this is the first one. I’m a big fan of ColorBurn from Firewheel. For those that haven’t seen it it’s a desktop widget that displays a new four colour colour scheme every day that looks a little something like:

I often use it for inspiration for colour sets, or just to appreciate interesting colour combinations. This just takes it a step further. A little reverse engineering and I have a script (PHP this time, running from a cron job) that writes out a new stylesheet once every day with the colours from ColorBurn (using the magic of XSL if you’re interested). I’ve used an alpha transparency on the image to the top left of the page which might result in strange effects in browsers that I dont care about too much. Sorry, but I did warn you.
I have a feeling that some days I’ll wake up and regret this experiment and others I’ll be pleasantly suprised. I’m not going to cheat either. ColorBurn provides me with four colours and the script will use them in the order they come in.
As mentioned when I moved the site over to Radiant I promised a few tutorials and behind the scenes footage of goings on. So here goes with a quick look at how I added a nice OPML feed to my site.
For those not familiar with OPML it’s an XML format for outlines, often used for storing blog rolls and the like.
What I wanted was a friends parent and the ability to add friends to it and for them to appear in a dynamic OPML feed. The following screenshot shows what I ended up with:

To accomplish this within Radiant I needed a custom layout, one which would simply render the content but also had the relevant header and footer markup. Note the mime type setting as well.

Next up is the page which actually uses the OPML layout to render the page.

Note that I set the status of all the friends records to hidden, and then set the status of the r:children:each loop to search for hidden children. This way the individual friends records dont show up as seperate pages.
All I need to do then is add children to the parent, friends-feed in my case. I added a few page parts to each friend page to make management a little easier; a link, a feed and for a little added spice xfn.

All in all it makes adding and updating friends easy, and the general approach will work with pretty much any content where you want to compose a single page based on it’s children.
I’m starting to get properly interested in CSS3 for a few reasons. I’ve subscribed to the mailing list, read up on some of the working group goings on and done a bit of research ahead of really jumping in with some code play. Some of this could be of interest to others so here goes:
The best two resources I’ve found are the CSS Working group home page and the CSS3 Info
The CSS Working Group Under Construction site over at the W3C has a huge ammount of information. For those that dont know CSS is being split into a number of distinct modules, namely:
Phew. Their is also some work going on on profiles for TV, Printed media and Mobile. I’m interested in a few of these modules – particularly those dealing with Web Typography and the Basic User Interface module which deals with the look of form elements in their various states and more cursors and colors to describe GUIs (graphical user interfaces) that blend well with the user’s desktop environment.
CSS3 Info also has lots of information worth reading, including some good examples of parts of CSS3 that have been implemented ahead of recommendation in the preview section. My favourite bit however has to be the CSS selector test by Niels Leenheer. You can read a little more about that, and how different browsers do, or at least how they did in October. The W3C test suite is available too, but no where near as slick.
I’ve even gone and installed a WebKit nighly build to try out some of the features only supported their at the moment. Who knows? I might even add new designs to the site using new selectors as a treat for those silly enough to be using such experimental browsers.
Their is nothing wrong with a nice header graphic. Clients, designers and customers alike love them. But I often see them used as an excuse for fixed width designs. Well no more I say (unless, of course you have a perfectly good reason for a fixed width design in which case you dont really need this technique.)
The plan was simple. Find a way of incorporating flash headers into sites using liquid or elastic designs in much the same way you might use lots of sliding doors background-image goodness.
And all it takes is a snippet of actionscript goodness:
Download example
Web.py is a really nice lightweight web framework written in Python. It’s not trying to be Rails or Django, it’s trying to be as simple as possible. Web.php is my homage to Web.py. I’ve unashamedly copied the ideas and build a very simple web framework in PHP. It’s not a complete port, nor does it do everything in the same way.
The code example from the project home page was what originally piqued my interest:
My PHP versions goes something like this, I’m sure you can see the similarities:
I’ve used it so far on a couple of projects, but it’s never been properly tested as such nor do I have lots of time to develop it. It’s purposely feature and code light (the core file is only 80 odd lines of code, includig comments).
Download Zip
My first real foray into writing some simple Python scripts was a few scripts to establish the Gunning-Fog index of some given text. I wont go into lots of details about the uses of such a script; if you’re interested read Gez Lemons writeup over on Juicy Studio.
I took quite a bit of inspiration, and some code, from a similar script, PyFlesch, for establising the Flesch reading score of text.
I expanded things a little, using the Feedparser module to parse content from RSS and Atom feeds and give a Flesch or Gunning-Fog score of the summaries.
Download all scripts
I’m a fan of Backpack from 37signals. Although you can make pages public their is currently no way to style those differently, or to add content from outside Backpack. That’s where Backgarden comes in. It’s a simple PHP (4 and 5, the XML/XSL implementations are slighly different) application that builds a page via the Backpack API and some XSL magic that you can put on your site. You just enter the page address and your API key.
Download Zip
Content management is one of those subjects I often get involved in at work, and something I find pretty interesting as a whole – even if I keep having to reiterate most of the problems with content management are people problems rather than technological ones.
I’ve been using Textpattern for this site and a few others for a couple of years. I’ve spoken before about the recent move to more hand rolled solutions, and the flexibility that gives people to innovate. As well as occasionally threatening to really kick the tyres on Django, Rails or similar (Seaside, Web.py, …). I’ve been wandering the halls of the web development frameworks all year it seems – playing with Python and Ruby along the way.
To a point however these seemed more developer focused that something I could see being used by a web designer or small company (some intentionally so in fairness) or in education. However I’m back playing with Rails and started looking at existing apps to see how they are put together (I love open source) and a quick recce of Rails based CMS products turned up (amongst others that didn’t interest) Radiant. It appears to have been developed along side the new Ruby site launched in September and boy is it a nice piece of kit.
Assuming you have Rails installed, installation of Radiant is as simple as one command (It’s available as a Gem) and then another to get a site up and running (Ok, plus a couple of other simple commands to configure a database). First impressions it looks well put together, with a strong concept well executed and a lovely Interface. It’s chunky without being cliched and simple without being simplistic. I’m in the process of making plans for next year and this is hopefully going to feature somewhere. I love looking for a problem where I might just already have the solution.
Oh, and a only vaguely related note (well, Wordpress is a content management system of sorts) a couple of friends have started blogging over on breadlinedesign.com and anotherblog.com. Consider this a shameless plug.
I’m something of a want to be foodie. I’ve been a fan of food since working in a restaurant as a KP way back when I was at school. I like staying in and cooking, watching the odd food related progamme and going out to nice restaurants. However, I suddently realised it’s one of those things I dont yet do online. Most of the other things I do, or am interested in, I do at least in part online – except food.
So I set out to see what I could find. I’d signed up for Cork’d a while back after the buzz around the site in the web standards and Rails community. I’ve added a few wines and a few drinking buddies and keep thinking I should keep it up to date but…
A quick search threw up Chug’d, Menuism and Recipe matcher to name but a few – all complete with Web 2.0 cliches. But nothing that really made me want to sign up. I’m not looking for a food community really, more a useful tool to store food related tit bits. Recipes. Which wine goes with what and when the best time to get perculier beers might be. Like a food focused version of backpack maybe? A mobile interface, hey maybe even a Palm based data entry setup would top things off nicely.
Any other food people reading with a workflow that works? We’re always talking about software workflow or getting things done but what about something important like food!
Whatever you might think about image rich emails you have to admit clients are a fan, and sometimes needs must.
The web, as always, comes to the rescue and their are a few good articles around – many of them over on campaignmonitor.com including a set of Email Design Guidelines for 2006, Optimizing CSS presentation in HTML emails and the excellent A Guide to CSS Support in Email
A List Apart also has an older article and their is some good infmation over on xavierfrenette.com about the support in web mail clients.
Another little bit of useful info I thought worth posting, after a quick conversation with a colleague, concerns the use of background images in emails.
Assuming your going down the inline styles route you might want to have something like this:

&lt;div style="background: url(image.jpg) top left no-repeat; width: 500px;"&gt;&lt;/div&gt;;

But this just doesn’t work. The image doesn’t appear when the email is sent. A quick fix is to include the image inline, but hide it as much as possible with a height and width of 1px and a style of display: none.

&lt;img src="image.jpg" width="1px" height="1px" style="display: none;"/&gt;

This throws up one of those interesting annoyances in Outlook. If you’re sending the email using Outlook the image appears twice in the editing pane, hence the 1px trick as well as the style display: none. When it’s received the style rule gets applied.
This assumes you are sending the images along with the email, rather than simply linking to them online were you dont have this problem – but do have a host of other issues with blocked images and the like.
I haven’t admittedly tested this extensively, however I’ve had a lot of luck with Outlook, which for business to business emails accouts for something like 75% of the audience according to Campaign Monitor (and probably around what I’d expect based on my experience).
Well, the year is ever so nearly over. Again. I’m sure their will be a good few round ups, I’ll probably even partake again, but thought I’d start off with a quick look at how I got on with my list of stuff to do.
I did the conference thing again and I’m sure I will be back this coming year. @media, Future of Web Apps and dConstruct all look like going from strength to strength. And all being different enought to be worth going along. And I stayed awake throughout the Ajax workshops and I’m pretty sure I got to everything @media had to offer. So that’s three ticked off.
On the article publishing thang. Well, I’m know writing for Vitamin – doing book reviews as often as I can do, two so far on Communicating Design and Using Microformats and more to come. Hopefully more to come on that front too. Another tick I reckon.
The next two, related to building and releasing stuff (specifically Rails and the feed syndicator) I sort of passed on. I actually have some things in the works but I’m not likely to get their this year. The reason was probably my flirtation with all sorts of other things – in particular Python. Releasing bits of code is something I’m going to do more of. Watch this space. Two crosses.
Locally I messed up the domain name but settled on the nicer refreshnewcastle.org and with a little help from Matthew Patterson Pennell have a nice new site. Next year I want to do more than just web site related bits an pieces. Maybe half a tick?
Now the “something vague” idea. Well, teaching web design at Newcastle College definately counts here I think. I didn’t see this coming and hopefully I’m doing a good job so far.
Ok, so not bad going, but I could do better. And looking back over a list I made a year ago is quite interesting. I guess I’m posting this early to encourage anyone reading it to post a list of things they want to do next year. Anyone got any ideas?
A couple of people asked for the details of the virtualhost script I modified for use with MAMP in a previous post. Apologies for taking a bit of time.
First I had to make these changes to the original virtualhost script:

DOC_ROOT_PREFIX="/Applications/MAMP/htdocs" 


<pre>APACHE_CONFIG="/Applications/MAMP/conf/apache" 
APACHECTL="/Applications/MAMP/Library/bin/apachectl"</pre>

I also made a few changes to httpd.conf in /Applications/MAMP/conf/apache. From memory first to allow local .htaccess files to override everything on a per directory basis.

<pre>&lt;Directory "/Applications/MAMP/htdocs"&gt;
oAllowOverride All
&lt;/Directory&gt;</pre>

and then to point to the virtual hosts directory.

<pre>NameVirtualHost 127.0.0.1
Include /Applications/MAMP/conf/apache/virtualhosts</pre>

I keep meaning to do a screen capture tutorial of sorts, so maybe something like this would be a good starting point? We shall see as and when time permits. Probably this time next year. Any problems let me know and I’ll try and debug.
I have an admission to make; for personal projects, of which I have many, I haven’t been using source control until now. The reason? Probably laziness. I do lots of reading around the whole software development process but most of that gets used at work, at least until now.
One of my favourite articles is the old Joel Test. Some of this is less relevant to the home developer, but it does give you somewhere to start from. Well, I’ve now got subversion up and running and all new projects will be going in their. I’ve already added the repository to my backup policy. The free book Version Control with Subversion provided all the information I needed to get up and running. I’m more of a fan of command line interfaces anyway so haven’t installed any special interface. I’m sure I’ll look into the Textmate integration once I know my way around the command line interface a bit more.
Next up I’m on the lookout for something to use for bug tracking. I already use backpack and basecamp for different activities but I’m not sure yet if they will do for proper bug tracking. I dont really like bugzilla, although maybe trac is what I’m looking for?
What does anyone else use, either for source control or bug tracking?
Every now and again I go off on a tangent about the problems of learning, and teaching, web design. Recently I got an interesting email from a chap at Newcastle College and just a few weeks later I’ve now taken a couple of sessions with students studying web design.
I’m doing 4 hours a week, alongside the day job. 2 hours of theory and 2 hours of practice. Teaching everything from research techniques, wireframing and sitemaps to the first steps on the road to CSS and HTML mastery.
So far I’d like to think it’s going well (any students reading, please let me know if you think otherwise!) I thought a couple of observations would be handy at this stage, for those involved in doing the same, and for me looking back:
Anyone else with a decent amount of experience and a little free time I’d recommend trying to do something similar. Our industry experience is a massively important part of the next generations educational needs and these are the people you’ll end up working with or managing in the future.
I got to thinking recently, after playing around with more IronPython and talking with a Ruby fan, about the future of computer languages. Is their a war brewing between Python and Ruby in unexpected quarters? That quarter being the .NET and Java croud.
Personally I’m too much of a fan of dynamic languages to be wholly impartial, but working in a mainly .NET shop I’m following IronPython when I get a chance, and a friendly Java officionando seemed pretty pleased with JRuby (although why not Jython I’m not sure?) when last I spoke with them.
Java and .NET are the prime candidates when it comes to big companies buying software, and all of that investment isn’t going to go to waste any time soon. But the potential speed benefits of Python or Ruby, and the work already going on with the likes of JRuby, Jython and IronPython, could make for a smooth transition.
I guess only time will tell, and I’m sure some people are still not convinced when it comes to dynamic languages (heathens). In the meantime I’ve ordered Beyond Java by Bruce Tate to do some swatting up.
In between personal projects, sleep and work I dont think time exists at the moment, but a few of those efforts are starting to bear fruit.
I’ve just had my first book review published over on Vitamin, a review of Brian Suda’s Using Microformats. Hopefully this will be the first of many.
Let me know what you think.
This will probably have some sort of effect on the hibernating webdesignbookshelf.com that I’ve simply not had time to maintain. This was I just get to concentrate on writing reviews, although I have a few ideas for the site which dont clash with that.
More personal web related news soon. If for no other reason than to prove I really am busy!
I have to admit to thinking that XSL was something of a waste of time a while back. I’ve changed my mine and wanted to muse about why.
XML is simple enough to jump right in, and a few years back it had a big enough band wagon to mean jumping on was pretty much required. I think in hindsight, at least in my mind, alot of this was just a little gratuitous. XML abstraction layers, XML content management systems, SOAP, etc.
I’d often seen XSLT as the template layer in an XML based content management system and after several looks went back to using light weight systems that made sense. The idea of going from a database to XML then outputting HTML via XSLT just made me wonder why. The rationale that you could then output it as anything (WML, PDF, whatever) was very nice, but rarely actually realised from what I saw, and easy enough to do with any half decent templating system. You also always seemed to need more XSLT that it looked like you should.
Ruby on Rails even wore as a badge of honor that it didn’t need to do any XML pushups to deal with configuration. And even those with a penchant for Java seem to have problems with using XML as a programming language.
Of late though I’ve been back playing with APIs, many of which produce XML in one way or another (whether that’s RSS, Atom or some custom format), and XSL actually comes in pretty handy here for just thowing around content. I did some work with a Google Mini too using XSLT and their are some interesting tricks with Microformats to boot. I haven’t listened to the podcast yet so I’m just guessing at the moment but I could see Drew’s “Your website as your API” talk at the recent WSG meeting making use of XSLT?
So, I think I might have changed my mind a bit. XSL, I think, is going to be a useful skill to have in your toolkit in a world of microformats and APIs. The W3Schools XSL tutorial is a pretty good starting place if you’ve not used it previously at all. Thought I’d still rather use a decent templating engine for site rendering tasks.
Matthew just posted a quick write up of setting up virtual hosts on windows and a little of the rationale behind why you would bother.
Being a Mac person when I’m not in the office I thought a quick follow up would be useful for those that way inclined.
On OS X their are a couple of things needed to setup a virtual host. As well as the apache virtualhost directives the host needs adding to NetInfoManager.
Although you can do all of this manually I’m more of a fan of a quick shell script and, as it happens, so are others. I’ve been using a modified version of Patrick Gibsons Virtualhost’s script for a while. Just drop it somewhere on your path (or run it from a specific folder) from Terminal like so:
Where morethanseven.dev is the local domain you want setting up. The default version of the script makes the assumption that you are using the default web server which comes with OS X. I’m not, but it’s easy enough to modifiy the configuration parameters at the top of the (well commented) script to point where you want. I have it working perfectly with a copy of the MAMP installation package for instance.
I’m something of a fan of utility scripts. Often repeated tasks can be bundled up into a shell script, python script or automator workflow. Automatic screen capture? Batch image processing? Backup? It can help keep everything tidy, as well as minimising repeated effort. Anyone else with any good examples of process automation with regards web design want to share?
I’ve been doing a little more light weight analytical research, this time on the readability of the content I read in my feed reader, which mainly consists of web design and development blogs and magazines.
A combination of things brough me to this point. An interest in accessibility and readability, being a maths and stats geek and my affair with Python. Yes, I know it’s just stats, and I did just parse the titles and descriptions from feeds once. I’m not claiming anything other than I though it interesting and worth sharing. If I get really carried away I’ll set up a league table.
Who knows; maybe it will encourage some to use less dense prose? As long as people dont start competing for the highest scores!
For more information on the background to some of these algorithms and what they mean have a read of Jez’s excellent write up
A few interesting things to note. I’m sure some parsing error or such has Juicy Studio scoring so highly!
I’ll hopefully get round to releasing the scripts once I finish a couple of other things off.
The growth of niche job boards took another jump today with the launch of Authentic Jobs by Cameron Moll, following on from the job boards over on 37signals and Joel on Software.
I find all this pretty interesting, not because I’m looking for job (I’m not), but more from a social research point of view. It’s the whole long tail thing again, and it will be intersting to see if these things take off in other, shall we say, less geeky circumstances. Niche job boards for the building industry for instance? or insurance?
This source of focused data also gave me a chance to play with Python as a data mining tool, something I’m finding it amazingly well suited to. My background was in experimental design and analysis so I’m something of a number crunching fan deep down.
The scripts are simple and have no niceties like error handling or comments so I’m not going to post them unless someone really wants them or I get chance to clean them up. Safe to say I used the outstanding Universal Feedparser module from Mark for most of the heavy lifting.
Brief methodology was to grab the complete feeds from the three sites mentioned and dump all the contents to a file, sans markup and lowercased. I then parsed them for a short list of words – in this case programming languages and counted the results.
Well, here are a few of the tidbits of info I pulled out. This is only the start obviously, more time may mean more detailed (and accurate) research ideally with a time element thrown in.
This I found interesting. The old school (C++ and Java) might not be hot but their certainly seems to be a need for good programmers. Javascript being up their is probably to be expected but strange non the less, given where it would have been a year ago.
This obviously has myriad problems; your unlikely to use the word C# in a description unless your talking about programming, but you might use the word flash) and was weighted towards the Joel on Software site as that both had much longer descriptions and lots more elements in the feed. Nothing about density either, so someone writing Java, Java, Java would skew the results. If I do some proper research and write it up you could get around most of these easily enough.
Anyone know of other niche job boards around so far? I’d be interested to have a look. I have a feeling recruitment is heading towards the same fate as advertising; where online advertising models are increasingly making the the old do work for free and charge a percentage on the backend ways nearly unworkable.
I’m something of a gadget fan when I think about it, and the growing ubiquity of the internet makes my life easier, as well as posing interesting questions when it comes to design. It would be a win win situation if it didn’t keep costing m money. Well I think I’m going to be spending again soon (unfortunately?)
I only heard about the Nintendo DS Lite getting a browser add on last week, but it peaked my interest. The spread of mass market devices with WiFi and a decent browser throw up interesting possibilities, so it was with a smile on my face that I discovered the browser was going to be Opera. Their is quite a bit of information over on their site on press releases and a special Nintendo section and all looks good.
I’d be interested to see how the two screens thing works for browsing, and how the device copes with the problems of content not being designed for the small screen very often, though Opera Mini did a ridiculously good job. Also of note is that the upcoming Wii is going to use an Opera browser as well. The move from the desktop to focus on devices looks like being a very clever move.
One minor let down is that the DS browser costs. This looks like it’s due to extra hardware being required but it’s still a shame after the PSP browser was added simply as a software upgrade. Hopefully that wont be a barrier to entry. If and when (ok, when) I get my hands on one (launch 6th of October) I might try and post some tests. The PSP browser does occasionally fall over on very large sites with a nasty memory error, and I’m interested in how complete the CSS and Javascript support are.
I’ve mentioned Code Igniter before but only now have I got round to really putting it through it’s paces by building something proper. I got the opportunity (if you could call it that) thanks to a six hour train journey from Oxford.
I’ve been thinking about sprucing up this site for a while, and have a few other personal projects that I really should get round to sorting out. Something I’ve noted of late is a move, amongst some parties, back to bespoke personal content management systems. Jeremy has been speaking about some of the feaures he has been adding recently, Christian had a few comments too plus Jonathan has been posting  about his recent move to CakePHP from Movable Type.
I find this kind of interesting. Everyone started off with hand rolled solutions, seemed to move to the comfort of Textpattern or Wordpress and the more geekely inclined are on the move again. With such large user bases the big blogging platforms are (rightly) spending more time on bug fixes and user experience, rather than kick ass features. And us geeks want more Microformats and mashups that we can shake a stick at. Only this time we get to stand on the shoulders of giants by using frameworks such as Rails, Django or Code Igniter which means less time doing the boring stuff.
Short sales pitch. Code Igniter is cool, and more. The documentation is first rate and comes along with the download rather than just being available online (useful when your on the train.) It all makes sense as well which is nice, especially if you have some familiarity with the MVC pattern. It covers the basics so you dont have to – validation, database abstraction, session hanling. It’s also easily extensible through a nice plugin architecture and makes no assumpions about what you intend to build. I have a quick and dirty Textile plugin that I need to package up and put out – some discussion of this over on the forums which I may peruse.
In short expect to see some minor additions and the like round these parts as I try to transition away from Textpattern and any problems caused are no doubt the fault of some poor coding somewhere.
And I wonder what the next step will be? We have been through hand rolled to stable applications to frameworks to hand rolled based on frameworks. Any takers?
Well dConstruct has come and gone and people are probably just recovering in some parts. Travelling down wasn’t fun but that was quickly forgotten when we finally got to the pub. An evening of geekery ensued (usual fare, getting told to move inside before the police turn in, bit like @media 2005 really) and then onto the conference.
Thanks to the backnetwork its easy enough to catch up on all the goings on so I thought I’d mention only a couple of thinks that are still rattling around my head:
Simon and Paul talked about Hack Day. This is a really cool idea and once I want to get going at work. We shall see.
Derek was excellent again, talking about all sorts of ways Javascript could be used to improve user experience and accessibility. A interesting near aside about using sound as part of alerts to feedbacks got me thinking… We shall see about that too.
As usual the crowd was full of people both strage and familiar. The backnetwork is again more useful that I could be here. Ah, microformats, how we all want to play with thee. As it happens I’m playing with something similar for work albeit in a light weight sort of way. It’s really a good job I have some holiday coming up, I think I have so many things I want to see about I could be here for ages!
Update A few additions I thought I’d add here rather than keep for another post. Sarat just messaged me with a link to CrossOver Mac. I now have IE 6 running natively on my mac. Which is nice. I also installed Twisted but that’s probably for later too.
I finally got round to getting myself a new MacBook (only a week or so after Phil) and a nice piece of kit it is too. I decided not to simply plug it into my previous machine via firewire and port everything across. I sort of figured something would break moving from a G4 to the new Intel machines but I think the real reason was that I quite like setting up a fresh machine. It might just be me but I quite like installing and tweaking things.
I thought, for posterity, I’d post a list of the things I installed and some of the tweaks I got up to.
First things first. I installed most of these apps into a Personal folder in the Applications folder, just to keep them separate from the preinstalled Apple applications.
As nice as the OS X interface is a couple of small additions can’t hurt. With all the talk about Spaces of late I’m not sure if people had come across Desktop Manager before. It’s basically spaces, or virtual desktops, now.
I also include a few Apple apps in my dock that I use on a regular basis:
I’m a UNIX geek at heart, from even before OS X moved in that direction, and both Fink and Darwin Ports make grabbing most packages easy enough.
Web development is what I do, and I couldn’t be doing that without a handfull of frameworks could I?
Their are lots of ways of setting up the Apache MySQL PHP stack on OS X, the OS already includes versions of Apache and PHP in fact. I instead went for using the MAMP installer to avoid the likely problems of upgrading from source the preinstalled versions. I then got to work tweaking and twisting the default MAMP install into shape, adding th ability to quickly setup virtual hosts, a central code library repository, sorting paths out and setting up Python support. I hope to post about this later on in more detail.
All in all, everything is up and running nicely.
How do you convince people you know what you’re talking about? I mean, if someone wants to double check that you have a clue about web design where can they go? I’m not talking about the little things – CSS hacks, binary things which are right or wrong based on something obvious – I’m mean the nearly imperceptible little bits that all come together to make the sum bigger than its parts. I’ll use Garrett Dimon’s concept of Front End Architecture as a good example of this.
If you’re an expert too, no problem. You visit your personal set of blogs, ask a few people over on the lists you inhabit, digg out articles you read ages ago and added to del.icio.us. You learn something new and get on with your work.
But what if you’re involved in management? Or are bringing your expertise from another field to web design (say marketing, or graphic design, all things you need)? Of course you need to trust the experts but it would still be useful to have at least a big picture idea about what is going on. It’s the same for specialists – someone working day in, day out with ASP.NET doesn’t need to know the in’s and outs of CSS in IE7, but having the big picture – from architecture to usability – is going to be useful.
I started gathering some of this information together as a resource for work. One thing I noticed was that it was pretty much impossible to find the sort of theory based resources I was looking for without knowing where they were already. Specific blog posts (just blog URLs are too big a moving target to be useful for something so focused), online magazines, podcasts and books; their is so much information around, and much of it out of date or not useful enough to begin with. An expert can filter this, often without even thinking about it, but what about a non expert?
I’m going to gather this information together somewhere in the hopefully not too distant future but in the meantime does anyone know of any similar resources already out there? Where would you send your marketing savvy boss to get a feel for Ajax? or your best developer to get the skinny on usability?
Some interesting observations after a post and comments over at The Watchmaker Project
It turns out, after me jumping to conclusions, that the interface for Newsgator Online (my favourite newsreader for the last year or so since I went web native) varies depending on locale. From a brief look at the US, French and UK versions the US version has a newer (and from a quick look nicer) interface than the others.
First the US only version:

And then the UK (the French version is the same, except with the relevant language changes):

The usability issue I have with this is the user chooses the locale when they login, wherever you login from (newsgator.com, newsgator.co.uk or newsgator.fr). OK so it’s an edge case issue, but I now know I have a choice of interfaces. Do I stick with being honest and saying I’m a brit and get an seemingly inferior interface? or do I lie and pretend to be a yank? If I was a French speaker I wouldn’t have that choice either? I dont actually know yet. I’m definately going to lie a few times just to see the new features and the interface though.
On occasion I even surprise myself. Sometimes you just need to geek. Rob at work was discussing his ideal device, basically something that would read text based RSS feeds to him while in the bath in the morning. Lazy I know.
I thought to myself That cant be too difficult and I’ve never played with any text to speach stuff before and off I went. A few hours later and I just got my Windows machine to read out the top story from this site.
Python is so cool to just hack in. Did I mention that before? I’ve yet to do anything real in it, but playing around with code is nice. Anyway, on with a solution;
Lots of things to install first. As mentioned I used Windows, in this case so I could access the Win32 Speech API. I looked at using Festival on Unix but left that for now. Tools of choice were Python 2.4 with the Win32 extensions. A couple of Python modules do most of the work. First up pyTTS which deals with the text to speach and SAPI integration. Next comes the Feedparser module. I’d heard this mentioned by Simon over on the Yahoo Python developer network and it really is nice.
The rest was easy.<pre>import pyTTS
import feedparser
d = feedparser.parse("http://morethanseven.net/rss")
tts = pyTTS.Create()
tts.Speak(d['items'][0].description)</pre>
Ok, so this isn’t an application. Just a proof of concept but you get the idea and I dont really want or need this app anyway. Why then? because I thought I could. Oh, and Python’s interactive prompt is so cool. I want one for everything.
Well, Christian’s book, Begining Javascript with DOM Scripting and Ajax is out and, as of writing this Amazon UK only have four copies left so hopefully that’s a good sign.
A quick competition on the site peaked my amusometer and I couldn’t resist.

Keep an eye on flickr for more hopefully.
Everyone who is everyone already resides at YAHOO!, with more recent movers that you can shake a stick at.  I’d just like to make a couple of observations:
Both Drew and Stuart posted posts titled Joining Yahoo!. The brain washing obviously starts in the interviews so you have been warned!
The upcoming BarCampLondon event is being held at the Yahoo offices in London. I’m thinking about going along, but I’m scared Yahoo might not let all those talented developers leave on the sunday?
I’m working on some redesign work for one of my under maintained sites at the moment, with something of a redesign in the works. It seemed a good opportunity to play more with YUI, the Yahoo User Interface Library, which I’d had a peak at before and heard nice things about (sorry, I like the long namespaces).
I’ve nothing finished yet past a proof of concept, but I thought a couple of lines of javascript wouldn’t go a miss. Nothing fancy mind, just some nice bits I like so far.
The case in point was wanting to add event handlers to an unknown number of links within an unordered list.
First off a quick, clever, one liner. onAvailable checks for the availability of the object (if you remove the quotes) or an object with a specified id in this case. It executes the callback function, init in this case, when the condition is true.
Moments later we have events added to all the links.
Again you just throw in whatever you happen to be working with (in this case an array returned by getElementsByTagName) and everything just gets dealt with. I originally had a for loop, iterating over the links and adding events, then I worked out YUI just did it all for me if I let it. I like that. I could have done it myself. YUI would have let me and not complained. But if I want to delve a bit I can find cool, quick, clean ways of doing the same thing. I refactored my code, with the help of YUI in ten minutes. I like that, but I think I said that.
Last year I missed out on dconstruct, this year I sat and watched the timer tick down. Which didn’t help hugely as a fair few other people were doing the same thing and the site went into what appeared to be a javascript induced page request meltdown. Oh well. Everything worked out in the end, I paid my money for myself and a colleague to attend and so did 348 or so other people and it’s now sold out. Why do I get a feeling I could name half the people attending? and be about right?
Their will again I’m sure be more web conference as trendy holiday for web geeks and nothing else conversations, but again I dont care. I haven’t had the chance to do any holidays at all for ages unless you count Carson and @media, and I learn lots again as well.
Well, I’ve fired up the dConstruct feed thing again, although I need to update the codebase with the latest version as it lacks the bits I added (like new post indicators and the stats) more recently. Give me a week or so.
Much more interestingly, and entertainingly, I have to recommend the dConstruct Podcast. I listen to podcasts on occasion, always grab talks from conferences and the like and subscribed because it was their. I didn’t expect it to be a rock and roll fourteen minutes of comedy genius. Everyone who’s been to one of Jeremy’s talks or sessions will know he’s an enthusiastic and engaging presenter, but little did we know about the comic inside. Ok, so you need to know everyone, so maybe it’s a bit incestuous, but it’s still funny. Through in some more info about the upcoming event and everyone will be laughing. More of the same please.
On the web design podcast front; anyone know of any good magazine style programmes on web design and development? or even better a weekly comic look at the industry?
I always take ages to get photos off my camera. It’s like the sort of delay you would get if you sent them off to be developed. Anyhow, I’ve finally sorted out my @media photos and uploaded a few to flickr. Have a look if you feel so inclined. I’ve included a few below, mainly because these ones made me chuckle:
A look of sheer horror if ever their was one from Patrick.

Is “Chris Wilson”: not the spitting image of He Man?

If this isn’t caption competition material I dont know what is. Norm was most definately the man that night.

Ah. The famous straw incident. Featuring Nic and an awful lot of straws.

And I’m still trying to decide what was funnier; the fact Patrick set up a bird bath in the middle of the pub, or the fact it took him and Meri quite a while with the instructions to put it together.

Some people have been saying that the socialising is getting in the way of the learning and sundry at these conferences. I’ve got to disagree. You just cant ignore the socialising, not when it’s this much fun. But that doesn’t mean the message doesn’t get drilled home too.
Update UCAS to the rescue. Here are lists of graphic design courses and their ilk. Go get em.
Jeremy posted a thought provoking read that tapped into somethings I’ve been thinking about of late, and others that have been somewhere in my head for a while. In a nut shell:
I’ve got to say I tend to agree with pretty much everything Jeremy said. I could pick up on liquid layouts being harder (I dont think that’s completely true once you’ve done a good number, it’s like anything new, it just takes getting your head around) but that’s just nit picking.
One thing I will comment on is some of what I see as the reasoning and, if I can think of anything, some things we can do to counter them.
One issue is that most of the designs are not done by designers in my opinion. I’m not really sure I count as a designer here, I flit between too many disciplines to be completely comfortable with the title (or most titles to be honest). The perceived technicalities of CSS (it’s plain text) can put those without that bend (ie. Photoshop users) off, and at the same time attract people who thing vim is cool. Combine this with a truly fantastic online community full of helpful tutorials and you have a recipe for technically good, but creatively stunted, websites. Throw into the mix easily replicated techniques (rounded corners and gradients anyone?) and gallery sites with a low barrier to entry and you get to where we are now.
And I dont really think all of this is bad? Anyone can put together web pages, even half decent web sites. For me that just makes it more fun/challenging to do it professionally and demonstrate real value. Everyone can paint, but how many people can say they can paint well? And I dont mean your skirting boards. Some are already sounding the death knell for gallery sites. The real gallery is and should be the web itself. If design is about communicating, unless your aim is to communicate to other web standards designers, the galleries should be the last thing on your mind.
But where are the new breed of designers? The people doing the sorts of things that Jeremy wishes were happening right now (or more likely happened last year)? Malarkey’s book will be stunning if the bits and pieces he mentioned at @media are anything to go by, but who is going to force it into the hands of the graphic design student, or the art college graduate. And beat them over the head with it until they are ready to come work with me? Or you? Or me and you if you work with me already.
I’m now vaguely miffed. I thought right, I’m going to go and get myself a list of all the uk design degrees and put it up here. People could then leave comments, ring them up and try and get in to speak to these people about web design and report back. If the mountain won’t come to mohamed and all that. Except guessing and googling didn’t turn up anything useful. The design council had lots about education (in theory) but then the search facility broke (and the fact it appears to be a table based love child of the old adobe site and BD4D). UCAS are doing maintenance and are offline until midnight. I’ll post a list when I can get by hands on one. Or if you have better luck let me know. Funky logos and pithy slogans can come later, as can a big pile of presentations and materials if anyone, inlcuding me, ever does anything about it.
So. Who’s with me? I dont care if your scared of public speaking (or just scared of speaking in general, or even scared of the public). You’ll know more than the students in any case and that makes you dangerous. And they might have the ideas, and enthusiasm, we need to pass beyond the limits of our current design problems.
I know I went off in something of a straight line here. I made a bee line for design education. I could have detoured to visual design tools or photoshop designers with a loving for print and a propensity to want a 350 page website look like their comps. Both of them. Feel free to go off on a tangent yourselves, and let me know what you think.
I love this idea. It has a nice warm fuzzy feeling about it. Molly was musing about a code of ethics for professional web developers. Talk went on at @media about certification and education and the like, and the general concensus was pretty negative, for good reason in my opinion.
However, a code of ethics, more a stamement of intent from professional to professional, strikes me as a really good idea. It’s easy to forget that it can be difficult when learning (and aren’t we always learning in this job?) to really get down to best practice. It can be difficult to just do it at times, never mind do it properly what ever that means.
After reading on the comments I was feeling pretty good, and a few from the bottom I was thinking, I’ll set up a wiki, as suggested. Well, Meri got their first so good job Meri, it keeps it in Newcastle any how. Head over there now, it’s fairly light on content at the moment but I guess that is the point of this point.
What do other people think of this anyhow? Now I need to find some time. Or rather more time. I love being all enthusiastic, if only I found the time! I’m going to have to cut down on sleep, and I so like sleep.
Chris Wilson, during his @media presentation mentioned in passing the IE 7 Readiness Toolkit. This vaguely uninspiring title hides a very nice collection of essential tools for discerning web developers on Windows.
IE is painful we already know that. We have a large number of tools available in Firefox to help us out, but sometimes you need to go to the source. Along with a nice rip off implementation of a web developer toolbar, it also put me onto the Fiddler (what is it about these names) which is an HTTP Debugging proxy. What? Well it sits next to IE and monitors all HTTP traffic, allowing you to find out exactly what’s going on. Very handy indeed, and worth running just to see what’s really going on, especially in Ajax land.
A couple of other interesting tools are included that I’ve not had a look at yet, the Expression Finder looks particularly interesting. It basically tracks down any hacks in your files and tells you off.
All in all a good move from Microsoft. What with Open Search being integrated into IE 7 as well, and RSS maybe reacing a tipping point eventually (Non geeks will believe my predictions one day) things might be looking up.
Yes folks. @media 2006 is all over. Except for the year thing, everything else really was doubled. Double the people. Double the speakers. Double the number of chairs (no sitting on the floor this time!)
I made a list of a couple of things to make sure I did. Well how did I do?
Apart from that I got up to the usual geekery. Met lots of people. Drank a few beers. Went to a few talks and panels.
I wandered around with a whole host of people. I’d try and name drop but I’d miss someone and make them feel bad. Well, ok, probably not but their you go.
The evening of the first night of the conference was particularly amusing. Wetherspoons for food, a short rant with PPK and James about (what else) Javascript and then spending the evening under the stars with Patrick, Andy and a few others was, er, entertaining? slightly disturbing? bordering on stalking? All of those and more.
Their is simply too much to say and too little time. Which sort of explains the lack of details about the sessions and my take on them. I’ll probably post more about how I think the social side of things really is an integral part of these do’s, and how Molly really does deserve some sort of medal, and, well, you get the slighty hyper idea. And with my wi-fi hopefully working at home by now keep your readers peeled for both irreverant (sorry – irrelevant) banter and hopefully a couple of interesting insights and highlights.
More name dropping next time. Promise. Oh, and leave a comment if I said hi, it’s always a pleasure guys.
Right. It might have taken an overlong trip to London but I’ve finally got a spare moment and internet access. All back to normal hopefully soon. (mmmm)
Anyway… @media! It was ages ago, then some magical time warp type thing happened and it’s June all of a sudden. And I cant wait. Which is fairly handy as I’ll shortly be heading out to the pre show party to meet at least some people I know (or at least vaguely recognise from somewhere, once, maybe) and hopefully some new faces, always a solid part of the conferences I think.
Some of the things I want to do over the next couple of days, besides listen to alot of good stuff. (Rereading this I realised I just refered to Molly, Tantek,  Jeffrey, Eric and everyone as good stuff. This is likely to be something of an understatement. No offence intended!)
And I’ve no idea which sessions I’m going along to before you ask. I’m still pretending I can go to all of them to avoid the disapointment.
I’ve been doing all sorts of stuff I want to blog about; I’m doing a Masters type usability and testing certificate, playing with more PEAR, I’ve been having a play with Codeigniter quite a bit as well, which is lovely (thanks Matt) and have a couple more small apps in the oven. I also want to clean up a couple of olders ones (the mailing list, the feed syndicator, etc.) and still need to get round to writing a proper article. And that’s before @media, I dont want to thing how busy I’ll make myself after than once I get even more ideas.
Oh, and apologies for the delay in getting the calendar updated. And I’ve no idea why some people couldn’t find it with the search tool. Anyway, updated now, so at least people shouldn’t get lost tomorrow.
I’m afraid you’ll have to bare with me at the moment. I’m busy, moving house, have no internet outside work (except open wi-fi points) and oh, did I mention I’m busy?
I have the usual raft of ideas, inability to really finish things unless I try and a dangerous penchant for playing with ever more scripting languages (XOTcl anyone?).
Oh, the friends bit? Inspired by Molly and her incredible ability to be open about her life, I thought I’d say a huge Thank you to everyone who helped me move house this week. Much appreciated. It could have gone oh so wrong, but everyone chipping in meant everything worked out just fine.
I dont often just post links to things I find interesting. I try and warble on more about something or other that’s on my mind. However, being rather busy and coming across something that really stood out makes this here post different.
I stumbled upon DabbleDB the other day. Watched the video. Thought to my self “That’s pretty darn slick”.
DabbleDB is, pretty simply, an online database. For everyone. Ever. The blurb from the site says:
What does that mean? It’s basically MS Access, but better and available through a web browser. Too complicated you say? Too hard to grasp? Watch the video. Be impressed.
One claim I’m interested in getting first hand experience of is the one regarding Spreadsheets. Most people use spreadsheets as light weight (ie. flimsy) databases, rather than the number crunching powerhouses they really are. DabbleDB seems an ideal replacement to the former, but what about the latter? I remeber seeing Dean Edwards’ playing with a Javascript Spreadsheet a while back.
I have to say I haven’t used DabbleDB yet. I haven’t signed up for a one month trial because I know I wouldn’t have time to explore properly just now. Everything that I’m excited about comes from the video, the site and the attached blog. When (not if) I get round to having a proper look I’ll try and remeber to post something.
Pretty much all your common desktop apps are now available through a browser, except crazy things like image editing. Oh and the browser itself. Now their’s an idea. A meta browser. Within your browser window open up several smaller windows, maybe using different rendering engines or setups running remotely?
Tis about time for another meet up in sunny Newcastle. Pretty short notice but hey, the mailing list has called.
Thursday 25th of May it is
We are aiming for Tilly’s on Westgate road. More info over on Upcoming.org
Already a good few people coming along so, if your in the general area, why not join us? If last time is anything to go by expect lots of people who either dont know each other or only know people by domain name discussing anything vaguely (and I do mean vaguely) related to the web.
The way of recognising everyone, as always, is the carrying of a recognisable web related book. Extra points for something really geeky, or really large.
I’m hoping for some good banter and some ideas for the future. Hey maybe even making these a regular shindig? It would save us taking ages to decide when and where to meet. Though on the where bit I pretty impressed. Their seems to be a strong correlation between web people in Newcastle and proper beer. Speaking of beer, I heard a rumour that Steve was buying a round to celebrate his new design.
At work we mainly use Microsoft technologies, .NET and such like, for our development. As I mentioned previously I’m playing with Python at the moment, and getting on quite nicely. With .NET being a framework, aimed at allowing a variety of different languages to access the same class library, I though
The rather large Programming Python tome alludes to it being on the cards after the implementation of Jython but no more details than that.
I came across a few dated references indicating that it was possible but, and mentioning very slow reference implementations, all of which didn’t bode too well. Then up popped IronPython from an article by Jim Hugunin. This looked like what I was looking for. But it looked unmaintained, the page not having been updated for a couple of years!
But the story continued. Jim, having moved to Microsoft, was still working on IronPython and it looks like it’s moving forwards apace. The latest beta was released on the 20th of April and it looks like 4 more betas before a 1.0 release.
This discovery makes Python appear even more interesting. A nicely designed, cross platform scripting language with full access to the might of the .NET framework and it’s own good looking web development framework in Django.
So, for anyone else looking around for similar details here’s a brief list of up to date information:
I’m quite a fan of mobile devices. I’ve got a Palm, I had a Pogo for jeepers. However, I dont really have a geek phone at the moment (long story) – I’m still using my K700i.
While watching the last day of premiership football in a suitable venue I thought I might see about going to the cinema. With my trusty phone I tried to find what was going on. I waded through pages and pages. I did searches. It sent me round in circles more than once and I gave up more than once. I found some info. I tried to buy tickets. It failed to acknowledge the existence of first the cinema, and then the film.
Someone asked me at a recent talk about how easy it was to make sites compatible with mobile devices and I said (something like) “pretty easy”. I’m not happy with that answer now, so I’m going to change the question. How easy should it be to makes compatible sites?
WML might not be sexy, but it’s still in use all over the place. Support for mobile style sheets for proper web browsing seems all over the shop, with hundreds of different browsers pulling every which way.
Their are a host of IA issues that browsing on a mobile device brings up. A vast number of slightly new challenges to look at – and importantly a whole new way of confusing the user.
It makes you think maybe we need a dominant browser in the mobile space? Something to occupy alot of the ground and for the other browsers to loath and persue endlessly?
We also need some centres of best practice. I’m sure their are some out there. I just dont know where yet. Some references. A Mobile Zen Garden. A good book or two to review. Any thoughts?
It’s rebooting time people. First up a shameless plug for my entry this time around. In the spirit of screenspire here’s a full length picture for those that cant be bothered to visit the site proper.

Only a couple of reviews up at the moment but we will be adding them as fast as out busy fingers can write them. Big thanks to Mathew Patterson and Nicola Dobiecka for their assists here. Otherwise you would just be listening to my blatherings (a little like here then?)
Anyway, let me know what you think. As with all such endeavours more features (and probably fixes) will see the light of day over the next few weeks and months. Once everyone else is up I’ll post my personal faves, along with everyone one else ever.
I do quite a bit of musing on the subject of the community. Lots of other people do similar, just a few of the groups I know about:
Conferences are massive as well. With Carson going down a storm, @media looking to rock the summer again and some people spoke highly of SXSW, but then I wasn’t there (this year).
We all know each other. Think about it?
We are like footballers. We are the Premiership. We hang out in the trendy bars (conferences), we do everything we can to get mentioned in the papers (A-list blogs, A List Apart) and we get multi million pound book deals (DOM Scripting).
But theirs a difference. Youth players (students), the lower leagues (Z-list) and amateurs watch football every week. They read about it every day online and in the papers. Everyone knows who the best players are, whether they like football or not. Even then thats before you get to scouts from the big clubs getting along to youth and lower league games to find the next big thing, or the World Cup. Web design still seems spit between those in the know (probably you) and those not, for whatever reason.
So my question, if their is one, is – is football simply more mature? Do we need scouting systems? World Cups and better training facilities? Or alternatively have I taken this football analogy way too far?
Google just announced GData, which is apparently a:
It is basically an amalgamation of Atom, RSS and bits and pieces of Goodle juice. It piqued my interest for a couple of reasons:
The former relates to a simple solution to the problem of concurrent updates. Say I grab a file and make a change, but in between another individual grabs the same file, and after I submit my changes, they also try top submit changes all my changes could be lost. GData specifies a solution to this by introducing version numbers, only if the version you are modifying has the latest version number will it be accepted. The version number is simply provided in the edit URI in the feed. Nice.
I wont ramble too much about REST, safe to say if you like URLs then it makes perfect sense and you can just get on with it. Lots of other posts about why if you missed the (ongoing) debate. It’s a bit like Microformats and the Semantic Web. The bariers to entry for microformats are the the same as those for building web pages.
I say the promise of Authentication above because it turns out that part of the system is not in place yet. The other part of that I’m sceptical about is, from my reading, a complete reliance on Google to provide the authentication. For some applications that’s fine, for others you probably dont want Google in charge of the keys.
It also has simple proscribed methods of dealing with categories and search queries, including Opensearch integration.
I was thinking about playing with a PHP based client/server for GData, or at least having a look, but I think I’ll wait until I know more about the authentication side.
I have a feeling Microsoft will do something like this as well, take existing open standards and republish them with a few bells and whistles. And again the developers, ie. us, are left supporting multiple slight variations on a theme. Maybe I’m just suspicious. Time will tell.
I’ve become something of a Gmail convert over the last good few months. Moving everything online has made my life oh so much easier. I’d done the same with my feed collection a good while back with Newsgator. Storing all my calendar related info was something that was on my perenial todo list. Well, Google finally got round to releasing their own calendar after months of “will they get on with it already”. First impressions are pretty good. It’s simple. It’s obvious (at least for a geek like me) and using Gmail already I quite like the styling.
One of the features I really like, but which will only mature with age, is the shared calendars. It makes publishing your itinerary to the entire world very easy. The more people use it the more useful it gets.
As a case in point I decided to enter the shedule for the upcoming @media conference and share the calendar – to be honest to see how it all worked. Of course, about 24 hours after I did this the lovely folks at Vivabit completely revised the initial schedule. Ok, so I was only momentarily annoyed considering the conference just got a whole lot better – though they did drop the CSS3 discussion panel which I though would have been interesting.
A little rejigging and the shared calendar is up to date again:
Feel free to subscibe, either via your own Google calendar or any iCal compatible application.
Well, it’s nearly 6 months since I launched the new design on here and entered into the 2005 Fall reboot with minimal acclaim but lots of nice comments and some traffic. That means it’s nearly time for the Spring event! I still like this design, which is something of a first for a pet project, so made a descision early to do something different.
I like the reboot. It’s a deadline for a start, with all the good and bad things that that entails. And as I’ve mentioned to a few people recently the 6 month release schedule seems just about right.
My new pet project aims to fill a gap that I’ve noticed when scouring the web for knowledge. I’ve mentioned before on here about being a fan of books, and have something of a stash of web, design and software related titles that I plough through pretty much constantly. But knowing what to buy, and what to read next, is never easy – even when I consider myself in the know. I’ve still got a fair few titles from when I started out which, in hindsight, were not the best starting point for a budding web designer.
So along comes Webdesignbookshelf.com

The domain is sorted. The design is nearly sorted (but subject to change a bit). The backend I did a while back (more on some of that later). I’ve no idea what happens to it in IE yet, but you cant have everything. One think I do need to sort however is content.
What I’m aiming for is good quality reviews. For the start I’m not looking to have a percentage score, or a star rating system, or similar. In my experience these tend to cheapen the end result, and become the be all and end all of the review. Books deserve more. Alot of it depends on context as well. The recent Javascript Anthology is a good book and worth the admission price, but I skipped most of it as it covered things from quite a basic point of view to begin with, something I wasn’t expecting. It really kicked into action later on, especially the accessibility chapter and the section on obect orientation. I would have liked to know all that before I bought it.
With so much information available online, and new sites popping up all the time, I still think books have a place. Especially with lots of talk of self publishing at the moment maybe we’ll see more books bypassing the old publishing processes. This could include books bypassing any kind of editorial process as well, good quality reviews can help act as a filter, a helping hand if you will.
Anyway, if anyone reading this far fancies contributing a review or two please let me know. I cant promise wealth, or fame and fortune just yet. But you should get a warm feeling that your helping a community, and you name up in lights if you like that sort of thing. And watch out for the May 1st launch over at the reboot, should be another good one.
Lets face it, web apps are getting bigger. At least in terms of the amount of data they store. Take two big poster siblings, Flickr and del.icio.us. All those user accounts, all that interlinking between then, all that meta data. Technorati is another example – trying to keep up with all blogs everywhere in as near to real time as possible has got to be seriously data intensive. And at times this has shown through, all have had problems  at some point in their short histories.
If you are looking to get into the world of big, community focused, web apps then you need space. And I know I cant afford a huge outlay on storage and bandwidth, even most small web shops would probably have something of a problem with the large outlay, limited short term money making problem.
An interesting solution to this would appear to exist in the shape of S3 from Amazon. It has been around a little time, and others have been intrigued enough to mention it, but I decided not to ramble on until I’d had a brief play. Well, it’s quite cool.
Amazon themselves provide a number of useful code examples
in different languages, including Perl, Python, C#, Java, Ruby and PHP. They also provide examples of the raw HTTP requests for thos that way inclined.
This openness (OK, so they dont really have a choicee, the whole product consists of an open API, it being closed would seem to limit it’s usage!), as always, has allowed the community to run with the idea, building up sample apps and some easy to use wrappers:
Nothing here feels complete as yet. Most are early hacks by people getting on board early and giving the rest of us a starting point.
Overall I like things like S3. If it works reliably and scales well then it could become something that just sits behind all sorts of realy cool ideas, preventing data loss and removing another potential headache for the small developer.
Any other resources, or ideas, more than welcome.
I’ve been going along to the local industry do Think and a Drink for a good long while now and they have talked me into speaking at the next do. So I wont be thinking for the first hour, rather rambling on about the chosen topic of Web 2.0. My loose plan is to mock the term and big up the technology involved. Someone else has the job of looking at the business implications, which should also be pretty interesting.
Apparently this is me:
Gareth works at TH_NK Ã¢â‚¬â€œ a newcastle based brand technology company. He is involved in a number of areas of the web design and development process, from the design of modern web applications to building effective online interfaces. In his spare time, he blogs on web design, development and accessibility at morethanseven.net and organises events for the local developer community on newcastlenewmedia.org. Keeping up-to-speed with web trends and new technology is something Gareth is passionate about Ã¢â‚¬â€œ and it shows.
If anyone who has never been before, or isn’t a codeworks member, wants to go along then let me know. I’m sure Heather would love more people along, and be good to see some familiar faces (or at least familiar domain names) in the audience.
I’ll publish my brief presentation if it’s any good after the event. See you there?
Rejoice. It’s CSS Naked Day!
Come on everybody, get your markup out for the ladies.
I dont often ramble about accessibility issues on here, something I’ve only just noticed. Quite suprising as it’s one of those things that I’m pretty interested in – and have been for a good long time.
Anyway, I’ve just finished reading through PAS 78 or Publicly Available Specification 78, Guide to Good Practice in Commissioning Accessible Websites. It’s a document aimed at those buying websites, especially in the public sector, who probably know accessibility is a good think but could get easily hoodwinked by anyone with even a modicum of knowledge and evil intentions.
Rather than just provide information about the what and why, considering the doc was launched a month ago, I’ll just point you to a good synopsis from Isolani. A few other notables have posted good reads. Joe Clark, as always, has an opinion which is both anally detailed and all true. Bruce also has a run down from a (some what) insiders point of view.
My feeling was, overall, pretty positive. I’d love to see all the clients I deal with having read this. It’s a pretty good intro to the issues at hand, though it is maybe a little long and technical for some of its audience. One area in particular that got quite a bit of coverage, rightly so in my mind, was testing. Their still isn’t enough testing of any sorts for websites, even with all these beta  tags going around. The testing area covers all sorts of bits and pieces, again not in enough detail for the implementor but certainly enough for any good procurement office to be able to separate the wheat from the chaff.
A good series of mentions goes to ISO 13407 which covers Human centred design processes for interactive systems and as luck would have it the subject of a workshop I’m attending with Gilbert in Sunderland on monday. More on that after the fact.
Anyhow. I’d recommend getting your bosses to pay the Ã‚Â£30 (or for bosses, stump up your selves) for the PAS 78 PDF. It’s a pretty good read anyhow, and hopefully more and more proper clients will be reading it soon. Forewarned is forearmed as they say.
Ok, well not quite first impressions. I’ve dabbled before but not for a particular reason and never actually built anything – but I’ve spent most of today getting stuck into Python.
I have a couple of reasons for wanting to learn another language properly, and I’m more interested in Python as a general scripting language than as a replacement for PHP as my web language of choice (Rails hasn’t really caught me  yet, sorry). I could do some of this with PHP I know but that brings me to my second reason. I’ve been reading the Pragmatic Programmer recently and one of the things it has to say is:
I like the idea of that. The rest of the book has a number of interesting things to say about programming in general, much of which is easily actionable.
A few of my regular reads sing occasionally about the joys of Python, which was probably another reason I’ve gone that way as well.
Anyway, first impressions is that Python is pretty easy to pick up. I keep wanting to put in braces and semi-colons but looking at the code it looks clean, simple and readable. Arrays, string functions and flow control appear to be present and correct. They work how you would expect which is what you want really for core functionality. Objects are next on my list of things to look at.
I’m building a simple log file analysis tool as I go. I’m fine with it being a command line app to be honest, but I might end up tinkering with some of the GUI features at a later date. One of the reasons for going for Python was it’s flexibility in this regard. Hopefully building something as I go will make it more likely that I’ll keep playing. I have a couple of other sample apps I’d like to make as well which is probably a good thing again to maintain interest.
Anyway, depending on how things go some more comments or code may emerge. Anyone else taking similar advice to learn new languages? If so which ones?
I’m playing with a few thinks at the moment and one of those had me thinking of tag clouds. For those who dont spend time on flickr or similar you may not have seen then, or at least heard of the terminology. All we are really taking about here is a visually weighted list.
I dont agree with Jeffrey here unfortunately, who has issues. I can see the point, that sometimes information is better organised by one person than many (think your weekly food shopping list) but in some cases it can be nice – especially where it provides an alternative navigation method. Maybe I just missed the part where everyone had a tag cloud? I’d love to see, for instance, a tag cloud on last.fm of bands that I might like based on my listening habits, and those of a similar musical persuasion.
An interesting article on where next for tags clouds throws up a few interesting points but one thing I have not seen discussed at all is the accessibility angle. Visual prominence is one thing, but without some sort of semantic prominence those not viewing the content visually lose out. A very unscientific survery involving the del.icio.us tag cloud confirmed what I though would probably be the case – the visual effects (ie. bigger or highlighted text) are implemented purely via classes on the links.
What would be the effect of using plain links, moving to emphasis for the next level, up to strong for the really important tags? We can style the content exactly the same as before, but we have added some sematic prominence to the content.
Anyone had any experience implementing tag clouds in live projects? Even better, anyone had experience of user testing such interface devices? Real bonus points for accessibility related testing.
I had cause today to stop and think about books relevant to learning the bigger picture side of building web sites and applications. Code, graphic design, whatever your bag in between, you are either aiming to become, will become or at least affected by people in management positions. Even small companies are not immune, though they can avoid some of the problems (but often by causing some other ones as well.)
I’m a big fan of books, I read lots online but when it comes to regular, night after night reading I much prefer to be in bed with a book than a laptop. I have no problem at all falling asleep and the book falling lightly on the floor for the morning.
Books do however date, or rather some do, or maybe if you take them as gospel they do. Anyhow, I see them as useful – perhaps even more so in distilling knowledge for consumption by other, less obsessive, members of out industry?
Without further ado I present a short(ish) list of books I think everyone involved in web software should read:
Some are most definately on the software side, others very web centric while more still are a little older and just plain odd. However all of them serve a purpose, they let people see into a way of thinking. The idea that programmers are wizards annoys me. It’s not magic, it’s creativity, just with tools most people are not familiar with.
If you are someone involved with the building of appications give some of the above a try if you ever feel like reading up on something different, apart from the latest CSS or Javascript tricks.
If your involved in the management of projects but come from a non software (dont kid yourself because it’s the web it’s not software) background then dive in. You’ll start to understand some of the little bits that puzzle you so.
Anyone else got any books in a similar vein? I’m always on the look out for an interesting read. Or do you complely disagree – do you even still read books?
I’ve finally decided that PEAR is a good thing. For those unfamiliar with it it’s a little like CPAN for Perl. For those unfamiliar with that it’s a repository of high quality PHP code, covering everything from benchmarking and debugging to database abstraction.
I’m about to dive back into PHP after a minor break playing with mainly client side toys, and with my work now mainly on the markup/styles and planning side of things PHP is a nice break. PHP isn’t the devil as far as I’m concerned, and I think now I’ve grown up PEAR is part of the non devil like side to PHP – the professional side if you will.
I like PHP for it’s flexibility. The same problems this causes in terms of it being easy to write poor, unmaintainable code are the same boons in brings if you use it nicely. I dont want static typing, on occasion being able to get down and dirty with procedural code is handy and with so much good code out there in places like PEAR it’s often very quick to put together something from existing parts.
I even went as far as getting a book, PHP Tools to have a good going through. I’ve been on something of a mad web design book shopping spree of late, so I’ll try do a quick round up of books soon as well.
Anyone else have much playtime with PEAR, or similar code repositories?
PS. Apologies for the two week hiatus. I’m something of a busy person at the moment. More quality posts as soon as possible. Promise.
Web applications are still evolving. This we know. One thing any power user tends to pick up on pretty quickly in desktop applications is the keyboard shortcuts. We just cant get enought of them. Why dont we see them more in web applications? Well a couple of reasons as it happens:
Personally I think in some cases keyboard shortcuts would be nice, I have a little plan for something in mind as well so started experimenting. The clash issue I’m going to leave for another post as it will take a bit of brute force experimenting but I did start playing with a simple way of assigning functions to keys.
The following code creates an object to store the lookup/hash/two dimensional array (pick one you like). This is a simple associate between a key, and a function to run when it’s pressed. A function is then assigned to the onkeyup and onkeydown events which monitors them, checks for one of the keys in the lookup to be pressed (in this case along with the ctrl key) and runs the assigned function.
A simple library function is used to add the event to the document onload. My library obviously includes other helper functions but we dont need any of those for this example.
Some improvements I can spot from here which I’ll implement as I actually take this out of the lab and into something production like.
I’ll try get round to the look at keyboard shortcut clashes, or link to someone who already has, as I get round to it. In the mean time, what do people think about the use of keyboard shortcuts in web applications? Any good or bad examples welcome.
Last friday I got the chance to stick around in London and attend the Clearleft Ajax workshop with Jeremy Keith. 30 people from different web disciplines and backgrounds trying to get as much out of a single day on the latest buzz word has to sound like fun?
I had heard Jeremy’s speak before at @media last year where he was one of the highlights and friday was much of the same. In a good way. Clear, to the point, explanations. Examples where they help to make a point, some wry humour and the odd culinary simile. The only real let down there was he’s cut his hair short. This tendency for minimalism in hair cuts  amongst web designers is disturbing me. Only Norm can save us now.
Back on topic the day covered everything from theory and first principals to some fully working examples, but stuck with playing with Javascript and code from scratch, rather than frameworks and server side stuff. A descision I think really made the day useful.
The morning was, to be fair, less useful from my standpoint. The speed introduction to Javascript, and some of the concepts of Ajax where covering ground I’d already covered personally. That’s not to say they weren’t well placed or interesting though, far from it, and did provide some people with what was probably the best crash course in Javascript imaginable.
The afternoon really kicked into gear though. Lots of examples; from simple updating content through to making use of the Yahoo API. Throw in some examples with server side code, the obligatory shopping basket example and, particularly impressive, a detailed look at JSON. This all in one afternoon remember.
The code examples we got to take away with, and much of this was generic enough to have already found it’s way into my working library. I’m very much a fan of Jeremy’s DIY ethic with regards Javascript. I too am sceptical of using huge frameworks to crack nuts without a clue about what they are doing. The actual simplicity of the Ajax methods I think was one thing that Jeremy got over better than anyone I’ve listened to date. It really is about application design more than new, fancy, complex technology.
A few other people who attended have posted their opinions .I’m really not stalking Molly by the way.
As Molly had hinted, Jermey is also very quotable. So here are a few of the things I’m sure he said, any loss of context here is completely my fault though:
We closed up the afternoon with some discussion of the accessibility concerns, some of the development issues (wireframing ajax apps anyone?) and a whistle stop look at frameworks. All in all a very useful day, both personally (ie. I had fun) and professionally (I think my employer got value for money.) It also reinforced what I’ve been thinking more and more recently. It’s nearly all about design when it comes to real work. When and where and why must come first, and how is often more trivial than you think.
Some people think that programming should be hard while others, like me, quite like the idea of anyone being able to build things, especially web sites. To me the fact that either of my housemates (non IT types) could probably build a website this evening if they so choose is great. And to be honest if they did I’d be unlikely to complain about the code or the programme they chose to build it with (Ok, I would, but that’s me and I’d like to pretend I wouldn’t.)
It’s pretty unlikely however that either of them, or any beginnner, would go along the route of choosing say ASP.NET or Ruby on Rails for that first site. Which leads me to some sort of assumption – that both of these are for experts and professionals. There are other facets which could be used to back up this argument but I dont want to go on – rails making use of the command line (in very cool ways) or ASP.NET requiring IIS configuration for a start. Not something I fancy talking non experts through to be fair.
Using that assumption brings me nicely to helpers. Both are frameworks that try (and in may places succeed) in making life easier for developers. This is a good thing. What’s not so good is where this goes too far, in my opinion. ASP.NET linkButtons are simply Evil. No questions. At All. Stop using them. Now. Rails wades in as well with it’s reliance, when it comes to certain controls, on inline Javascript. Did someone not get the memo?
As good web standards people we have divided content from presentation. Behaviour from both (sort of). Now we have great (in both senses of the word) frameworks coming along and riding roughshod over our labour of love to make the web a better place. Making the developers life easier, but impacting negatively on the end users. Such wide ranging and powerful tools for experts need not treat all users as experts in everything. ASP.NET (or Ruby on Rails) does it so it must be OK? cannot be allowed to undo good works.
Lots of similar talk abounds at the moment, with certainly some light in some of these areas showing through. Visit Dan and Jeremy for more.
end Rant – mmm Rails
Well, yesterday was the long awaited Carson Summit which seemed to go pretty well all in all. Lots of reviews and notes around at the moment but I’ll just throw a couple of my personal observations into the mix.
The presentation where all good, but nothing mindblowing. Most had something going for them; whether enthusiasm, numbers or a special announcement
I liked Tom Coates diatribe about clean URLs, it something I’m fairly keen on too, I’m uming and arring over whether I too am a URL Fetishist
The Google vs Yahoo show was entertaining and interesting. Reading that Google tend to be more academic with Yahoo being a little bit more unhinged/entreprenurial is one thing – going to a conference where the guy from Google came across as something of a mad german professor with something of an axe to grind, compared to the Yahoo mob who ranged from a self confessed hippy to Cal and his three quarter length trousers. It’s appears all the rumours are true.
I did a couple of stints of SubEthaEdit collaborative note taking with Simon which was fun and interesting at the same time. The who’s going to write the next part bit was quite amusing – at times writing the same things down, then everyone deleting them, then a pause.
The after show was, as always, good fun and fairly drunken. The pub was nice, but whomever suggested it on the wiki was playing a cruel joke. A conference with 800 geeks and a pub about 3 people wide at it’s widest point is not sensible. Neither was Patrick’s suggestion about not eating, but it all worked out in the end.
Was good to meet up with lots of people, both those I had met before and those that I hadn’t. Hope to see everyone again soon, if not it will have to be @media.
Not so subtle hint in the subject line, but without further ado, Molly (yes, that Molly) is coming to Newcastle for a talk.
All the details are still being confirmed but we are talking:
15th February
Room 149 Northumberland Building, Northumbria University, City Campus, Newcastle, NE1 8ST
5-7pm
After talk drinks and food to be confirmed. Probably going to be along the lines of “Webstandards and Usability” with some time for a Q&A session as well.
Big up to Tom Simcox here. He’s been threatening for a while and with Molly in the country she’s agreed to come and visit little old us. Mark (of sunderland uni and NEUA fame) also sorted out the venue, amongst other things. Good job everyone.
We need people along for this so tell you employers, whoop to your colleagues, threaten you employees and be there.
I’ll post more details as they are confirmed. If your from outside the region and want to come along let me know. It’s short notice but sometimes these things are – and you know you dont want to miss out.
To help us gauge interest a little it would be grate if you interested  to leave a comment below. If you cant come along because say, it’s your birthday, then what questions would you like asking?
Of late I’ve found myself making quite a few smaller purchases of software. I’ve always been a fan of Open Source (and before that shareware – mainly on the ZX Spectrum) and still use lots of open source apps (anyone say Firefox) but for some really specific jobs it’s nice to have something you know is maybe a little more polished.
I dont know if it’s just me getting older and having more disposable income, or a greater appreciation of what it takes to make really good software (probably a bit of both) but it’s sort of snuck up on me? Anyone else finding the same? I know Jon has been mentioning some cool new apps he’s using. What about everyone else?
Recent purchase have included the following, nearly all bought with PayPal as it happens:
I’m also evaluating (and at the moment being impressed by) a few other apps. Upcoming purchases probably.
On a blog related point, I’m aware I’ve been blogging more technology related stuff of late. I’ll be back with alot more web design and development banter soon, and some things happening up this neck of the woods (including a revamped newcastlenewmedia site hopefully) – it’s a matter of having the ideas but not having the time! Hope it’s still all interesting to everyone.
I’m quite a numbers person and have something of a penchant for statistics  As if you couldn’t guess from the site name. So it is with Google (who also have a mathematical inspired site name). But unlike me they have more people (it’s just me) and lets face it, more computing power (only so many computers I can get in my house without getting told off).
So they bring us something of a large, automated, study (including some decent analysis) of what markup is actually being used, interestingly also including common class names. I’d love to do something like that – over 1,000,000,000 sites analysed!
On a standards front seems to be good news and bad news, some of the common classes are things like:
Most of those I can see use for. They Mean something for a start. Although some of the other top entries included things such as:
Which maybe aren’t as well chosen.
On the other hand there are still lots of weirdness, proprietary   tags and bizarre attributes. iWeb might be using crazy nested divs but it’s still valid.
Being Google it’s all written fairly amusingly, including a few digs at competitors with applications which generate nightmare markup. Overall, a good read. There are probably a good number of other useful nuggets of information burried in the document as well. Anyone coming across anything of particular relevance leave a comment.
Help! I’m not sure what to do? I appear to have been set upon by nearly published author simon in an attempt to spread a deadly meme. Oh well.
Here endeth the lists. I’d be intrigued to see a rather cumbersome tree diagram of this meme thang in action. You know, track it back to it’s source, looking at relationships and all that jazz. But it’s late and I’m tired.
I’m something of an avid reader and, making use of the nice Amazon API I’ll try to keep my reading list up to date on here. I’m just including books that are maybe a little relevant to the web – otherwise this will get out of hand. I’d like to include some brief reviews if the time ever comes along, though dont hold your breath.
Like probably quite a few others I’m doing more Javascript of late, both professionally and at play. It’s not just the whole buzz around it, all the way back to Jeremy using Javascript for Good not Evil but I moved away from doing backend development when I moved jobs, Javascript is most definately in my client side job remit.
As an avid reader I’ve got hold of  a few good quality Javascript books that have come along and was on the lookout for more when I came upon Foundations of AJAX from Apress’s black and yellow experts voice series. I haven’t read enough to give a full conclusion but I’ve been impressed with other books in the series and it looks promising so far. A few chapters did stand out (I jumped in and read them first) that had little to do with AJAX, concentrating instead on setting up a proper Javascript development environment.
I’m not a fan of bloated IDEs so the lack of one for Javascript doesn’t bother me as much as it does some. However debugging by browser is a pain for simple scripts, never mind larger applications. Firefox’s Javascript Console is mighty handy here, as is the DOM inspector and the view rendered source extension. There is also the Windows Script Debugger but I personally haven’t had much luck with that as yet.
Projects should have documentation and code should have comments. But it’s time consuming (honestly). Automatic generation of docs from source comments both makes you comment more efficiently and comprehensively and saves time. Everyone’s happy, at least as long as they are using the JSDoc perl application to do just that. Anyone familiar with Java will have come across JDoc and the name isn’t the only similarity. It’s now installed and ready to be used next time I write any Javascript.
Another area of interest recently has been application design, analysis and methodology, so I’d come across the idea of test driven development before and experimented briefly – but not with Javascript. The idea is that you write tests that can be run automatically on your finished code before you write the code, it makes you design more which cant be a bad think. Anyway, Javascript has JSUnit, a nice unit testing framework. Again, installed and ready to give it a whirl.
Hopefully this brief list will prove useful to anyone else making more use of Javascript recently and looking to be more productive. Anyone else with any other useful tools do post a comment, or is their a mythical application that you just wish existed to help you with your Javascript woes?
I’ve quite a music fan, I used to be more so but have been catching up recently with various bits and pieces. I tend to stick with more web centric posts on here but, while loading lots of music into iTunes I thought what the hell. I’ll touch on a few points of web goodness just in case.
I’ve noted quite a strong link between music and web standards style blogs for a while. Lots of end of year posts touched on music and simon even has his music monthlies My guess here is that the amount of time spent on the computer (both in and out of work) and a tendency to be male tends to lead to some musical interest or other. Having said that alot of the same bands seem to crop up as well. It seems to be lots of indie, newish brit popesque, garagish bands. Guitars. Men. That sort of thing.
A number of other instances spring to mind. A discussion a long while back on designersinhouse about listening to music at work, Jeremy Keith’s in a real band and visiting a certain Rock night after an accessify meetup with Patrick a few months back.
Where is all the music related web software produced by all of these people though? last.fm is nice and I keep meaning to get into it a little bit more than I have. iTunes and Amazon cater to most mainstream buying tastes but from very much a mainstream angle. I’d love to see, or get involved in, something that combines some of the cool aspects of flickr and del.icio.us et al with music. Time to come up with some ideas, or look around for interesting new sites me thinks.
Oh and while I’m at it the new music I’m listening to at the moment includes:
Sorry, the about the last bit. It’s quite cathartic listing my musical leanings on here. Not sure why but I’m in good company as already noted. What music, or music web tools, do you swear by? Please note that I reserve the right to edit posts which seem to advocate music I dont like. You probably know who you are.
More of a statement of intend, for posterity’s sake hopefully. Something I can look back on and think “Oh, yeah, I meant to do that” later in the year. Also so I dont forget, or just decide I couldn’t be bothered. Why this all of a sudden? Well, it’s the start of the new year anyway, but in particular the first meetup of people from the Newcastle New Media list got me thinking. And in particular more than thinking, wanting to act on existing thoughts.
So, without further ado, here is a brief list of thinks I’m going to do this year – fingers crossed, in no partricular order and missing several things that no one reading this will care about:
I may have forgetten a couple of things, I’ll add them as I see fit. Anyone else got a list that they would like to share? Either on here or post a link to your own site.
Of late I’ve got round to installing a few widgets on my computers – Dashboard on my Mac and Yahoo Widgets on Windows (no linux widgets as yet). I’m actually still running OS X 10.3.9 and using the very nice Amnesty to allow run to Mac Widgets.
I’ve been meaning to play with widgets for a while, mainly because I’ve been using HTML, CSS and Javascript for years and widgets simply reuse these technologies – making it easy to just jump in an make something handy. I’m also a fan of web services in general and  widgets provide another handy interface to many online services.
I’ve mainly been using the Junior Mint widget (and Minty) for keeping an eye on my site stats, I think I mentioned before I’m hooked). I’ve also been using WiFi monitoring widgets, battery monitoring widgets and looking at the backpack widget which looks nice.
As for having a go at developing them – I just made available my first go, a widget to keep track of the @media2006 feed agregator I have built. It’s relatively simply – it polls the site for a couple of variables and displays them across a couple of panes. The numbers in question are the total number of posts, the number of new posts this week and today – as well as a countdown to the event. I had to do a little work to expose these properties in an XML format from the site but nothing major.
I learned by looking at a couple of tutorials as well as existing widgets. Disecting code prooved the most useful – the majority of tutorials seemed to provide good Hello World examples but you could probably guess most of that. If anyone has any good intermediary or advanced widget tutorials then let me know.
The main problem with the development process was definately the iterative testing. With web pages I tend to work in large steps (with very little testing) early on and then move to more small changes and quick testing later on when it’s down to the details rather than the brush strokes. I’d definately echo the sentiment of others by testing in safari (it uses the same rendering engine) first, and also add that getting all the markup and CSS in place before moving to Javascript is probably the best bet. You really need to test the Javascipt with a real running widget as it has it’s own functions available via the widget object. I’ve heard Apple may be working on a suitable IDE for widgets (my guess would be a tie in with Xcode) and I’ve come across Wcode which appears to offer some useful tools.
I’ve not yet looked into the situation with Yahoo Widgets – I intend to port my @media widget to Yahoo as an experiment and we’ll see how that goes (and yes I know that’s the wrong way round – Dashboard was, to some, a copy of Konfabulator, now Yahoo widgets. It’s early days I think for widgets – the move to more of a distributed systems feel, with open APIs and web services, amongst modern web applications would seem to be a perfect match for small desktop applications that tie into them. Watch this space.
Yes it’s back. @media returns as promised by Patrick last year and, well, it’s definately bigger. Two streams, more speakers and panels than you can shake a pink elephant at. I know for a fact that this was being planned even before Molly had left the bar last year and boy does it show.
The blogs are alight with the sound of people coming up with something, anything to tell their bosses in the morning. I’m sure I’m not the only one still awake either posting comments (Yes Zach, Patrick I do mean you) and making blog posts.
I’ll post more about what I want to see, how I’m going to decide which sessions to go to (I’ll need some sort of system – simply trying to decide which one’s will be best will only end in heartbreak) over the next 6 months or so. Yes it really is that far away. With the Carson Summit coming up so soon as well that should more than keep me going – especially with them having different emphasise (web standards/accessibility and web application development/design respectively) to keep thinks from getting boring.
And with all that in mind the reason it’s late when I post this makes sense. Yes it’s another feed agregator thing.
morethanseven.net/atmedia2006
The place is already starting to hot up and I’d guess that will only get worse (or better?) over the next few days. I’m glad to get in early this time. Last year proved useful (at least to me) in keeping up with the aftershow goings on with 150 posts from 52 feeds – but  I only got it together a week or so after the event, thanks mainly to Faruk for compiling the initial list of posts.
Any bugs or suggestions let me know. Also if anyone has a good name (feed agregator is pretty mechanical) then please do tell. I’ve added a few of the requested features from previous editions and it’s now possible to quickly suggest new feeds and to search through all the posts.
Anyway – Hope to see people in June. My predictions of a year of real world activity to mirror our enlightened online existence are coming to fruitition already.
Since the brief discussion based around ben’s original idea for using del.icio.us to track comments you have made on other blogs I’ve been having a play.
The main issue that people saw was simplicity, which I see falling under two headings:
It’s the later that I’ve been playing with and thought I’d present here in it’s early stages. The plan is as follows:
The bookmarklet section is simple enought modifications of the existing del.icio.us bookmarklets. See the download at the end of this post for details.
The intermediary page uses HTTP authentication to ask for a del.icio.us username and password (note that I’m not storing these at all, or for that matter at present checking if they are correct.) This needs only happen the first time you use the page if you use your browser to store the details or once per browser session otherwise.
The page then makes a request to the del.icio.us API using the lovely PHP library from dietrich.ganx4.com/delicious/
On a failed request the page reports that an error has occured (I’ll expand on this with more pertinent error messages as time permits.)
On success however we have two options, depending on the bookmarklet used. The default is to close the page down, with an alternative beiong to redirect back to the refering page. We do however run into a slight problem here with Firefox. Our favourite browser will only allow windows to be closed via javascript if they where opening by a script (NOT including our bookmarket). Internet Explorer has a similar set up where it prompts the user for an action, although a simple fudge get’s us around that one. At the moment therefore the close window bookmarklet is of limited usage for Firefox users.
This could quite easily be expanded for use on a blog as the requests are simple HTTP requests with query string parameters. However the security implications of giving out your username and password would likely limit this approach. I’m going to look into the sending of bookmarks to others that ben mentioned and see if that can be used here – as hopefully that may only require you to give out your username. A service like gravatar could then be used to do lookups between email addresses and del.icio.us usernames and away we go.
Any thoughts, comments or suggestions welcome. And feel free to try out the bookmarklets. I’ve included a handy download below including a quick readme and the bookmarklets.
download @Commented-on pack
Well, that’s nearly 2005 over and done with. Quite an eventful year all told and I couldn’t let it end without the obligatory next year post .
So without further ado, this year various things happened to me:
Next year my crystal ball predicts:
On less of a self centred trip:
All in all, I’m looking forward to next year for a range of reasons including the above. Hope to see (or meet) all of you (again) over the coming year.
Update I’ve updated this site to include a list of recent comments I’ve made powered by del.icio.us and @commented-on. A quick hack of the txt.icio.us plugin and some thinking on at rules for del.icio.us later and their we go. Expect more details soon.
Update Ben has posted an update to his post with a quick hack of the del.icio.us bookmarklet.
Ah Ha. Nice when you have a vague need and think about spending some time coming up with a quick solution, only to forget about it until you come across a solution.
I’m busy (ok sometimes) but only part of that is down to my blog reading habit. I only occasionally find time to comment and then often forget where I’ve commented and no doubt miss the follow up. If only their was a quick way of keeping track. Where did I comment and when? What if (strange, probably stalkerish) people wanted to know where I had been commenting and what I’d been saying? Well thanks to Ben we may have a solution.
Enter del.icio.us. Ben has suggested using the tag commented-on and who am I to argue? By tagging any page I make a comment on in this way del.icio.us will keep a record at <a href="http://del.icio.us/garethr/commented-on">del.icio.us/garethr/commented-on</a> as well as make a feed available at <a href="http://del.icio.us/rss/garethr/commented-on">del.icio.us/rss/garethr/@commented-on
Nothing on those at the moment but as I comment on things around the web I’ll try and remember. The whole flock thing would be particularly useful for this (at least once it’s a bit more stable) with it’s built in del.icio.us setup. A couple of apps spring to mind on that back of this, and playing with rss is still one of my fave things. A textpattern plugin (I’ve been meaning to at least have a look at building one at some point) or some social network app would be interesting. Watch this space (maybe).
Anyone else start using this technique leave a comment. I’d be interested to see it catch on.
Well last week or so the Newcastle New Media list kicked into life once again with a flurry of posts (some of a sickening nature regarding future rock star Nathan Hardwick). After a suggestion to have a look at meetwithapproval.com (more on in a moment) we finally got round to organising a proper meetup.
The date has been set as the 12th of January 2006. The location as The Bridge Hotel and the time around 17:45.
So anyone in the area fancy coming along feel free. The plan is nothing formal. Lots of people have lots of ideas from organising a full blown conference to a vaguely regular meetup. My feeling is that anything we can do to help build a sense of community can only be a good think – what to do with the website and education are on the top of my list in reality. The latter both in terms of learning from everyone else and hopefully giving back. I’d like to get some sort of response to the Local Uni’s web coursess as well, something me and Phil discussed a while back.
With regard to meetwithapproval we used it and organised something so there you go. A success. However it had a wide range of little bits that didn’t quite work or things that appeared missing – plus no documentaion I could see. I’m going to note them down here, but before I do I’d like to say it’s not in a vengeful sort of a way. I’d much rather be using web applications like this (in Beta if you must) now rather than the finished, untested things in six months. Ok, here goes:
As I mentioned I hope the people behind the app dont take this the wrong way. I’ll love to keep using it in the future and with a couple of improvements (especially the RSS and documentation) I can see a nice future. Maybe a nice API as well? Any other comments or suggestions leave a comment.
Anyone coming along on the 12th as well who feels like saying hi, or getting going early leave a comment too.
It’s nearly that time of year again where all of us working types get more than a week off work (the students amongst you are probably already on holiday, or at least acting like you are).
As well as all the obvious things everyone will be up to I always try and have some sort of computer project on the go. Something I can have a proper run at over the festive period. New language. Application. New site. You get the idea.
I’m intrigued to see if that’s just me? Is everyone else thinking “No computer for a week!” or similar? Or has anyone got any exciting projects on the go, and fancy letting us in on a little secret.
Needless to say I’ll have more on what I’m up to nearer the time but obviously it will start in beta and if I’m really pushing the boat out I’ll set up an AJAXified sign up page and maybe an invitation only period. I might just be mocking others here but maybe not, time will tell.
A while back I decided to jot down a couple of practical, simple techniques that have served me well when actually building fluid designs.
One of the situations that often comes up is the need to make an elements width vary. This is no problem when that element is a block level element (say a div or a paragraph) as unless you tell them otherwise their width willl vary with the space available. However, when it comes to graphics this can be a problem. After all, these will (probably) have been designed with a fixed size in mind.
Their are a couple of possible solutions here, both useful in different circumstances. However, both rely on the image itself to some extent.
The sometimes obvious method is using the CSS background rules, the image wont actually be part of the page and so wont take up any space, the size of the image can then be controlled by the size of the element (say a div) on which the background image is placed. This works but has a couple of ugly side effects – if the image is content based (ie. it carries some sort of important message rather than being just for it’s asthetic attributes) then placing it in the CSS removes it from the page completely, not good for those without CSS support. Another consequence is in site management. Having content imagery in the CSS can make life a little tricky.
The second approach involves using the CSS overflow property. By wrapping the inline image with a container (again, maybe a div) and assigning this an overflow: hidden we can control the viewable area of the image by controlling the size of the surrounding div (with CSS, natch). An example of this can be seen on these pages, with the favourite sites and flickr images. Resize the window. These images should expand to fill the available space.
OK, so we have added some additional markup to the page, depending on your feelings on this that solution may be fine. Want to go a bit further? What about assigning a class to the image and using some nifty DOM scripting to add in the additional markup?
Ok, so everyone by now has no doubt had a play with XML">AJAX in some shape or form, and their are lots of good tutorials and articles around. I’m not going to add to the pile, but on my travels I did do something that raised a couple of questions. Let me know if anyone has any answers.
The questions are more ethical than technological however and came about when considering online advertising and it’s never ending need for metrics – at the moment that generally means impressions and click throughs (maybe also including converting click throughs). These two states are imposed to some extent by the stateless nature of most web pages. We can tell what gets shown to the visitor. But they then disappear from our radar until they do something – in this case click on an advert (or dont).
But what about the asynchronous nature of our new toy AJAX? We are starting to break out from the constrains of the stateless web so can anything be added to our add example? Why yes of course it can.
I’ve set up a quick working example that should hopefully make my point a little clearer. Open in a new tab (I know 80% of you can – mint is great) if you will – morethanseven.net/monitor
Mouse over the two Ad areas a few times. Click on them a few times too. Obviously the appearing message telling you what you have done is hardly exciting but it’s actually a response from an AJAX (actually it’s not returning XML so is that AJ?) call. All the call does is log the action inn a text file and return a success or failure. This isn’t high tech stuff – it’s proof of concept remember, it gets interesting a couple of jumps forwards.
The two buttons at the bottom, clear log and view log, allow you to see the log of actions (and delete them). I dont imagine this attracting high but the log file will log everyone’s actions not just yours, so you might want to clear the log first to more easily see your actions appear.
OK, so what? Well lets imagine instead of just the actions I log your IP address, some date details, maybe store a cookie on your computer while I’m at it. Also as well as hooking up two add areas as in the example I hook up everything on the page. Not only can I build up a picture of your activities between pages (your path through the site), I can build up a picture of what your mouse was doing on a given page. Now where stalking.
OK, so I’m not sure if their is a correlation between a web users mouse pointer and what they intend but think of other examples – I could trigger the events on focus for form elements. You might get half way through registering or purchasing (you’ve entered your name and an email address) and then decide against it. Normally all I’d be able to get would be someone went to the page in question and did not complete the required actions – with AJAX I could log who you where and were you got to in the process (and then email you to ask why).
Ethics anyone? Anyone else see problems? Let me know.
Note. The javascript examples are not great and I’ve testing in recent versions of IE and Firefox only. It’s a proof of concept rather than tutorial.
Well – It looks like february has made a quick move to try and secure the prestigious month of the year award.
With two events happening in London, namely Clearleft’s Ajax Training Workshop on the 10th and Carson Workshops Future of Web Apps summit on the 8th it’s all go in the rapidly expanding world of web geek meetups.
After missing d.Construct I dont feel two bad about the outlay for going to both of these, especially with the assistance of my lovely employers. So I need to work out the details; travel, accomodation, what to do on the thursday, etc. but that should hopefully not be too painful. Any suggestions welcome – especially if anyone else is going along to one or both – be good to be in the same hotel as other attendess – I met Nic at @Media last year as she was in the same hotel.
So, anyone else going along? Want to meet up? Ideas for accomodation or things to do on the Thursday? See you in February!
Back from Manchester after a pretty good weekend all in all.
It didn’t start particularly well. Very briefly I was late getting to the station. Missed train. Waited an hour and the next train was delayed half an hour. Missed meeting everyone at the station and had no clue where the place was. After a trip (in the opposite direction to the pub) to the tourist information centre I finally got there and things picked up markedly.
Present where Patrick Lauke, Vigo, Andy Saxton, Jack Pickard, Pixeldiva, Robert Wellock, Dan Champion and myself.
We started out in a small deserted bar where Patrick had booked a table (including a reserved sign with bad fonts). Considering that we accounted for the vast majority of people in their throughout the afternoon this was mildy amusing.
I missed most of the introductions but not to fear. Conversation jumped around – people who made it to d.Construct filled everyone else in with the highlights and the behind the scenes gossip. Tales of massages and ladies mainly. We had a wireless laptop connection and browsed around a little. Bad examples of error messages (I’ll dig out the links later on), brand new public sector sites that simply consist of sliced up images (including links with blue underlines and ovely image maps) and goings on at the BBC. But we cant speak about the later. Some talk as well on privacy issues tied in with the whole open API goings on and an idea we intent to sell to google – stalking.google.com – which is fairly self explanatory but we got into a worrying level of detail!
After a brief break to stop off at the hotel, and for some of the group to depart, we got some Indian food on the way to a pub recommended by Patrick, who was going to meet the rest of us there. It was a ROCK pub. Rage on the jukebox, lots of proper beers, people wearing black, etc. All of a sudden we found something else we had in common – musical tastes. More discussions going backwards and forwards between the music (what’s that tune?) and the state of the industry. All good fun.
Overall, another enjoyable meetup. Thumbs up to everyone turning up and I’ll look forward to the next one. I’ll post links and the obligatory flick details once everyone else has their say.
I’m not going to prattle on but something that’s bugged me for an age. The usage of the word “Creatives”. And lo and behold it bugs other people too.
I like knowing that it’s not just me!
Just to say as well that anyone reading this and going along to the Accessify Meetup in Manchester this weekend let me know. Leave a comment. Be good to put names and faces to email addresses and web sites!
Right. I wasn’t at d.Construct I’m still a bit gutted really as it sounded like a hoot and lots of it was up my street and would have been good to meet up with ‘people’. But hey, I can still watch the feeds pour in right?
With that in mind, to again make my life easier and hopefully be of some use I’ve set up another (the first one was for @media) feed syndicator.
morethanseven.net/dconstruct
All I’m doing is parsing, storing and displaying all the feeds. On first run I picked up 27 and hopefully by the time you read this more will have been said. The collection of feeds are a combination of people I’ve met and people who where blogging about media. If you have a feed and want it adding just leave a comment on here or drop me an <a href="mailto:garethmorethanseven.net">email and I’ll add it to the rest.
I’ll just say a big shout for everyone at Clearleft for organising it on such short notice. I’d rather thinks like this happen and me not be there than they not happen at all because of problems with size or cost or admin. Maybe next year?
After getting a few comments about the site design, and in particular a few of the fluid nature comments about it being tricky to get right. I’ve found myself tending to develop fluid designs more often that not over the last few years and thought I’d do a short series of posts about some of the techniques I use. I’ve generally found it pretty easier to find discussions about the pro’s and cons and the basic principles but less so on actual techniques. I’ll try cover some or all of the following over the next few weeks or so:
I’ll kick off with tricks for background images.
This techniques makes extensive use of the background CSS property which is used as following: div#iBackground { background: url(path/to/image) center top no-repeat; }
I’ll assume some familiarity with this and CSS in general, basically all we are doing is setting a background image to the div with an id of iBackground which is set to align to the top vertically and centered horizontally.
Moving the image to the background has a couple of interesting effects, the one that we are interested in here is that the the size of the div is not affected by the size of the image (as it would be if the image was placed inside it). It acts as a kind of viewport into the world of the image. By specifying the width of the div in percentages (or em’s if you are dealing with an elastic design) the size of the viewport can change dependent on the size of the browser window. An example is inorder:
joshuaink.com makes use of a large background graphic that stretches the full width of the site. This means the space can be used for pretty large imagery but without breaking the experience (sideways scrolling!) for those with lower resolution displays. Note that the flower background is 2000px wide.
The only problem with this technique that I have come across is generating the images in such a way that they degrade gracefully. You need to make sure real content (logos or text) appear in a smaller area that the whole image and live with the fact that not everyone is going to see the whole thing. In my mind this is a small price to pay for asthetically interesting fluid layouts.
Well. Lots of goings on since the reboot. Rather than fill up lots of posts I thought I’d summarise – If something is interesting enough I’ll probably come back to it.
Thanks everyone for positive comments about the redesign, lots of new visitors to the site from the reboot site itself and people who liked the design and said so on their sites:
I even got onto screenspire which was nice.
All of this I’ve had the joy of watching from the vantage point of mint. I mentioned that I’d installed it a while back, as much to have a play around as anything but as many before me had stated, you can easily get hooked on the stats – and I’m enough of a numbers fan as it is. So what follows is a very brief, but real world opinion, of Mint. In case your only interested in the final opinion – I’m a fan (you could tell that anyway).
First things first, purchase, download and installation was an absoolute breeze. OK so I’m fine with all kinds of command line driven installs but their was non of that anyhow. Just change a simple text configuration file, create a database and upload. Everything else was just point at a URL and away we go.
I decided to install a couple of the official peppers (plugins) along the way, again a simple matter of upload, login and click install.
Login presents the nice interface everyone was googling over. It’s all nice shades of green, shadows and javascript scrolling. The pane’s themselves provide information about Visits, Referrers, Pages and Searches. This isn’t all the information you might be used to from other log file based packages but once you get over that you dont really need anything else under most circumstances.
One thing that did interest me was information about screen resolutions and that’s where the User Agent 007 pepper comes in. Displaying information about screen resolution, along with details of Browser and Flash versions. It’s all very voyeuristic I know but hey.
The Visits and Referrers panes are probably where most of the action is. The Referrers section is great from my point of view for seeing who is linking to me, really useful for tracking down interesting people I have no other way of finding out about. Visits details lots of information about the number of visits and visitors – cunningly logged using javascript so as to avoid lots of referrer (ie. spider) spam. Having said that, Google cropped up in my User Agent 007 pane so maybe Google triggers it too?
So. Is it for you? I get the feeling that it depends. It’s come out of the blogging scene so to speak and it shows. Design decisions have been made that strengthen it immeasurably for blogs while, in my mind, weaken it for other kinds of sites – in particular from my past experience for campaign style sites where you really want to look at this sort of detail.
For example the choice of PHP, how many well known blogs (at least in the corner of the web inhabited by web designers) dont use PHP? So for blogs that’s not a problem but for other sites it can be.
Another area is the ability to query the statistics. For a blog you just want to know traffic all the time probably. For a campaign you want to know what happened when. And that might mean going back in time so to speak – when putting together a report for instance. It might be possible to mine this data, even build a pepper around it – but I’m not sure that’s playing to Mints strengths.
As I said initially, these aren’t really critisisms. Specialist software is where it’s at in my mind. And this is definately a very nice piece of software.
The real CSS Reboot should be going off anytime now. But I’m in Rome (this message is coming at you through the magic of Textpatterns publish on date facility) so had to put things up a little prematurely. Oh well. In fairness I know which one I’d prefer to be doing.
I’ll miss the initial rush of running through lots of screen shots and sites, finding new things and generally wanting to redo the design at least for a week anyway. When I’m back I’ll post a list of one’s that caught my eye.
In the meantime a couple of other ongoing bits. I’ll try and sort out lots of photos onto Flickr when I’m back, hopefully including some shots from Rome and a few other series I’ve taken and not got round to. Also after meetups, lots of discussion on and around newcastlenewmedia.org hopefully a sense of renewed focus and some hapenings around that.
All in all I’m sure I wont be missed, but enjoy the reboot. If you have any personal favourites then leave a comment. If you have opinions about mine then leave those too. Especially if they’re nice.
UPDATE Well. Five people seemed a good number on the notice and the size of the table we got at the Forth. The trick of leaving a copy of bulletproof web design casually on the side of the table worked a treat.
Lots of hello’s, some vague recognition and lots of shared experiences and good ideas floated around. All in all a positive thing I though and hopefully the first of similar meetups in Newcastle.
Hi to, in no particular order:
The accessify forum meetup is tonight was on Thursday at the Forth for anyone who doesn’t know about it and is around the area. Looking to be a small number turning up but should hopefully be good to meet new people. I’ll post more details on here, and any photos (no photos, sorry), post event.
What do I want out of it besides the beer? Hopefully some ideas and enthusiasm. I keep meaning to do more with newcastlenewmedia and a few people have expressed an interest in pulling some things off – the mailing list even kicked off yesterday with a flurry of posts. So some concrete ideas, a broader network and some beer will do nicely.
If you want to talk to me about, well anything really, let me know. Your best bet is probably emailing me on gareth@morethanseven.net and hopefully I should get back to you. Unless you’re a spam bot in which case expect to be automatically deleted or ignored.
On the other hand I can be found wandering the corridors over at designersinhouse.com or appearing in person at anything I can get along to that sounds interesting.
I’m actually going on holiday soon and seem to be getting all sorts done before I go away. The obvious one is the change of scenery around these parts. Let me know what you think. It’s not complete as yet and I have a few more bits to add and check (yes I mean you IE) but I’m all for release early and often.
I’d been meaning to move on a little recently, and the move to textpattern cemented that. I’m still impressed with it and wanted to delve further into setting up flexible forms and pages. I’ve set up a few areas of the site to hopefully make me organise things a little better – specifically an experiments section to play with code and an articles section if I every actually get round to writing something properly. Both appear in the sidebars.
Another reason for the sudden change was, after using ShortStat for quite a while and looking over the Mint information on the haveamint.com site I made the plunge. Some had moaned about the cost but really Ã‚Â£17 is the price of a DVD and with a DVD you dont get the sense of supporting a really smart developer. Ok so I could have seemlessly moved from one to the other without a change in the design but I didn’t want to (confuse my database).
I’ll hopefully post a review of sorts (better late than never) as the dicussion doesn’t seem to have died down yet. I’d personally like to see more of this sort of small, clever, personal application development. I’ve been a bit Open Source advocate for a good while which has it’s merits but I’m still of the mind that something as focused and polished as Mint is more likely to come out of one mind. The question is, I guess, about the market? If you’re friends blog to a huge number of other web designers, bloggers and developers then you have an in on the promotional front but will that become the norm rather than a very clever marketing ploy?
Also on the paying part, lots of people thought $30 was quite steep, I disagreed but will we be back to Shareware and people not paying? Lots of questions. Not many answers yet. But should be interesting to see where things go – at least for people like me with an interest in the sociology of software.
morethanseven is the personal playground of Gareth Rushgrove, a 26 year old web designer and developer living and working in Newcastle, UK.
Before bored people ask, morethanseven is a reference to an interesting essay The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information, by George A. Miller, originally published in The Psychological Review, 1956, vol. 63, pp. 81-97. It’s about people’s ability to remember sets of data.
All of a sudden it’s all about the real world. In fact that’s my prediction for 2006 – geeks moving out into real social activities (often involving beer) around blogs and mailing lists what not. Anyway…
After the Northern Geekender yesterday, which was a really good do for all those that couldn’t make it, I found out about another meet up even closer to home – in Newcastle no less.
So people over at the Accessify forum are organising a get together  on the 27th of this month. Things are coming fast and furious at the moment. I’ll hopefully be going along, although it starts at 5:30 and I finish work at 6:00 so might be a bit late.
I’ll post more details about that as I get anything, and I’ll post any half decent photos (and link to the inevitable flickr group) as stuff happens.
In something of a shock I’ve just ported the site (well some of it so far) to Textpattern. I’d alluded that I was looking to do something similar (I mentioned a crazy Ruby fueled idea, but strayed away after having a run in with fastcgi) a little time ago but have finally gotten around to it.
The reasons where many fold, but mainly came down to wanting to do other things that write content management applications. I know I can do it, and it’s certainly not a solved problem, but it’s not as much a part of my job any more – every few months I was wanting to go back and rewrite something fundamental (or everything) and it was getting in the way of me doing other, hopefully more interesting and useful things – like Newcastle New Media , Fluid Flash or playing with whatever buzz word is in this week (definately Web 2.0).
So after what was quite a brief look around I settled on Textpattern mainly for it’s positioning. It is a blogging ap, but has lots of other features that allow content management of other elements – hopefully useful for side projects and the like. It’s PHP and MySQL so I can rip it to pieces if I want. It’s mature, stable, feature rich and does, with plugins, everything I probably need – the big question will be if it does them in a way I like, so far it’s thumbs up. Oh and John Hick’s has written a number of really good articles on textpattern, including a migration piece which gave me some pointers.
I’ve ported recent posts and comments, I’ll move the rest on an as time permits basis but, after some mod_rewriting mad skills I’ve kept the old site fully functional (no link rot – yeah).
All in all it probably took me 4 hours, in a couple of sessions, to get all the styles into textpattern, set up a few plug-ins (del.icio.us links, live previews – whoo), port the latest content and move from my development box to here. A pretty good testament to the ease of use of textpattern. If I had not had the original design or content I would have been up and running in minutes – the installation process is smooth as a new cuddly toy.
Their are a couple of changes that hopefully only the anal will even see. I’ll try and resurect some of them, probably in a more satisfying manner that before.
For the moment I’ll keep the existing rss feed up to date but only for a little while, the new textpattern feeds can be accessed from the navigation menu, and now include an atom feed as well (basically because I can).
In conclusion, anyone looking to move, or start, a blog style site with other ambitions then textpattern could be what your looking for – although some PHP skills wouldn’t go a miss.
Anyway, here’s to a more organised life. Anything fundamentally wrong let me know!
Peter J Lambert of Pixelicious fame has had a great idea. At least the best idea I heard today. Pretty short notice but if anyone fancies a trip down (or up?) to York for something of a geek get together let me know.
I can’t say I’m definately going as I just found out but decided to spread the word anyway – and with any luck will definately try and make the trip. I had been feeling (and I think I even complained once) about the lack of a sense of Northerness (not that southerners, or Scots for that matter, aren’t nice people or anything but…) amongst web design. Real Ale, Grit, Coal, that sort of thing – you just dont see that much web design that looks like it was inspired by that.
Anyway, the 15th (Saturday) looks to be the date of choice. Head over to pixelicious.co.uk to express an interest or complain or find out more information.
If anyone from around Newcastle does fancy it let me know as well – I’d probably be even more inclined to go and meet strangers if other strangers (or Chris) who live and work near me where going.
After complains of the lack of a decent magazine, and after the passing and resurection of Design in Flight another one raised it’s head. Yeh. Have a look at treehouse.
Another great, low cost ($15), well designed publication that you can print out and read in the moments away from the computer.
OK, so I haven’t read that much yet but what I have was both interesting, informative and hey, pretty well edited.
Thumbs up to any and all involved.
Disclaimer: I went to uni but not to study anything even remotely to do with the web. I basically make everything up as I go along. Bear that in mind when reading my commentry.
Phil (xlab.co.uk) just made an observation regarding webstandards and education that I thought particularly interesting. Are kids being taught web standards?
Based on my experience, limited as it may be, I’d have to vote no. I’d maybe even go further – are kids (sorry to any young readers, I’m just going with the flow) being taught web design?
Web design, at least to my mind, involves a wide range of disciplines – Human Computer Interaction, Information design and theory, Graphic design, Application design and development and so on and so forth. Without a general appreciation of all of these you are not, again to my thinking, a web designer. That isn’t to say you aren’t a fantastic Graphic Designer or what not, but a web designer is simply a broader disciple.
Back to education. And teaching. Now to teach such a rapidly changing medium? A course, I would imagine, needs planning over three years. The problem their is that in web design and development three years is an awful long time in terms of best practice and up to date thinking. I want to work with people who know AJAX, real world CSS, semantic thinking XHTML and unobtrusive Javascript. That sort of course simply could not have been written 4 years ago. And writing it now will leave it dramatically out of date in 4 years time – when all we’ll be wanting is more Python Programmers (or something)
In a sort of unlikely to get picked up on sticking neck out statement, I’d say I’d love to help out. I see the only option is getting the local web industry involved in education. Seminars, chats, meet-ups, anything that get’s those in education nearer to the reality (yes – you do have to consider users other than your mates, tutors and design magazines) the better.
Anyone else out their think the same? Willing to get involved? Maybe the time is now?
Leave a comment here or on phil’s post or post something on your site and lets discuss the issue. Does anyone else feel strongly about this? Even as someone not involved in the recruitment side of things directly at the moment I still want to work with the best – and in my mind so should everyone else.
Ok. I just came to a realisation. I think somewhere along the line I got addicted. First I tried one. It seemed pretty good. Not really having any effect on my and generally just being good to feel part of something.
But I couldn’t stop at one. I thought I could handle more and more. So I went in for some of the hard stuff. And lots of it.
And look at me now. Typing this while waiting for more email that is reasonable for a small company in a week to download because I didn’t check my email for a day or so. Time for rehab I think.
I dont want to chuck the habit completely, just pair back my addiction, get some control. So who stays and who goes?
DiH (designersinhouse.com) is certainly leading at the moment as a keeper. It’s proved useful, interesting and informative so far – plus I met Matt at @media.
I think maybe their’s room for one more. CSS-d, WSG, thelist, webdesign-L your time is coming.
I’ve been a (distant) admirer of flickr since I had a brief play a while back. Nice interface, good idea and all that. Well I finally got round to setting up a proper account.
I’d recently got myself a reasonable digital camera. Nothing expensive or fancy – in the past whenever I’ve thought about taking photo’s I’ve always got bored and given up. This time is going to be different. Hopefully.
I’ve only really got round to one proper shoot and in non ideal conditions (well, in terms of photography – basically in a room under a pub) but I’m pretty happy with a couple of the photo’s. But maybe that’s because I like somewhat abstract orange/red shots?
Anyway, hopefully I’ll get round to taking and uploading some more shots. And maybe using some of them on this site.
I all one for nice url’s, that is removing all the  gumf involving question marks and ampersands and the like for something that is both shorter, more human readable and more search engine friendly.
Doing it in PHP is actually pretty straightforward once you get the hang of it and can be pretty powerful – so here goes with a short tutorial of sorts. It’s will probably be brief and make too many assumptions of the reader – any questions just let me know and I may even try and write it up properly.
The basic principle is to stop using the GET array (accessible from $_SERVER[‘GET’]) to make descisions with. The question then becomes what to replace it with. A simple class borrowed from the Sitepoint PHP Anthology book written by Harry Fuecks is at least my answer. The PathVars class does pretty much what is says in the name, specifically providing access to the path contents in the form of an array. I’ll provide a link to a code archive for this article rather than just copy and paste whole classes – download it now if you are interested in the behind the scenes bits.
So we now create a new PathVars instance like so:

<pre>
/* Pathvars object */
$pathvars = new PathVars($_SERVER['SCRIPT_NAME']);
</pre>

And we can now magically get access to each element of the path (separated by a /) from the instance variable pathvars:
$params = $pathvars-&gt;pathVars;
Or more correctly by using the method provided:
$pathvars-&gt;fetchByIndex(0)
Where the fetchByIndex parameter is the element you want from the array.
Ok, so so far I’m really just repeating the work of others, if you like this sort of thing I’d really recommend the book. The next step on top of this is however is to decide what to do with the information when you have it. The following shows a very simple example – potentially for use in a simple blog or news site:

<pre>
switch ($pathvars-&gt;fetchByIndex(0)) {
}


In this example where we have www.example.com/article or www.example.com/article/a_sample_article_name the article view include file is used.
What these include files then do is up to you, in my case each of them passes different parameters to a simple templating system which then outputs the page. Again, have a look at the code samples to see that in action.
Let me know if anyone found this interesting, useful or informative and I might post other titbits, otherwise it’s probably back to inane ramblings and misplaced conjecture.
Well, their are no doubt lots of typo’s on this site but I’m referring to the Ruby on Rails blogging application Typo (currently version 2.5.5)
So far I have to say it’s quite nice. I’m only playing at the moment – the site you are currently reading (for those of you actually on the site, and not reading the feed) is a product of my PHP skills. Some of it I like (ie. the bits I wanted to write), others I’m not too sure about and well, it doesn’t do anything I cant be bothered to write. Which up until not hasn’t been a problem. But I’d still quite like live searching, categories and tags and all that malarkey.
Browsing the usual supect sites the choice between using a bespoke system and an existing solution seems fairly split. John Hicks is always raving about TextPattern and saying nice things. You see lots of MovableType fans and WordPress fans but non to be fair I never really liked them. Maybe that was just me?
It might be that all, especially WordPress and MovableType are pretty mature solutions and that I like something to hack on? So a custom self built solution seems the way to go? Or does it? With everything providing very similar featuresets – and differing only in a few important areas building something bespoke means 95% redesigning and rebuilding the wheel and maybe 5% innovation. Which is cool as a learning experience but I’m now in more of a get stuff done mood of late.
I haven’t decided as yet one way or the other but Typo might just be the anwser. Small enough to hack, an excuse to learn Rails and featureful to boot.
A somewhat grandiouse title but hey, dont you just love it when things just work? A slow evening lead to following two tutorials at once, the outcome I now have andy budd’s fab iTunes playlists set up and I have rails running on my host. Woo Hoo.
I love working things out and generally dont mind things ending up not working even but sometimes it’s nice when everything takes only a small amount of time and effort and then works. A sense of acomplishment, and admiration, if you will.
Oh, proof of the rails pudding: morethanseven.net:3000/
WEBrick at the moment, FastCGI is for another day, hopefully soon. And before you ask, I didn’t just copy the source from a local install. Really.
The tutorial I followed for that I grabbed from xmlareas.com/ruby-rails-howto.html which might be of use on a range of reasonable shared hosting environments – though remember to remomve the temp directory as it will contain LOTS of files that may flag some hosting file limits.
Makes me want to write some sort of how to articles on some subject or other. Question on a post card is what?
So a big thanks for Andy’s rather geeky approach to cataloging music (my librarian housemate is going to love that) and to goldenratio (probably not a real name?) for the Rails article. Long live the Internet and all that.
I’ve been a practicing web designer/developer/whatever in Newcastle for over three years now and about a year ago I set up a mailing list to try and act as a useful resource, shamelessly stealing from brightonnewmedia.org
So was born newcastlenewmedia.org.
The grand plan was to find other people like me with an interest. Anyone I could learn new bits from and hopefully the utopian dream of everyone getting something out of particupating in a community. It’s been, I guess, something of a partialy something. Traffic goes up and down depending on, well I dont really know. Hopefully it’s been useful on occasion.
One issue was I never really promoted it. So, with a little time on my hands rather than redesigning this site (again) I did a little work freshening up the website, adding a showcase of local designers personal sites in the process and the plan is to promote it a little. Hopefully get some more people on their and in particular some students from the local Uni’s – quite a few of which teach something along the lines of web design but approach it in wildly different ways – something I might write about next.
Anyway, I guess the main aim is to get a snap shot of the industry and in particularly the people who make up the industry, in and around the North East. Are their people in their bedrooms going crazy CSS zen Garden designs? Are their any DOM Scripting leviathans? British web designers seem to be the cream of the crop, or at least make up a decent percentage of the cream (why do Standards based design analogies always end up being cake related?), but where are the Northern designers, never mind the North East.
Anyone else think differently? Let me know. Send me sites of designers and developers in and around the North East and I’ll post them into the newcastlenewmedia.org site and we shall see.
Lots of talk around at the moment about unobtrusive  javascript on the usual websites and discussion lists. However alot of people, maybe outside the inner circles, are still finding old school examples of inline code on out of date pages.
So here’s my tuppence. I guess the more websites that post new, up to date, examples and reasoning the better?
The premise is pretty simple. In the same way as controlling formating with tables and having everything being mixed up in one .htm file is frowned upon as simply being too much work – dealing with lots of inline snippets of javascript raises the same problems. Namely being time consuming to update and maintain.
The solution is to keep ALL your javascript in separate .js files, and that includes removing things like href="javascript:" or onclick=. The first question is therefor how – when I started looking down this route I found a simple answer hard to come by, having to look at more complex examples and reverse engineer them.
The following is the method I’m using at the moment. It may have issues I’m unaware – if your a javascript guru let me know. The examples are from the fluidflash page on this site, see it in action at www.morethanseven.net/fluidflash
We first I set up an initialisation function:
This sets the onclick event on the element with the id iNavLiquid that will trigger the function liquidChangeClass. Next we define that function:
The specifics aren’t too important to this example but you can see the liquidChangeClass function which is triggered by the onclick event handler.
And finally we need to trigger the init function when the page loads. Their are lots of ways of doing this which is probably a conversation for another time – I used a schedule function from themaninblue.com which you can find below:
So their we have it, hopefully a simple enough example of how to move to unobtrusive javascript. As most of this is generic and used all the time it’s far simpler that it first appears.
I’m doing a lot more IA stuff of late and being in a Microsoft shop that meant getting to play with Visio. And I have to say it’s a great tool. It’s pretty simple to pick up, powerful, lots of options and customisations and even better – simple to get to grips with.
Quite a bit of work I’ve been doing has been around process and specification. I’d read The Elements of User Experience a while back but have been getting more into it recently, and the visual vocabulary stuff is excellent. Have a look on www.jjg.net/ia/visvocab/ if your into this kind of planning. You can download Visio Shapes as well and then it’s just a matter of dragging, dropping and drawing arrows.
Another good resourse has been boxesandarrows.com,  a site I’ve read on occasion but found both more useful and more time to read recently. Documentation seems pretty key to getting projects working efficiently within team environments – and pictures say a thousand words (or something).
Their is nothing wrong with a nice header graphic. Clients, designers and customers alike love them. But I often see them used as an excuse for fixed width designs. Well no more I say (unless, of course you have a perfectly good reason for a fixed width design in which case you dont really need this technique.)
The plan was simple. Find a way of incorporating flash headers into sites using liquid or elastic designs in much the same way you might use lots of sliding doors background-image goodness.
A quick actionscript solution, courtesy of Mike Ord combined with some CSS and Javascript is presented for perusal on www.morethanseven.net/fluidflash .
This is a simple example, the header at the top is flash and writes to screen it’s current dimensions. Try resizing the screen (liquid layout) or text (elastic layout) and see for yourself.
I need a nice attractive flash movie to really show this off so anyone who wants to send something let me know – I’ll make sure to provide credit.
Any previous visitors will notice something of a redesign which is pretty much finished – although no doubt their will be a few tweaks over the coming weeks as time permits.
The main reason was to bring in a liquid design that will allow more space – something I’m quite fond of as an idea.
The default for web pages, before we add any styles, is a liquid layout – where the site content fills the window whatever the width. This should be ideal for everyone, whatever resolution or monitor (or device) screen size. Why then do a great deal of sites use a fixed width design, my previous design included?
Design expediancy seems to me to be the main culprit in my view. Not always I’ll admit – lots of blog style sites use a fixed width centered column in order to get a readable number of words per line (whether they know it or not), which was my original excuse I’ll admit. For most designers, and most people, it’s simply easier to think in terms of thinks that dont change. Using percentages (in a liquid layout) or em’s (in a elastic layout) simply takes a bit of getting your head round.
It’s the web, not print, and that is something that from my experience (second hand I’ll admit) is something that isn’t drilled home on the majority of university or college courses.
Attractive header graphics is something that lots of sites, in particular commercial sites, employ – and with good reasons. It’s a great way of getting across a brand ethos in an attractive and eye catching manner. It’s also sometimes used as an excuse to use a fixed width design – the question is why? Try resizing windows and text on www.vivabit.com, www.thebgroup.co.uk or www.capgemini.com .
One outstanding issue I see is using Flash in a similar way. With broadband usage, especially amongst companies, on the up and up, using Flash (and video) in a similar way becomes increasingly more attractive. So something I’m going to have a look at is how Flash and either unobtrusive javascript or sliding doors techniques can be used to get the cake (attractive, flexible layouts which use Flash – without breaking the layout) and eat it. More soon.
I’m something of a fan of mobile devices and really should do more mobile web stuff but hey, the support for mobile CSS is shody at best. Enter this article.
The idea is really pretty simple. All you really want when your paying for bandwidth on your phone (or you PDA connected to your phone via bluetooth) is the content. So lets get rid of everything (and we mean everything) else. The solution is fairly devious – using some PHP jiggery pokery, a sub domain and some text parsing.
Have a look at mobile.morethanseven.net for an example on this site.
In reality it’s a non ideal solution, but considerng it really does take two minutes I dont see that as a problem, and I think that was what the author of the article was getting at. We want to use mobile CSS but some browsers ignore it while others pretend they are grown up browsers and try and render the whole site CSS file.
I’m hopefully going to alter this a little, rather than removing all stylesheets leaving any  mobile media sheets intact. Assign these to the main site and we have an interim solution. Fancy devices that make use of the correct style sheets can use the main domain fine, others can get the same design by visiting the mobile sub domain.
For those like me behind the times, I was quite impressed to get passed the batton (thanks phil)
Total volume of music on my computer
    19.51Gb
Last CD I bought
    Foo Fighters – In Your Honor
Song playing right now?
    Well the last song I listened to was The English Motorway System by Black Box Recorder
5 songs that I listen to a lot or mean something to me?
Note that that list probably changes every day but hey, that’s for today. I quite like Simon’s music monthlies idea but dont know whether I could ever really get round to it. Oh well.
Been a little busy with the new job but had chance to do a few behind the scenes bits and pieces, including:
Should be back with more soon. I need to reply to phil (xlab) about my musical preferences, jot down notes on using NewsGator as an online feed reader and a web standards redoing of the QWikiWiki output layer that I’m part way through.
[UPDATE] Visiting the website I now find out that after the next mini issue it’s all over for Design in Flight. I can see how it could take over someone’s life considering the quality but I for one would have paid more to keep it around.
I’ve just got round to buying all the back issues of Design in Flight (www.designinflight.com) magazine – which features articles on design, web development and similar fields.
For only 10 dollars (via Pay Pal) for the 4 issues it seems excellent value at about 150 pages or so, brimming with interesting information.
I’ve obviously not read that much as yet but so far so good and for the price you cant really go wrong.
After a little work I’ve put together an app to keep up to date with all the news around @media. Based on the collection of feeds from kurafire.net it collects and filters for posts for @media references and displays them.
I’m hoping it will proove useful to anyone else who wants to keep up to date. Especially when details start coming up about next year and in the lead up to that.
Have a look at morethanseven.net/atmedia. If you have any more feeds just let me know – I want it to be as extensive as possible.
I now have an ASP.NET environment setup and ready to go and the will to use it. Depending on how it goes I’ll try and post any thoughts as I go. In particularly related to how I get on with good standards markup.
I can knock together whatever I want in PHP reasonably well and with my new job focusing on CSS and XHTML amongst other related bits, and working at .NET shop I thought I now have a reason to play.
With all the goings on off the back of @media its pretty hard to keep up. Many of the ideas that where discussed seem to be taking hold and coming into the world and lots of sites have had some recdent TLC. Anyway, here goes with a few observations.
Simplified Standards
Simplifiedstandards (simplifiedstandards.com) looks like a great idea. A few people where discussing a central repository for concensus on issues of web standards application in the real world and this aims to be just that. Simon Collison of www.collylogic.com is bringing everything together as far as I can see.
Everyones thoughts
Faruk AteÃ…Å¸ appears to have spent every day since getting back compiling a list of what anyone anywhere said about the event. Its a great idea and makes finding peoples thoughts easier but I wonder if their would be an easy way of doing this (like pinging a web service with any new articles?) without one guy doing all the work?
Look for everyones comments at kurafire.net
Designersinhouse
One guy I did meet at the conference was Matt Patterson who has set up a small (but radidly growing) mainling list over at www.designersinhouse.com aimed at designers working within larger organisations. One thing I took away from the conference was the large number of people working outside service or agency environments.
And that is just some of the goings on. A good amount of discussion going on around the level of legislation needed to govern accessibility doing the rounds, the ATF (Accessibility TaskForce) from WaSP just starting up with several of the speakers on board and god knows what else. Oh, and I still haven’t done a zoom layout but I did add some word completion to the search box!
I’ve just install the ShortStat package from www.shauninman.com   and the Mac OS X widget from www.keeganjones.com/widgets/shortstat/ to go along with it. My desktop now tells me how few people are visiting my site!
It’s the first really useful widget I can see myself using for more than pure eye candy.
And ShortStat itself seems to be a very nice addition. Simple to set up and the output is nice and clear.
To both the people visiting the site – dont worry I wont use your browsing habits for anything other than improving the site!
It’s official. It’s DOM Scripting.
I’m reading the DHTML Utopia book at the moment (excellent so far) and following on from @Media it does seem to be where it’s at, at least for a little while.
The discussion of naming raised it’s head and I kind of agree – AJAX has buzz word written all over it. Read the full article at quirksmode.org following on from the Javascript get together after @media (darn train tickets).
If CSS is too much for designery types just wait… CSS, without its programmatic concepts (variables? arrays? etc.) is one thing but Javascript is, lets remember, an Object Orientated scripting language. Interesting to see who ends up doing this sort of work in small companies with limited people.
Sorry for the title. As a few people already know I’m moving jobs at the end of the month to Th_nk. Basically to do more web design with the emphasis on XHTML, CSS and the like.
I’m not going to go on about it as I’m trying to stick to issues on here about web stuff rather than about me but thought it worth a post.
Wow. I’m still pretty wired after two days of conference and several late nights but boy was it worth the effort and more.
First I just want to say a big thanks to Patrick (htmldog.com) for organising the event and to all the speakers and attendees I spoke to.
I must mention Molly Holzschlag (www.molly.com) individually for being amazing thought. Thanks for the beer.
I’ll hopefully get something more useful up here with more salient points over the next couple of weeks but for the moment I just want to say it was an inspirational couple of days, and make sure to watch the blogs of the speakers for real indepth stuff (rather than just awe).
I’ve of late been playing around with a number of different ways of storing bookmarks after realising I was just not bothering before. I added del.icio.us links to the site a bit ago and I’ve just come across Scuttle (scuttle.org) which is a GPL PHP del.icio.us clone which is really quite nice.
I’ve set it up at the moment on one of my machines and might look to add a styled and modified version to this site as time and inclination allow.
After much going backwards and forwards I finally sat down and had a protracted play with Ruby on Rails and so far I have to say I’m impressed.
Installation was simple enought and I got it running on both mac and linux hardware pretty easily, always a bonus, and I have vim set up to play with ERuby syntax.
I had dabbled previously with Ruby and coming from PHP the syntax is fairly self explanatory (remember end not }).But ActiveRecord in particular seems just great.
I’m going through the Four days on Rails tutorials at the moment so should have something more insightful to say in a week or so (I dont actually have four days in a row to just do this really).
I’m in the process of setting up the site so I can post from my Palm Tungsten T5. The image appearing here, of the device in question, will appear now for any posts I make from it when I’m out and about. How often this will be is anyones guess but I’m heading to @media in London soon so maybe some updates from there?
Right. I’ve nearly finished adding all the bits to this site that I liked about previous things I had up and rebuilding around all the things I disliked. So here’s to some new projects on the horizon.
The plan is to document and upload thoughts and anything of interest on my programming things I’m up to. I’m at the moment pondering C# and ASP.NET (Mono), Ruby on Rails and maybe something (Python?) to do with my Palm device. Like I have that much time!
A new project that I have been working on looking to bring together anyone making use of rss feeds in the north east. A slightly odd idea perhaps, but one that has me playing with collecting and caching feeds from other sites and a bit of minimal design. Visit www.northeastrss.com for the work in progress
php-eml is a sourceforge open source project I started a while back to develop a php based email mailing list. At the moment this in at a stage where it works but is not really pretty or easy to install unless you enjoy a little sysadmin.
newcastlenewmedia.org is a current project I maintain for the local web design and development community, centered around an email mailing list.
email2rss is one of my php projects. The aim is to allow simple access to rss feeds from any email client. Ideally suited to mobile devices without a reader or to less technical users.
It makes use of the lastRSS library to read external rss feeds and output the details, as well as a few tricks to read in an email (from stdin) sent to its own address and to email back the user with the news requested in the subject line.
Visit www.morethanseven.net/email2rss for more details.
